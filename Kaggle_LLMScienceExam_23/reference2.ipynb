{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center style=\"font-family: consolas; font-size: 32px; font-weight: bold;\"> Kaggle - LLM Science Exam</center>\n<p><center style=\"color:#949494; font-family: consolas; font-size: 20px;\">Use LLMs to answer difficult science questions</center></p>\n\n***","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# <center style=\"font-family: consolas; font-size: 32px; font-weight: bold;\">(ಠಿ⁠_⁠ಠ) Overview</center>\n\n<p style=\"font-family: consolas; font-size: 16px;\">⚪ The goal of the competition is to answer difficult science-based questions written by a Large Language Model (LLM).</p>\n\n<p style=\"font-family: consolas; font-size: 16px;\">⚪ The competition aims to help researchers understand the ability of LLMs to test themselves and explore the potential of LLMs that can be run in resource-constrained environments.</p>\n\n<p style=\"font-family: consolas; font-size: 16px;\">⚪ The scope of large language model capabilities is expanding, and researchers are using LLMs to characterize themselves.</p>\n\n<p style=\"font-family: consolas; font-size: 16px;\">⚪ Many existing natural language processing benchmarks have become trivial for state-of-the-art models, so there is a need to create more challenging tasks to test increasingly powerful models.</p>\n\n<p style=\"font-family: consolas; font-size: 16px;\">⚪ The dataset for the competition was generated by providing snippets of text on various scientific topics to the gpt3.5 model and asking it to write multiple choice questions (with known answers). Easy questions were filtered out.</p>\n\n<p style=\"font-family: consolas; font-size: 16px;\">⚪ Sign language recognition AI for text entry lags far behind voice-to-text or even gesture-based typing, as robust datasets didn't previously exist.</p>\n\n<p style=\"font-family: consolas; font-size: 16px;\">⚪ The largest models currently run on Kaggle have around 10 billion parameters, while gpt3.5 has 175 billion parameters.</p>\n\n<p style=\"font-family: consolas; font-size: 16px;\">⚪ The competition aims to explore whether a question-answering model more than 10 times smaller than gpt3.5 can effectively answer questions written by gpt3.5. The results will shed light on the benchmarking and self-testing capabilities of LLMs.</p>","metadata":{}},{"cell_type":"markdown","source":"#### <a id=\"top\"></a>\n# <div style=\"box-shadow: rgb(60, 121, 245) 0px 0px 0px 3px inset, rgb(255, 255, 255) 10px -10px 0px -3px, rgb(31, 193, 27) 10px -10px, rgb(255, 255, 255) 20px -20px 0px -3px, rgb(255, 217, 19) 20px -20px, rgb(255, 255, 255) 30px -30px 0px -3px, rgb(255, 156, 85) 30px -30px, rgb(255, 255, 255) 40px -40px 0px -3px, rgb(255, 85, 85) 40px -40px; padding:20px; margin-right: 40px; font-size:30px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(60, 121, 245);\"><b>Table of contents</b></div>\n\n<div style=\"background-color: rgba(60, 121, 245, 0.03); padding:30px; font-size:15px; font-family: consolas;\">\n<ul>\n    <li><a href=\"#0\" target=\"_self\" rel=\" noreferrer nofollow\">0. Import all dependencies</a></li>\n    <li><a href=\"#1\" target=\"_self\" rel=\" noreferrer nofollow\">1. Data overview</a></li>\n    <li><a href=\"#2\" target=\"_self\" rel=\" noreferrer nofollow\">2. Train overview</a></li>\n    <li><a href=\"#3\" target=\"_self\" rel=\" noreferrer nofollow\">3. Supplemental overview</a></li>\n</ul>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"0\"></a>\n# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 0. Import all dependencies </b></div>","metadata":{}},{"cell_type":"code","source":"import json\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px","metadata":{"execution":{"iopub.status.busy":"2023-07-15T08:50:37.091494Z","iopub.execute_input":"2023-07-15T08:50:37.091986Z","iopub.status.idle":"2023-07-15T08:50:37.847270Z","shell.execute_reply.started":"2023-07-15T08:50:37.091947Z","shell.execute_reply":"2023-07-15T08:50:37.846258Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 1. Data overview</b></div>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: consolas; font-size: 16px;\">⚪ The dataset for this competition consists of multiple-choice questions generated by a Large Language Model (LLM).</p>\n\n<p style=\"font-family: consolas; font-size: 16px;\">⚪ The questions are accompanied by options labeled A, B, C, D, and E, and each question has a correct answer labeled \"answer\".</p>\n\n<p style=\"font-family: consolas; font-size: 16px;\">⚪ The goal is to predict the top three most probable answers given a question prompt. </p>\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 1.1 train.csv</b></div>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: consolas; font-size: 16px;\">⚪ The train.csv file contains <b>200 questions</b> with their corresponding correct answers.</p>\n\n<p style=\"font-family: consolas; font-size: 16px;\">⚪ Each question consists of a prompt (the question text) and options <b>A, B, C, D, and E</b>.</p>\n\n<p style=\"font-family: consolas; font-size: 16px;\">⚪ The correct answer is indicated by the <code>answer</code> column, which contains the label of the most correct answer, as defined by the generating LLM. </p>\n","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-07-15T08:50:37.849236Z","iopub.execute_input":"2023-07-15T08:50:37.849558Z","iopub.status.idle":"2023-07-15T08:50:37.874442Z","shell.execute_reply.started":"2023-07-15T08:50:37.849532Z","shell.execute_reply":"2023-07-15T08:50:37.873574Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.2\"></a>\n## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 1.2 test.csv</b></div>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: consolas; font-size: 16px;\">⚪ The test.csv file contains the test set for the competition.</p>\n\n<p style=\"font-family: consolas; font-size: 16px;\">⚪ <b>The task is to predict the top <code>3</code> most probable answers</b> for each question prompt.</p>\n\n<p style=\"font-family: consolas; font-size: 16px;\">⚪ The format of the test set is the same as the training set, with questions, options (A, B, C, D, and E), and the prompt text.</p>\n\n<p style=\"font-family: consolas; font-size: 16px;\">⚪ The test set has approximately 4,000 different prompts, which may differ in subject matter from the training set.</p>\n\n<p style=\"font-family: consolas; font-size: 16px;\">⚪ <b>NOTE</b>: The test data you see here just a copy of the training data without the answers.</p>\n","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-07-15T08:50:37.875875Z","iopub.execute_input":"2023-07-15T08:50:37.876251Z","iopub.status.idle":"2023-07-15T08:50:37.891757Z","shell.execute_reply.started":"2023-07-15T08:50:37.876218Z","shell.execute_reply":"2023-07-15T08:50:37.890894Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stem_1k_df = pd.read_csv(\"/kaggle/input/wikipedia-stem-1k/stem_1k_v1.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-07-15T08:50:37.893686Z","iopub.execute_input":"2023-07-15T08:50:37.893955Z","iopub.status.idle":"2023-07-15T08:50:37.918452Z","shell.execute_reply.started":"2023-07-15T08:50:37.893931Z","shell.execute_reply":"2023-07-15T08:50:37.917609Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 3. Text classification</b></div>","metadata":{}},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom typing import Optional, Union\nimport torch\n\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer","metadata":{"execution":{"iopub.status.busy":"2023-07-15T08:50:37.919731Z","iopub.execute_input":"2023-07-15T08:50:37.920403Z","iopub.status.idle":"2023-07-15T08:50:50.552456Z","shell.execute_reply.started":"2023-07-15T08:50:37.920368Z","shell.execute_reply":"2023-07-15T08:50:50.551469Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"len(stem_1k_df)*0.1","metadata":{"execution":{"iopub.status.busy":"2023-07-15T08:50:50.553748Z","iopub.execute_input":"2023-07-15T08:50:50.554742Z","iopub.status.idle":"2023-07-15T08:50:50.562534Z","shell.execute_reply.started":"2023-07-15T08:50:50.554707Z","shell.execute_reply":"2023-07-15T08:50:50.561619Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"100.0"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_df)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T08:50:50.563782Z","iopub.execute_input":"2023-07-15T08:50:50.564626Z","iopub.status.idle":"2023-07-15T08:50:50.577508Z","shell.execute_reply.started":"2023-07-15T08:50:50.564573Z","shell.execute_reply":"2023-07-15T08:50:50.576537Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"200"},"metadata":{}}]},{"cell_type":"code","source":"# eval_sampled_train_df = train_df.sample(frac=0.5, random_state=42)\n# eval_sampled_stem_df = stem_1k_df.sample(frac=0.1, random_state=42)\n\n# eval_sampled_df = pd.concat([\n#     eval_sampled_train_df,\n#     eval_sampled_stem_df,\n# ])\n# len(eval_sampled_df)\n\n# train_sampled_train_df = train_df.drop(eval_sampled_train_df.index)\n# train_sampled_stem_df = stem_1k_df.drop(eval_sampled_stem_df.index)\n\n# train_sampled_df = pd.concat([\n#     train_sampled_train_df,\n#     train_sampled_stem_df,\n# ])\n# len(train_sampled_df)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T08:50:50.579181Z","iopub.execute_input":"2023-07-15T08:50:50.579571Z","iopub.status.idle":"2023-07-15T08:50:50.586808Z","shell.execute_reply.started":"2023-07-15T08:50:50.579539Z","shell.execute_reply":"2023-07-15T08:50:50.585909Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# eval_sampled_df = train_df.sample(frac=0.5, random_state=42)\n\n# train_sampled_train_df = train_df.drop(eval_sampled_df.index)\n# train_sampled_df = pd.concat([\n#     train_sampled_train_df,\n#     stem_1k_df,\n# ])\n# len(train_sampled_df)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T08:50:50.588116Z","iopub.execute_input":"2023-07-15T08:50:50.588636Z","iopub.status.idle":"2023-07-15T08:50:50.597734Z","shell.execute_reply.started":"2023-07-15T08:50:50.588603Z","shell.execute_reply":"2023-07-15T08:50:50.596561Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"new_train_df = pd.concat([\n    train_df,\n    stem_1k_df,\n])\nnew_train_df.index = list(range(len(new_train_df)))\nnew_train_df.id = list(range(len(new_train_df)))\n\neval_sampled_df = new_train_df.sample(frac=0.1, random_state=42)\n\ntrain_sampled_df = new_train_df.drop(eval_sampled_df.index)\n\nlen(train_sampled_df)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T08:50:50.602901Z","iopub.execute_input":"2023-07-15T08:50:50.603258Z","iopub.status.idle":"2023-07-15T08:50:50.622226Z","shell.execute_reply.started":"2023-07-15T08:50:50.603223Z","shell.execute_reply":"2023-07-15T08:50:50.621395Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"1080"},"metadata":{}}]},{"cell_type":"code","source":"train_ds = Dataset.from_pandas(train_sampled_df)\neval_ds = Dataset.from_pandas(eval_sampled_df)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T08:50:50.625037Z","iopub.execute_input":"2023-07-15T08:50:50.625297Z","iopub.status.idle":"2023-07-15T08:50:50.649947Z","shell.execute_reply.started":"2023-07-15T08:50:50.625274Z","shell.execute_reply":"2023-07-15T08:50:50.649097Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dir = \"microsoft/deberta-v3-large\"","metadata":{"execution":{"iopub.status.busy":"2023-07-15T08:50:50.651136Z","iopub.execute_input":"2023-07-15T08:50:50.651439Z","iopub.status.idle":"2023-07-15T08:50:50.655193Z","shell.execute_reply.started":"2023-07-15T08:50:50.651411Z","shell.execute_reply":"2023-07-15T08:50:50.654148Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_dir)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T08:50:50.656580Z","iopub.execute_input":"2023-07-15T08:50:50.657216Z","iopub.status.idle":"2023-07-15T08:50:55.537145Z","shell.execute_reply.started":"2023-07-15T08:50:50.657184Z","shell.execute_reply":"2023-07-15T08:50:55.536203Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4883b2eec3f42dda69480a85303481c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36b9abadb47b46f4877221c2934aba18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34838c601bdb4e87b3a4250c4f1363d8"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"# We'll create a dictionary to convert option names (A, B, C, D, E) into indices and back again\noptions = 'ABCDE'\nindices = list(range(5))\n\noption_to_index = {option: index for option, index in zip(options, indices)}\nindex_to_option = {index: option for option, index in zip(options, indices)}\n\ndef preprocess(example):\n    # The AutoModelForMultipleChoice class expects a set of question/answer pairs\n    # so we'll copy our question 5 times before tokenizing\n    first_sentence = [example['prompt']] * 5\n    second_sentence = []\n    for option in options:\n        second_sentence.append(example[option])\n    # Our tokenizer will turn our text into token IDs BERT can understand\n    tokenized_example = tokenizer(first_sentence, second_sentence, truncation=True)\n    tokenized_example['label'] = option_to_index[example['answer']]\n    return tokenized_example","metadata":{"execution":{"iopub.status.busy":"2023-07-15T08:50:55.541840Z","iopub.execute_input":"2023-07-15T08:50:55.544096Z","iopub.status.idle":"2023-07-15T08:50:55.554313Z","shell.execute_reply.started":"2023-07-15T08:50:55.544058Z","shell.execute_reply":"2023-07-15T08:50:55.552993Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"tokenized_train_ds = train_ds.map(preprocess, batched=False, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\ntokenized_eval_ds = eval_ds.map(preprocess, batched=False, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])","metadata":{"execution":{"iopub.status.busy":"2023-07-15T08:50:55.558716Z","iopub.execute_input":"2023-07-15T08:50:55.561337Z","iopub.status.idle":"2023-07-15T08:50:56.903803Z","shell.execute_reply.started":"2023-07-15T08:50:55.561302Z","shell.execute_reply":"2023-07-15T08:50:56.902900Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1080 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7175184af604ff7b4097c465b87dfd5"}},"metadata":{}},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/120 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61cd52a03ae54f9286e60e69dae8a099"}},"metadata":{}}]},{"cell_type":"markdown","source":"Following datacollator (adapted from https://huggingface.co/docs/transformers/tasks/multiple_choice)\nwill dynamically pad our questions at batch-time so we don't have to make every question the length\nof our longest question.","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass DataCollatorForMultipleChoice:\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    \n    def __call__(self, features):\n        label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n        labels = [feature.pop(label_name) for feature in features]\n        batch_size = len(features)\n        num_choices = len(features[0]['input_ids'])\n        flattened_features = [\n            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n        ]\n        flattened_features = sum(flattened_features, [])\n        \n        batch = self.tokenizer.pad(\n            flattened_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors='pt',\n        )\n        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n        return batch","metadata":{"execution":{"iopub.status.busy":"2023-07-15T08:50:56.905336Z","iopub.execute_input":"2023-07-15T08:50:56.905705Z","iopub.status.idle":"2023-07-15T08:50:56.917174Z","shell.execute_reply.started":"2023-07-15T08:50:56.905671Z","shell.execute_reply":"2023-07-15T08:50:56.915777Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForMultipleChoice.from_pretrained(model_dir)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T08:50:56.918671Z","iopub.execute_input":"2023-07-15T08:50:56.919034Z","iopub.status.idle":"2023-07-15T08:51:13.739706Z","shell.execute_reply.started":"2023-07-15T08:50:56.919001Z","shell.execute_reply":"2023-07-15T08:51:13.738746Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b282742ad014eac8713d559eaca4c48"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForMultipleChoice: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias']\n- This IS expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm -r /kaggle/working/finetuned_bert\n# !rm -r /kaggle/working/wandb","metadata":{"execution":{"iopub.status.busy":"2023-07-15T08:51:13.741087Z","iopub.execute_input":"2023-07-15T08:51:13.741576Z","iopub.status.idle":"2023-07-15T08:51:14.758510Z","shell.execute_reply.started":"2023-07-15T08:51:13.741539Z","shell.execute_reply":"2023-07-15T08:51:14.757285Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nrm: cannot remove '/kaggle/working/finetuned_bert': No such file or directory\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nos.environ['WANDB_API_KEY'] = user_secrets.get_secret(\"wandb_api\")","metadata":{"execution":{"iopub.status.busy":"2023-07-15T08:51:14.760911Z","iopub.execute_input":"2023-07-15T08:51:14.761356Z","iopub.status.idle":"2023-07-15T08:51:15.088844Z","shell.execute_reply.started":"2023-07-15T08:51:14.761317Z","shell.execute_reply":"2023-07-15T08:51:15.087637Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"model_dir = 'finetuned_bert'\ntraining_args = TrainingArguments(\n    output_dir=model_dir,\n    evaluation_strategy =\"steps\",\n    eval_steps = 50, # Evaluation and Save happens every 5 steps\n    save_steps = 50,\n    save_total_limit = 3, # Only last 3 models are saved. Older ones are deleted\n    logging_steps=1,\n    load_best_model_at_end=True,\n    learning_rate=3e-6,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    num_train_epochs=4,\n    warmup_steps=50,\n    report_to='wandb'\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T08:51:15.090407Z","iopub.execute_input":"2023-07-15T08:51:15.090800Z","iopub.status.idle":"2023-07-15T08:51:15.136327Z","shell.execute_reply.started":"2023-07-15T08:51:15.090765Z","shell.execute_reply":"2023-07-15T08:51:15.135385Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_ds,\n    eval_dataset=tokenized_eval_ds,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer)\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T08:51:15.137918Z","iopub.execute_input":"2023-07-15T08:51:15.138268Z","iopub.status.idle":"2023-07-15T08:51:20.701941Z","shell.execute_reply.started":"2023-07-15T08:51:15.138234Z","shell.execute_reply":"2023-07-15T08:51:20.700830Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# !mkdir -p finetuned_bert/checkpoint-30\n# !cp -a /kaggle/input/llm-se-deberta-v3-large-training/. /kaggle/working/finetuned_bert/checkpoint-30/.","metadata":{"execution":{"iopub.status.busy":"2023-07-15T08:51:20.703711Z","iopub.execute_input":"2023-07-15T08:51:20.704101Z","iopub.status.idle":"2023-07-15T08:51:20.708507Z","shell.execute_reply.started":"2023-07-15T08:51:20.704067Z","shell.execute_reply":"2023-07-15T08:51:20.707500Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-07-15T08:51:20.710228Z","iopub.execute_input":"2023-07-15T08:51:20.710576Z","iopub.status.idle":"2023-07-15T09:19:03.514805Z","shell.execute_reply.started":"2023-07-15T08:51:20.710542Z","shell.execute_reply":"2023-07-15T09:19:03.513652Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mleo27heady\u001b[0m (\u001b[33mi2p-onseo\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666945548333274, max=1.0)…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"434af9699c8a41b98bca37feeb92d26b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230715_085126-revl8kew</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/i2p-onseo/huggingface/runs/revl8kew' target=\"_blank\">peachy-sea-26</a></strong> to <a href='https://wandb.ai/i2p-onseo/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/i2p-onseo/huggingface' target=\"_blank\">https://wandb.ai/i2p-onseo/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/i2p-onseo/huggingface/runs/revl8kew' target=\"_blank\">https://wandb.ai/i2p-onseo/huggingface/runs/revl8kew</a>"},"metadata":{}},{"name":"stderr","text":"You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2160' max='2160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2160/2160 27:01, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.497800</td>\n      <td>1.607024</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.611300</td>\n      <td>1.604378</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.508300</td>\n      <td>1.596205</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.478900</td>\n      <td>1.585963</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.712600</td>\n      <td>1.559823</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.519600</td>\n      <td>1.505588</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.751500</td>\n      <td>1.427319</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.954500</td>\n      <td>1.360281</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.555100</td>\n      <td>1.319088</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.648900</td>\n      <td>1.283518</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.939200</td>\n      <td>1.269223</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.609400</td>\n      <td>1.241735</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.798500</td>\n      <td>1.234487</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.520700</td>\n      <td>1.220839</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>1.013300</td>\n      <td>1.247177</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.998800</td>\n      <td>1.248347</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.323900</td>\n      <td>1.247031</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>2.661400</td>\n      <td>1.240830</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>1.136800</td>\n      <td>1.234334</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.988700</td>\n      <td>1.232028</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.857800</td>\n      <td>1.203790</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.920800</td>\n      <td>1.203234</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>1.163600</td>\n      <td>1.215710</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.669100</td>\n      <td>1.202645</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.396000</td>\n      <td>1.182231</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.046200</td>\n      <td>1.245212</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.259800</td>\n      <td>1.271869</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.229800</td>\n      <td>1.316038</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.272800</td>\n      <td>1.268972</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.027100</td>\n      <td>1.322172</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.861800</td>\n      <td>1.366985</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.958400</td>\n      <td>1.367240</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.032300</td>\n      <td>1.403181</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.622000</td>\n      <td>1.397504</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.790900</td>\n      <td>1.345450</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.154400</td>\n      <td>1.402849</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.157200</td>\n      <td>1.373161</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.910700</td>\n      <td>1.326109</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.194800</td>\n      <td>1.361119</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.328900</td>\n      <td>1.370295</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.486900</td>\n      <td>1.377043</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>1.110600</td>\n      <td>1.366213</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.012100</td>\n      <td>1.371677</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2160, training_loss=1.018808510694308, metrics={'train_runtime': 1660.1988, 'train_samples_per_second': 2.602, 'train_steps_per_second': 1.301, 'total_flos': 1340019889268400.0, 'train_loss': 1.018808510694308, 'epoch': 4.0})"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !rm -r /kaggle/working/finetuned_bert/checkpoint-100\n# !cp -a /kaggle/working/finetuned_bert/checkpoint-70/. .","metadata":{"execution":{"iopub.status.busy":"2023-07-15T09:19:03.519999Z","iopub.execute_input":"2023-07-15T09:19:03.521470Z","iopub.status.idle":"2023-07-15T09:19:03.535022Z","shell.execute_reply.started":"2023-07-15T09:19:03.521420Z","shell.execute_reply":"2023-07-15T09:19:03.532477Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# !rm -r finetuned_bert\n# !rm -r wandb","metadata":{"execution":{"iopub.status.busy":"2023-07-15T09:19:03.538814Z","iopub.execute_input":"2023-07-15T09:19:03.543925Z","iopub.status.idle":"2023-07-15T09:19:03.551007Z","shell.execute_reply.started":"2023-07-15T09:19:03.543886Z","shell.execute_reply":"2023-07-15T09:19:03.549662Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# !rm -r /kaggle/working/finetuned_bert/checkpoint-840/","metadata":{"execution":{"iopub.status.busy":"2023-07-15T09:19:03.554328Z","iopub.execute_input":"2023-07-15T09:19:03.562164Z","iopub.status.idle":"2023-07-15T09:19:03.572865Z","shell.execute_reply.started":"2023-07-15T09:19:03.562118Z","shell.execute_reply":"2023-07-15T09:19:03.571644Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"trainer.save_model(f'.')","metadata":{"execution":{"iopub.status.busy":"2023-07-15T09:19:03.576226Z","iopub.execute_input":"2023-07-15T09:19:03.578083Z","iopub.status.idle":"2023-07-15T09:19:07.325873Z","shell.execute_reply.started":"2023-07-15T09:19:03.578042Z","shell.execute_reply":"2023-07-15T09:19:07.324609Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"!ls -a","metadata":{"execution":{"iopub.status.busy":"2023-07-15T09:19:07.340660Z","iopub.execute_input":"2023-07-15T09:19:07.343647Z","iopub.status.idle":"2023-07-15T09:19:08.517210Z","shell.execute_reply.started":"2023-07-15T09:19:07.343583Z","shell.execute_reply":"2023-07-15T09:19:08.515877Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n.\t\t\t   config.json\t\t    tokenizer.json\n..\t\t\t   finetuned_bert\t    tokenizer_config.json\n.virtual_documents\t   pytorch_model.bin\t    training_args.bin\n__notebook_source__.ipynb  special_tokens_map.json  wandb\nadded_tokens.json\t   spm.model\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm -r finetuned_bert\n!rm -r wandb","metadata":{"execution":{"iopub.status.busy":"2023-07-15T09:19:08.519690Z","iopub.execute_input":"2023-07-15T09:19:08.520434Z","iopub.status.idle":"2023-07-15T09:19:11.915023Z","shell.execute_reply.started":"2023-07-15T09:19:08.520393Z","shell.execute_reply":"2023-07-15T09:19:11.913798Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm __notebook_source__.ipynb","metadata":{"execution":{"iopub.status.busy":"2023-07-15T09:19:11.917533Z","iopub.execute_input":"2023-07-15T09:19:11.918334Z","iopub.status.idle":"2023-07-15T09:19:13.017656Z","shell.execute_reply.started":"2023-07-15T09:19:11.918295Z","shell.execute_reply":"2023-07-15T09:19:13.016315Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}