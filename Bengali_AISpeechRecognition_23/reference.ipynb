{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training Whisper for Bengali ASR ğŸµğŸ‘‚â¡ï¸ğŸ‡§ğŸ‡©ğŸ“\n\nThe [Whisper models, developed by OpenAI](https://openai.com/research/whisper), are the best open source multilingual Automatic Speech Recognition models available.\n\n## Architecture ğŸ—ï¸\n\n\nThe architecture is relatively straightforward: it's a sequence-to-sequence model containing an audio encoder and a text decoder. The feature extractor turns the 1d audio signal (amplitude over time) to a log-mel spectrogram. The encoder creates hidden states which are then passed to the decoder to generate text. It's basically BART with a few convolution layers at the input.\n\n![](https://huggingface.co/blog/assets/111_fine_tune_whisper/whisper_architecture.svg)\n\n\n## Training Data ğŸ“Š\n\nWhile the architecture is nothing novel, the OpenAI team created an enormous dataset on nearly 700k labeled audio data. Of that data, 117k hours were on multilingual ASR. Sadly, it looks like there was barely any Bengali data (Less than 2 hours!) in that training set, but that's why we're here! This competition provides 1200 hours of data which can be used to fine-tune the existing Whisper models. Even though Whisper wasn't trained on much Bengali data, it will be able to learn very quickly.\n\nImage from [figure 11 on page 27 of whisper paper](https://arxiv.org/pdf/2212.04356.pdf)\n![](https://raw.githubusercontent.com/nbroad1881/kaggle-images/main/bengali-ai-asr/whisper-bn.png)\n\n\n\n\n## In this notebook ğŸ““\n\n\n### Preprocessing ğŸµâ¡ï¸ğŸ”¢\n\nI show how to preprocess the data and how to train. The preprocessing can be a bit slow, so it is recommended to use a CPU notebook, or better yet, a bulkier CPU VM in your favorite cloud. I used another instance to preprocess 10k training samples and 1k eval samples.\n\n### Training ğŸ‹ï¸\n\nI show how to train on 2x T4 GPUs using Hugging Face transformers in pytorch. These GPUs have tensor cores which makes them go fast while using mixed precision. The code doesn't do anything fancy, but it can serve as a starting point for understanding ASR training. [@mbmmurad](https://kaggle.com/mbmmurad) pointed out that `bangla-speech-processing/BanglaASR` is a whisper model that has already been trained on Bangla. This notebook will do more fine-tuning  on 10k out of 960k training samples. \n\n\n### Validation ğŸ•µï¸\n\nI take a random split of files for train and validation, but since the domain is unknown, it is hard to get a good sense of how well the model will do on out-of-domain data. It would be nice if train.csv had domains as a column.\n\n\nNotebook Version | Model | WER \n- | - | -\n1 | openai/whisper-base | 0.69\n2 | bangla-speech-processing/BanglaASR | 0.529\n\n---\n\n## How to improve the model ğŸ’ª\n\n1. Train on more data. \n  - Like I mentioned above, there over 900k files and I only used 10k of them. \n2. Use a larger model. \n  - I'm only using the small-sized model which has 244M params. The largest model [1550M params](https://huggingface.co/openai/whisper-large-v2), which is probably too big, but there is also a [769M](https://huggingface.co/openai/whisper-medium) model.\n3. Data augmentations.\n  - I use [spec augment](https://arxiv.org/abs/1904.08779)\n  - You could also consider [BPE dropout](https://arxiv.org/abs/1910.13267) \n4. Better hyperparameters.\n  - learning rate is usually the most important","metadata":{}},{"cell_type":"code","source":"# Necessary packages\n!pip install -U evaluate datasets transformers jiwer -q","metadata":{"execution":{"iopub.status.busy":"2023-07-19T00:13:12.879710Z","iopub.execute_input":"2023-07-19T00:13:12.880280Z","iopub.status.idle":"2023-07-19T00:13:41.258402Z","shell.execute_reply.started":"2023-07-19T00:13:12.880242Z","shell.execute_reply":"2023-07-19T00:13:41.257156Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing\n\nThis should be done on CPU because this saves GPU time and the CPUs in CPU notebooks are faster than the CPUs in GPU notebooks. Better yet, use an even better CPU in a cloud VM.","metadata":{}},{"cell_type":"code","source":"%%writefile preprocess.py\n\nimport logging\nimport warnings\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union\n\nimport datasets\nfrom datasets import DatasetDict, load_dataset\n\nfrom transformers import (\n    AutoConfig,\n    AutoFeatureExtractor,\n    AutoTokenizer,\n    HfArgumentParser,\n    Seq2SeqTrainingArguments,\n    set_seed,\n)\n\n\nwarnings.simplefilter(\"ignore\")\n\n\n@dataclass\nclass Config:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n\n    model_name_or_path: str = field(\n        metadata={\n            \"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"\n        }\n    )\n\n    apply_spec_augment: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Whether to apply *SpecAugment* data augmentation to the input features. This is currently only relevant for Wav2Vec2, HuBERT, WavLM and Whisper models.\"\n        },\n    )\n    overwrite_cache: bool = field(\n        default=False,\n        metadata={\"help\": \"Overwrite the cached training and evaluation sets\"},\n    )\n    preprocessing_num_workers: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n    forced_decoder_ids: List[List[int]] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"A list of pairs of integers which indicates a mapping from generation indices to token indices \"\n                \"that will be forced before sampling. For example, [[0, 123]] means the first generated token \"\n                \"will always be a token of index 123.\"\n            )\n        },\n    )\n    suppress_tokens: List[int] = field(\n        default=None,\n        metadata={\"help\": \"A list of tokens that will be suppressed at generation.\"},\n    )\n    max_train_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    max_eval_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    audio_column_name: str = field(\n        default=\"audio\",\n        metadata={\n            \"help\": \"The name of the dataset column containing the audio data. Defaults to 'audio'\"\n        },\n    )\n    text_column_name: str = field(\n        default=\"text\",\n        metadata={\n            \"help\": \"The name of the dataset column containing the text data. Defaults to 'text'\"\n        },\n    )\n    max_duration_in_seconds: float = field(\n        default=20.0,\n        metadata={\n            \"help\": (\n                \"Truncate audio files that are longer than `max_duration_in_seconds` seconds to\"\n                \" 'max_duration_in_seconds`\"\n            )\n        },\n    )\n    min_duration_in_seconds: float = field(\n        default=0.0,\n        metadata={\n            \"help\": \"Filter audio files that are shorter than `min_duration_in_seconds` seconds\"\n        },\n    )\n    preprocessing_only: bool = field(\n        default=False,\n        metadata={\n            \"help\": (\n                \"Whether to only do data preprocessing and skip training. This is especially useful when data\"\n                \" preprocessing errors out in distributed training due to timeout. In this case, one should run the\"\n                \" preprocessing in a non-distributed setup with `preprocessing_only=True` so that the cached datasets\"\n                \" can consequently be loaded in distributed training\"\n            )\n        },\n    )\n    language: str = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"Language for multilingual fine-tuning. This argument should be set for multilingual fine-tuning \"\n                \"only. For English speech recognition, it should be set to `None`.\"\n            )\n        },\n    )\n\n    data_dir: str = field(\n        default=\"/kaggle/input/bengaliai-speech\",\n        metadata={\n            \"help\": (\n                \"Language for multilingual fine-tuning. This argument should be set for multilingual fine-tuning \"\n                \"only. For English speech recognition, it should be set to `None`.\"\n            )\n        },\n    )\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef main():\n    parser = HfArgumentParser((Config, Seq2SeqTrainingArguments))\n\n    cfg, training_args = parser.parse_args_into_dataclasses()\n\n    # Set seed before initializing model.\n    set_seed(training_args.seed)\n\n    config = AutoConfig.from_pretrained(cfg.model_name_or_path)\n\n    config.update(\n        {\n            \"forced_decoder_ids\": cfg.forced_decoder_ids,\n            \"suppress_tokens\": cfg.suppress_tokens,\n        }\n    )\n\n    # SpecAugment for whisper models\n    if getattr(config, \"model_type\", None) == \"whisper\":\n        config.update({\"apply_spec_augment\": cfg.apply_spec_augment})\n\n    feature_extractor = AutoFeatureExtractor.from_pretrained(cfg.model_name_or_path)\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name_or_path)\n\n    raw_datasets = DatasetDict()\n\n    data_dir = Path(cfg.data_dir)\n\n    raw_ds = load_dataset(\"csv\", data_files=str(data_dir / \"train.csv\"), split=\"train\")\n\n    def add_mp3_path(examples):\n        return {\n            \"audio\": [str(data_dir / f\"train_mp3s/{id_}.mp3\") for id_ in examples[\"id\"]]\n        }\n\n    raw_ds = raw_ds.map(add_mp3_path, batched=True, num_proc=cfg.preprocessing_num_workers)\n    raw_ds = raw_ds.train_test_split(\n        test_size=0.2, seed=training_args.seed, shuffle=True\n    )\n\n    raw_datasets[\"train\"] = raw_ds[\"train\"]\n    raw_datasets[\"validation\"] = raw_ds[\"test\"]\n\n    if cfg.max_train_samples:\n        raw_datasets[\"train\"] = raw_datasets[\"train\"].select(\n            range(min(cfg.max_train_samples, len(raw_datasets[\"train\"])))\n        )\n\n    if cfg.max_eval_samples:\n        raw_datasets[\"validation\"] = raw_datasets[\"validation\"].select(\n            range(min(cfg.max_eval_samples, len(raw_datasets[\"validation\"])))\n        )\n\n    # cast to audio\n    raw_datasets = raw_datasets.cast_column(\n        cfg.audio_column_name,\n        datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate),\n    )\n\n    if cfg.language is not None:\n        # We only need to set the task id when the language is specified (i.e. in a multilingual setting)\n        tokenizer.set_prefix_tokens(language=cfg.language, task=\"transcribe\")\n\n    # Preprocessing the datasets.\n    # We need to read the audio files as arrays and tokenize the targets.\n    max_input_length = cfg.max_duration_in_seconds * feature_extractor.sampling_rate\n    min_input_length = cfg.min_duration_in_seconds * feature_extractor.sampling_rate\n    audio_column_name = cfg.audio_column_name\n    num_workers = cfg.preprocessing_num_workers\n    text_column_name = cfg.text_column_name\n    model_input_name = feature_extractor.model_input_names[0]\n    # if SpecAugment is used for whisper models, return attention_mask to guide the mask along time axis\n    forward_attention_mask = (\n        getattr(config, \"model_type\", None) == \"whisper\"\n        and getattr(config, \"apply_spec_augment\", False)\n        and getattr(config, \"mask_time_prob\", 0) > 0\n    )\n\n    def prepare_dataset(batch):\n        # process audio\n        sample = batch[audio_column_name]\n        inputs = feature_extractor(\n            sample[\"array\"],\n            sampling_rate=sample[\"sampling_rate\"],\n            return_attention_mask=forward_attention_mask,\n        )\n        # process audio length\n        batch[model_input_name] = inputs.get(model_input_name)[0]\n        batch[\"input_length\"] = len(sample[\"array\"])\n        if forward_attention_mask:\n            batch[\"attention_mask\"] = inputs.get(\"attention_mask\")[0]\n\n        # process targets\n        input_str = batch[text_column_name]\n        batch[\"labels\"] = tokenizer(input_str).input_ids\n        return batch\n\n    with training_args.main_process_first(desc=\"dataset map pre-processing\"):\n        vectorized_datasets = raw_datasets.map(\n            prepare_dataset,\n            remove_columns=next(iter(raw_datasets.values())).column_names,\n            num_proc=cfg.preprocessing_num_workers,\n            desc=\"preprocess train dataset\",\n        )\n\n    # filter data that is shorter than min_input_length or longer than\n    # max_input_length\n    def is_audio_in_length_range(length):\n        return length > min_input_length and length < max_input_length\n\n    vectorized_datasets = vectorized_datasets.filter(\n        is_audio_in_length_range,\n        num_proc=num_workers,\n        input_columns=[\"input_length\"],\n    )\n\n    def save_chunks(ds, chunk_size, prefix):\n        for i in range(0, len(ds), chunk_size):\n            ii = min(i + chunk_size, len(ds))\n\n            ds.select(range(i, ii)).to_parquet(f\"{prefix}_{i}_to_{ii}.parquet\")\n\n    # for large datasets it is advised to run the preprocessing on a\n    # single machine first with `args.preprocessing_only` since there will mostly likely\n    # be a timeout when running the script in distributed mode.\n    # In a second step `args.preprocessing_only` can then be set to `False` to load the\n    # cached dataset\n    if cfg.preprocessing_only:\n        cache = {k: v.cache_files for k, v in vectorized_datasets.items()}\n        logger.info(f\"Data preprocessing finished. Files cached at {cache}.\")\n\n        save_chunks(\n            vectorized_datasets[\"train\"], 1000, f\"train_{training_args.output_dir}\"\n        )\n        save_chunks(\n            vectorized_datasets[\"validation\"], 1000, f\"eval_{training_args.output_dir}\"\n        )\n        return\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2023-07-19T00:13:41.261668Z","iopub.execute_input":"2023-07-19T00:13:41.262481Z","iopub.status.idle":"2023-07-19T00:13:41.278879Z","shell.execute_reply.started":"2023-07-19T00:13:41.262444Z","shell.execute_reply":"2023-07-19T00:13:41.277897Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Writing preprocess.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# I've already uploaded a preprocessed dataset with 10k train samples and 1k eval samples so I won't run this","metadata":{}},{"cell_type":"code","source":"# !python preprocess.py \\\n#  --model_name_or_path \"openai/whisper-small\" \\\n#  --language \"Bengali\" \\\n#  --output_dir \"75k-samples\" \\\n#  --preprocessing_num_workers 90 \\\n#  --preprocessing_only \\\n#  --text_column_name \"sentence\" \\\n#  --data_dir \"data\" \\\n#  --min_duration_in_seconds 2 \\\n#  --max_duration_in_seconds 30 \\\n#  --max_train_samples 75000 \\\n#  --max_eval_samples 5000 \\\n#  --apply_spec_augment","metadata":{"execution":{"iopub.status.busy":"2023-07-19T00:13:41.280461Z","iopub.execute_input":"2023-07-19T00:13:41.281138Z","iopub.status.idle":"2023-07-19T00:13:41.291549Z","shell.execute_reply.started":"2023-07-19T00:13:41.281080Z","shell.execute_reply":"2023-07-19T00:13:41.290480Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Training script\n\nAdapted from [here](https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py)","metadata":{}},{"cell_type":"code","source":"%%writefile train.py\n\n#!/usr/bin/env python\n# coding=utf-8\n# Copyright 2021 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nFine-tuning the library models for sequence to sequence speech recognition.\n\"\"\"\n# You can also adapt this script on your own sequence to sequence speech\n# recognition task. Pointers for this are left as comments.\n\nimport logging\nimport os\nimport sys\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union\n\nimport datasets\nimport evaluate\nimport torch\nfrom datasets import DatasetDict, load_dataset\n\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    AutoFeatureExtractor,\n    AutoModelForSpeechSeq2Seq,\n    AutoProcessor,\n    AutoTokenizer,\n    HfArgumentParser,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    set_seed,\n)\nfrom transformers.trainer_utils import get_last_checkpoint, is_main_process\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass Config:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n\n    model_name_or_path: str = field(\n        metadata={\n            \"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"\n        }\n    )\n    freeze_encoder: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to freeze the entire encoder of the seq2seq model.\"},\n    )\n    train_data_dir: str = field(default=None, metadata={\"help\": \"Path to train files\"})\n    validation_data_dir: str = field(\n        default=None, metadata={\"help\": \"Path to eval files\"}\n    )\n    forced_decoder_ids: List[List[int]] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"A list of pairs of integers which indicates a mapping from generation indices to token indices \"\n                \"that will be forced before sampling. For example, [[0, 123]] means the first generated token \"\n                \"will always be a token of index 123.\"\n            )\n        },\n    )\n    suppress_tokens: List[int] = field(\n        default=None,\n        metadata={\"help\": \"A list of tokens that will be suppressed at generation.\"},\n    )\n    apply_spec_augment: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Whether to apply *SpecAugment* data augmentation to the input features. This is currently only relevant for Wav2Vec2, HuBERT, WavLM and Whisper models.\"\n        },\n    )\n    max_train_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    max_eval_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    audio_column_name: str = field(\n        default=\"audio\",\n        metadata={\n            \"help\": \"The name of the dataset column containing the audio data. Defaults to 'audio'\"\n        },\n    )\n    text_column_name: str = field(\n        default=\"text\",\n        metadata={\n            \"help\": \"The name of the dataset column containing the text data. Defaults to 'text'\"\n        },\n    )\n\n    language: str = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"Language for multilingual fine-tuning. This argument should be set for multilingual fine-tuning \"\n                \"only. For English speech recognition, it should be set to `None`.\"\n            )\n        },\n    )\n\n\n@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    \"\"\"\n    Data collator that will dynamically pad the inputs received.\n    Args:\n        processor ([`WhisperProcessor`])\n            The processor used for processing the data.\n        decoder_start_token_id (`int`)\n            The begin-of-sentence of the decoder.\n        forward_attention_mask (`bool`)\n            Whether to return attention_mask.\n    \"\"\"\n\n    processor: Any\n    decoder_start_token_id: int\n    forward_attention_mask: bool\n\n    def __call__(\n        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n    ) -> Dict[str, torch.Tensor]:\n        # split inputs and labels since they have to be of different lengths and need\n        # different padding methods\n        model_input_name = self.processor.model_input_names[0]\n        input_features = [\n            {model_input_name: feature[model_input_name]} for feature in features\n        ]\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n\n        batch = self.processor.feature_extractor.pad(\n            input_features, return_tensors=\"pt\"\n        )\n\n        if self.forward_attention_mask:\n            batch[\"attention_mask\"] = torch.LongTensor(\n                [feature[\"attention_mask\"] for feature in features]\n            )\n\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n\n        # replace padding with -100 to ignore loss correctly\n        labels = labels_batch[\"input_ids\"].masked_fill(\n            labels_batch.attention_mask.ne(1), -100\n        )\n\n        # if bos token is appended in previous tokenization step,\n        # cut bos token here as it's append later anyways\n        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n\n        batch[\"labels\"] = labels\n\n        return batch\n\n\ndef main():\n    # 1. Parse input arguments\n    parser = HfArgumentParser((Config, Seq2SeqTrainingArguments))\n\n    cfg, training_args = parser.parse_args_into_dataclasses()\n\n    # 2. Detecting last checkpoint and eventually continue from last checkpoint\n    last_checkpoint = None\n    if (\n        os.path.isdir(training_args.output_dir)\n        and training_args.do_train\n        and not training_args.overwrite_output_dir\n    ):\n        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n            raise ValueError(\n                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n                \"Use --overwrite_output_dir to overcome.\"\n            )\n        elif (\n            last_checkpoint is not None and training_args.resume_from_checkpoint is None\n        ):\n            logger.info(\n                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n            )\n\n    # Set seed before initializing model.\n    set_seed(training_args.seed)\n\n    # 3. Load dataset\n    vectorized_datasets = DatasetDict()\n\n    if training_args.do_train:\n        train_files = list(map(str, Path(cfg.train_data_dir).glob(\"train*.parquet\")))\n        vectorized_datasets[\"train\"] = load_dataset(\n            \"parquet\", data_files=train_files, split=\"train\"\n        )\n\n    if training_args.do_eval:\n        eval_files = list(map(str, Path(cfg.validation_data_dir).glob(\"eval*.parquet\")))\n        vectorized_datasets[\"eval\"] = load_dataset(\n            \"parquet\", data_files=eval_files, split=\"train\"\n        )\n\n    # 4. Load pretrained model, tokenizer, and feature extractor\n    #\n    # Distributed training:\n    # The .from_pretrained methods guarantee that only one local process can concurrently\n    config = AutoConfig.from_pretrained(\n        cfg.model_name_or_path,\n    )\n\n    config.update(\n        {\n            \"forced_decoder_ids\": cfg.forced_decoder_ids,\n            \"suppress_tokens\": cfg.suppress_tokens,\n        }\n    )\n\n    # SpecAugment for whisper models\n    if getattr(config, \"model_type\", None) == \"whisper\":\n        config.update({\"apply_spec_augment\": cfg.apply_spec_augment})\n\n    feature_extractor = AutoFeatureExtractor.from_pretrained(\n        cfg.model_name_or_path,\n    )\n    tokenizer = AutoTokenizer.from_pretrained(\n        cfg.model_name_or_path,\n    )\n    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n        cfg.model_name_or_path,\n        config=config,\n    )\n\n    if model.config.decoder_start_token_id is None:\n        raise ValueError(\n            \"Make sure that `config.decoder_start_token_id` is correctly defined\"\n        )\n\n    if cfg.freeze_encoder:\n        model.freeze_encoder()\n        model.model.encoder.gradient_checkpointing = False\n\n    if cfg.language is not None:\n        # We only need to set the task id when the language is specified (i.e. in a multilingual setting)\n        tokenizer.set_prefix_tokens(language=cfg.language, task=\"transcribe\")\n\n    if cfg.max_train_samples is not None:\n        vectorized_datasets[\"train\"] = vectorized_datasets[\"train\"].select(\n            range(cfg.max_train_samples)\n        )\n\n    if cfg.max_eval_samples is not None:\n        vectorized_datasets[\"eval\"] = vectorized_datasets[\"eval\"].select(\n            range(cfg.max_eval_samples)\n        )\n\n    # 5. Load Metric\n    metric = evaluate.load(\"wer\")\n\n    def compute_metrics(pred):\n        pred_ids = pred.predictions\n\n        pred.label_ids[pred.label_ids == -100] = tokenizer.pad_token_id\n\n        pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n        # we do not want to group tokens when computing the metrics\n        label_str = tokenizer.batch_decode(pred.label_ids, skip_special_tokens=True)\n\n        wer = metric.compute(predictions=pred_str, references=label_str)\n\n        return {\"wer\": wer}\n\n    # 6. Create a single speech processor\n    # make sure all processes wait until data is saved\n    with training_args.main_process_first():\n        # only the main process saves them\n        if is_main_process(training_args.local_rank):\n            # save feature extractor, tokenizer and config\n            feature_extractor.save_pretrained(training_args.output_dir)\n            tokenizer.save_pretrained(training_args.output_dir)\n            config.save_pretrained(training_args.output_dir)\n\n    processor = AutoProcessor.from_pretrained(training_args.output_dir)\n\n    # if SpecAugment is used for whisper models, return attention_mask to guide the mask along time axis\n    forward_attention_mask = (\n        getattr(config, \"model_type\", None) == \"whisper\"\n        and getattr(config, \"apply_spec_augment\", False)\n        and getattr(config, \"mask_time_prob\", 0) > 0\n    )\n\n    # 7. Define data collator\n    data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n        processor=processor,\n        decoder_start_token_id=model.config.decoder_start_token_id,\n        forward_attention_mask=forward_attention_mask,\n    )\n\n    # 8. Initialize Trainer\n    trainer = Seq2SeqTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=vectorized_datasets[\"train\"] if training_args.do_train else None,\n        eval_dataset=vectorized_datasets[\"eval\"] if training_args.do_eval else None,\n        tokenizer=feature_extractor,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics\n        if training_args.predict_with_generate\n        else None,\n    )\n\n    # 9. Training\n    if training_args.do_train:\n        checkpoint = None\n        if training_args.resume_from_checkpoint is not None:\n            checkpoint = training_args.resume_from_checkpoint\n        elif last_checkpoint is not None:\n            checkpoint = last_checkpoint\n        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n        trainer.save_model()  # Saves the feature extractor too for easy upload\n\n        metrics = train_result.metrics\n        max_train_samples = (\n            cfg.max_train_samples\n            if cfg.max_train_samples is not None\n            else len(vectorized_datasets[\"train\"])\n        )\n        metrics[\"train_samples\"] = min(\n            max_train_samples, len(vectorized_datasets[\"train\"])\n        )\n        trainer.log_metrics(\"train\", metrics)\n        trainer.save_metrics(\"train\", metrics)\n        trainer.save_state()\n\n    # 10. Evaluation\n    results = {}\n    if training_args.do_eval:\n        logger.info(\"*** Evaluate ***\")\n        metrics = trainer.evaluate(\n            metric_key_prefix=\"eval\",\n            max_length=training_args.generation_max_length,\n            num_beams=training_args.generation_num_beams,\n        )\n        max_eval_samples = (\n            cfg.max_eval_samples\n            if cfg.max_eval_samples is not None\n            else len(vectorized_datasets[\"eval\"])\n        )\n        metrics[\"eval_samples\"] = min(\n            max_eval_samples, len(vectorized_datasets[\"eval\"])\n        )\n\n        trainer.log_metrics(\"eval\", metrics)\n        trainer.save_metrics(\"eval\", metrics)\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2023-07-19T00:13:41.294285Z","iopub.execute_input":"2023-07-19T00:13:41.294724Z","iopub.status.idle":"2023-07-19T00:13:41.316977Z","shell.execute_reply.started":"2023-07-19T00:13:41.294691Z","shell.execute_reply":"2023-07-19T00:13:41.316091Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Writing train.py\n","output_type":"stream"}]},{"cell_type":"code","source":"data_dir = \"/kaggle/input/bengali-ai-asr-10k\"\n\n!torchrun --nproc_per_node 2 train.py \\\n --model_name_or_path \"bangla-speech-processing/BanglaASR\" \\\n --train_data_dir $data_dir \\\n --validation_data_dir $data_dir \\\n --language \"Bengali\" \\\n --output_dir \"whisper-base-bn\" \\\n --do_train \\\n --do_eval \\\n --fp16 \\\n --group_by_length \\\n --predict_with_generate \\\n --dataloader_num_workers 1 \\\n --overwrite_output_dir \\\n --per_device_train_batch_size 4 \\\n --length_column_name \"input_length\" \\\n --report_to \"none\" \\\n --metric_for_best_model \"wer\" \\\n --greater_is_better False \\\n --evaluation_strategy \"epoch\" \\\n --save_strategy \"epoch\" \\\n --save_total_limit 1 \\\n --logging_steps 10 \\\n --gradient_checkpointing \\\n --warmup_steps 50 \\\n --apply_spec_augment True \\\n --num_train_epochs 3 \\\n --learning_rate \"1e-5\"","metadata":{"execution":{"iopub.status.busy":"2023-07-19T00:13:41.318244Z","iopub.execute_input":"2023-07-19T00:13:41.318690Z","iopub.status.idle":"2023-07-19T03:26:23.867155Z","shell.execute_reply.started":"2023-07-19T00:13:41.318657Z","shell.execute_reply":"2023-07-19T03:26:23.865966Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\nDownloading and preparing dataset parquet/default to /root/.cache/huggingface/datasets/parquet/default-81385bd261e94688/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\nDownloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 2851.33it/s]\nExtracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.35it/s]\nDataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/default-81385bd261e94688/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7. Subsequent calls will reuse this data.\nDownloading and preparing dataset parquet/default to /root/.cache/huggingface/datasets/parquet/default-4e41ae61686f3754/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\nDownloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 5249.44it/s]\nExtracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 39.93it/s]\nDataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/default-4e41ae61686f3754/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7. Subsequent calls will reuse this data.\nDownloading (â€¦)lve/main/config.json: 100%|â–ˆ| 1.29k/1.29k [00:00<00:00, 7.49MB/s]\nDownloading (â€¦)rocessor_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 339/339 [00:00<00:00, 2.22MB/s]\nDownloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 805/805 [00:00<00:00, 2.27MB/s]\nDownloading (â€¦)olve/main/vocab.json: 100%|â–ˆ| 1.04M/1.04M [00:00<00:00, 20.5MB/s]\nDownloading (â€¦)olve/main/merges.txt: 100%|â–ˆâ–ˆâ–ˆ| 494k/494k [00:00<00:00, 7.31MB/s]\nDownloading (â€¦)main/normalizer.json: 100%|â–ˆâ–ˆ| 52.7k/52.7k [00:00<00:00, 109MB/s]\nDownloading (â€¦)in/added_tokens.json: 100%|â–ˆ| 2.08k/2.08k [00:00<00:00, 12.3MB/s]\nDownloading (â€¦)cial_tokens_map.json: 100%|â–ˆ| 2.08k/2.08k [00:00<00:00, 13.3MB/s]\nDownloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 967M/967M [00:19<00:00, 50.2MB/s]\nDownloading (â€¦)neration_config.json: 100%|â–ˆ| 3.50k/3.50k [00:00<00:00, 18.5MB/s]\nDownloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.49k/4.49k [00:00<00:00, 4.31MB/s]\nDownloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.49k/4.49k [00:00<00:00, 2.36MB/s]\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n  0%|                                                  | 0/3750 [00:00<?, ?it/s]`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n{'loss': 0.5544, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.01}        \n{'loss': 0.9674, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.02}        \n{'loss': 0.7809, 'learning_rate': 5.400000000000001e-06, 'epoch': 0.02}         \n{'loss': 0.3443, 'learning_rate': 7.4e-06, 'epoch': 0.03}                       \n{'loss': 0.7277, 'learning_rate': 9.4e-06, 'epoch': 0.04}                       \n{'loss': 0.2186, 'learning_rate': 9.981081081081082e-06, 'epoch': 0.05}         \n{'loss': 0.528, 'learning_rate': 9.954054054054056e-06, 'epoch': 0.06}          \n{'loss': 0.554, 'learning_rate': 9.927027027027028e-06, 'epoch': 0.06}          \n{'loss': 0.2881, 'learning_rate': 9.9e-06, 'epoch': 0.07}                       \n{'loss': 0.5892, 'learning_rate': 9.872972972972974e-06, 'epoch': 0.08}         \n{'loss': 0.2527, 'learning_rate': 9.845945945945948e-06, 'epoch': 0.09}         \n{'loss': 0.3988, 'learning_rate': 9.81891891891892e-06, 'epoch': 0.1}           \n{'loss': 0.4872, 'learning_rate': 9.791891891891892e-06, 'epoch': 0.1}          \n{'loss': 0.2716, 'learning_rate': 9.764864864864866e-06, 'epoch': 0.11}         \n{'loss': 0.6239, 'learning_rate': 9.737837837837839e-06, 'epoch': 0.12}         \n{'loss': 0.2231, 'learning_rate': 9.71081081081081e-06, 'epoch': 0.13}          \n{'loss': 0.3773, 'learning_rate': 9.683783783783785e-06, 'epoch': 0.14}         \n{'loss': 0.4326, 'learning_rate': 9.656756756756757e-06, 'epoch': 0.14}         \n{'loss': 0.2545, 'learning_rate': 9.62972972972973e-06, 'epoch': 0.15}          \n{'loss': 0.5861, 'learning_rate': 9.602702702702703e-06, 'epoch': 0.16}         \n{'loss': 0.2098, 'learning_rate': 9.575675675675677e-06, 'epoch': 0.17}         \n{'loss': 0.3757, 'learning_rate': 9.54864864864865e-06, 'epoch': 0.18}          \n{'loss': 0.49, 'learning_rate': 9.521621621621623e-06, 'epoch': 0.18}           \n{'loss': 0.2616, 'learning_rate': 9.494594594594595e-06, 'epoch': 0.19}         \n{'loss': 0.5796, 'learning_rate': 9.467567567567568e-06, 'epoch': 0.2}          \n{'loss': 0.1665, 'learning_rate': 9.440540540540541e-06, 'epoch': 0.21}         \n{'loss': 0.3666, 'learning_rate': 9.413513513513515e-06, 'epoch': 0.22}         \n{'loss': 0.4569, 'learning_rate': 9.386486486486488e-06, 'epoch': 0.22}         \n{'loss': 0.248, 'learning_rate': 9.35945945945946e-06, 'epoch': 0.23}           \n{'loss': 0.5482, 'learning_rate': 9.332432432432434e-06, 'epoch': 0.24}         \n{'loss': 0.2248, 'learning_rate': 9.305405405405406e-06, 'epoch': 0.25}         \n{'loss': 0.3899, 'learning_rate': 9.278378378378378e-06, 'epoch': 0.26}         \n{'loss': 0.4889, 'learning_rate': 9.251351351351352e-06, 'epoch': 0.26}         \n{'loss': 0.2386, 'learning_rate': 9.224324324324326e-06, 'epoch': 0.27}         \n{'loss': 0.5748, 'learning_rate': 9.197297297297298e-06, 'epoch': 0.28}         \n{'loss': 0.2252, 'learning_rate': 9.17027027027027e-06, 'epoch': 0.29}          \n{'loss': 0.344, 'learning_rate': 9.143243243243244e-06, 'epoch': 0.3}           \n{'loss': 0.5127, 'learning_rate': 9.116216216216217e-06, 'epoch': 0.3}          \n{'loss': 0.2372, 'learning_rate': 9.08918918918919e-06, 'epoch': 0.31}          \n{'loss': 0.5928, 'learning_rate': 9.062162162162163e-06, 'epoch': 0.32}         \n{'loss': 0.1913, 'learning_rate': 9.035135135135135e-06, 'epoch': 0.33}         \n{'loss': 0.4092, 'learning_rate': 9.008108108108109e-06, 'epoch': 0.34}         \n{'loss': 0.4593, 'learning_rate': 8.981081081081083e-06, 'epoch': 0.34}         \n{'loss': 0.1973, 'learning_rate': 8.954054054054055e-06, 'epoch': 0.35}         \n{'loss': 0.5533, 'learning_rate': 8.927027027027027e-06, 'epoch': 0.36}         \n{'loss': 0.1856, 'learning_rate': 8.900000000000001e-06, 'epoch': 0.37}         \n{'loss': 0.3331, 'learning_rate': 8.872972972972973e-06, 'epoch': 0.38}         \n{'loss': 0.3994, 'learning_rate': 8.845945945945946e-06, 'epoch': 0.38}         \n{'loss': 0.2869, 'learning_rate': 8.81891891891892e-06, 'epoch': 0.39}          \n{'loss': 0.6238, 'learning_rate': 8.791891891891893e-06, 'epoch': 0.4}          \n{'loss': 0.1848, 'learning_rate': 8.764864864864866e-06, 'epoch': 0.41}         \n{'loss': 0.3666, 'learning_rate': 8.737837837837838e-06, 'epoch': 0.42}         \n{'loss': 0.4215, 'learning_rate': 8.710810810810812e-06, 'epoch': 0.42}         \n{'loss': 0.2572, 'learning_rate': 8.683783783783784e-06, 'epoch': 0.43}         \n{'loss': 0.5675, 'learning_rate': 8.656756756756758e-06, 'epoch': 0.44}         \n{'loss': 0.245, 'learning_rate': 8.62972972972973e-06, 'epoch': 0.45}           \n{'loss': 0.3526, 'learning_rate': 8.602702702702702e-06, 'epoch': 0.46}         \n{'loss': 0.4307, 'learning_rate': 8.575675675675676e-06, 'epoch': 0.46}         \n{'loss': 0.2833, 'learning_rate': 8.54864864864865e-06, 'epoch': 0.47}          \n{'loss': 0.5343, 'learning_rate': 8.521621621621622e-06, 'epoch': 0.48}         \n{'loss': 0.1867, 'learning_rate': 8.494594594594595e-06, 'epoch': 0.49}         \n{'loss': 0.322, 'learning_rate': 8.467567567567569e-06, 'epoch': 0.5}           \n{'loss': 0.4052, 'learning_rate': 8.440540540540541e-06, 'epoch': 0.5}          \n{'loss': 0.2654, 'learning_rate': 8.413513513513513e-06, 'epoch': 0.51}         \n{'loss': 0.5586, 'learning_rate': 8.386486486486487e-06, 'epoch': 0.52}         \n{'loss': 0.178, 'learning_rate': 8.359459459459461e-06, 'epoch': 0.53}          \n{'loss': 0.3187, 'learning_rate': 8.332432432432433e-06, 'epoch': 0.54}         \n{'loss': 0.4521, 'learning_rate': 8.305405405405405e-06, 'epoch': 0.54}         \n{'loss': 0.2641, 'learning_rate': 8.27837837837838e-06, 'epoch': 0.55}          \n{'loss': 0.5606, 'learning_rate': 8.251351351351352e-06, 'epoch': 0.56}         \n{'loss': 0.1741, 'learning_rate': 8.224324324324325e-06, 'epoch': 0.57}         \n{'loss': 0.3591, 'learning_rate': 8.197297297297298e-06, 'epoch': 0.58}         \n{'loss': 0.4855, 'learning_rate': 8.170270270270272e-06, 'epoch': 0.58}         \n{'loss': 0.2625, 'learning_rate': 8.143243243243244e-06, 'epoch': 0.59}         \n{'loss': 0.6077, 'learning_rate': 8.116216216216218e-06, 'epoch': 0.6}          \n{'loss': 0.1854, 'learning_rate': 8.08918918918919e-06, 'epoch': 0.61}          \n{'loss': 0.3615, 'learning_rate': 8.062162162162162e-06, 'epoch': 0.62}         \n{'loss': 0.4208, 'learning_rate': 8.035135135135136e-06, 'epoch': 0.62}         \n{'loss': 0.2361, 'learning_rate': 8.008108108108108e-06, 'epoch': 0.63}         \n{'loss': 0.5831, 'learning_rate': 7.98108108108108e-06, 'epoch': 0.64}          \n{'loss': 0.1952, 'learning_rate': 7.954054054054054e-06, 'epoch': 0.65}         \n{'loss': 0.3937, 'learning_rate': 7.927027027027028e-06, 'epoch': 0.66}         \n{'loss': 0.3957, 'learning_rate': 7.9e-06, 'epoch': 0.66}                       \n{'loss': 0.2243, 'learning_rate': 7.872972972972973e-06, 'epoch': 0.67}         \n{'loss': 0.5005, 'learning_rate': 7.845945945945947e-06, 'epoch': 0.68}         \n{'loss': 0.1769, 'learning_rate': 7.818918918918919e-06, 'epoch': 0.69}         \n{'loss': 0.3444, 'learning_rate': 7.791891891891893e-06, 'epoch': 0.7}          \n{'loss': 0.4449, 'learning_rate': 7.764864864864865e-06, 'epoch': 0.7}          \n{'loss': 0.2357, 'learning_rate': 7.737837837837839e-06, 'epoch': 0.71}         \n{'loss': 0.4785, 'learning_rate': 7.710810810810811e-06, 'epoch': 0.72}         \n{'loss': 0.1633, 'learning_rate': 7.683783783783785e-06, 'epoch': 0.73}         \n{'loss': 0.3221, 'learning_rate': 7.656756756756757e-06, 'epoch': 0.74}         \n{'loss': 0.4611, 'learning_rate': 7.62972972972973e-06, 'epoch': 0.74}          \n{'loss': 0.2137, 'learning_rate': 7.6027027027027035e-06, 'epoch': 0.75}        \n{'loss': 0.5512, 'learning_rate': 7.575675675675677e-06, 'epoch': 0.76}         \n{'loss': 0.1783, 'learning_rate': 7.548648648648649e-06, 'epoch': 0.77}         \n{'loss': 0.3181, 'learning_rate': 7.521621621621622e-06, 'epoch': 0.78}         \n{'loss': 0.3538, 'learning_rate': 7.494594594594595e-06, 'epoch': 0.78}         \n{'loss': 0.2383, 'learning_rate': 7.467567567567569e-06, 'epoch': 0.79}         \n{'loss': 0.5014, 'learning_rate': 7.44054054054054e-06, 'epoch': 0.8}           \n{'loss': 0.1733, 'learning_rate': 7.413513513513514e-06, 'epoch': 0.81}         \n{'loss': 0.3787, 'learning_rate': 7.386486486486487e-06, 'epoch': 0.82}         \n{'loss': 0.4272, 'learning_rate': 7.35945945945946e-06, 'epoch': 0.82}          \n{'loss': 0.1732, 'learning_rate': 7.3324324324324326e-06, 'epoch': 0.83}        \n{'loss': 0.4909, 'learning_rate': 7.305405405405406e-06, 'epoch': 0.84}         \n{'loss': 0.175, 'learning_rate': 7.278378378378379e-06, 'epoch': 0.85}          \n{'loss': 0.3066, 'learning_rate': 7.251351351351353e-06, 'epoch': 0.86}         \n{'loss': 0.401, 'learning_rate': 7.224324324324324e-06, 'epoch': 0.86}          \n{'loss': 0.1926, 'learning_rate': 7.197297297297298e-06, 'epoch': 0.87}         \n{'loss': 0.6278, 'learning_rate': 7.170270270270271e-06, 'epoch': 0.88}         \n{'loss': 0.1746, 'learning_rate': 7.143243243243244e-06, 'epoch': 0.89}         \n{'loss': 0.3277, 'learning_rate': 7.116216216216216e-06, 'epoch': 0.9}          \n{'loss': 0.4311, 'learning_rate': 7.089189189189189e-06, 'epoch': 0.9}          \n{'loss': 0.2134, 'learning_rate': 7.062162162162162e-06, 'epoch': 0.91}         \n{'loss': 0.4394, 'learning_rate': 7.035135135135136e-06, 'epoch': 0.92}         \n{'loss': 0.2035, 'learning_rate': 7.0081081081081086e-06, 'epoch': 0.93}        \n{'loss': 0.3377, 'learning_rate': 6.981081081081082e-06, 'epoch': 0.94}         \n{'loss': 0.3591, 'learning_rate': 6.954054054054055e-06, 'epoch': 0.94}         \n{'loss': 0.2264, 'learning_rate': 6.927027027027028e-06, 'epoch': 0.95}         \n{'loss': 0.5234, 'learning_rate': 6.9e-06, 'epoch': 0.96}                       \n{'loss': 0.167, 'learning_rate': 6.872972972972973e-06, 'epoch': 0.97}          \n{'loss': 0.3224, 'learning_rate': 6.845945945945946e-06, 'epoch': 0.98}         \n{'loss': 0.3624, 'learning_rate': 6.81891891891892e-06, 'epoch': 0.98}          \n{'loss': 0.2354, 'learning_rate': 6.791891891891892e-06, 'epoch': 0.99}         \n{'loss': 0.504, 'learning_rate': 6.764864864864865e-06, 'epoch': 1.0}           \n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 1250/3750 [47:26<1:17:04,  1.85s/it]\n  0%|                                                    | 0/63 [00:00<?, ?it/s]\u001b[A\n  3%|â–ˆâ–                                          | 2/63 [00:10<05:28,  5.38s/it]\u001b[A\n  5%|â–ˆâ–ˆ                                          | 3/63 [00:21<07:43,  7.73s/it]\u001b[A\n  6%|â–ˆâ–ˆâ–Š                                         | 4/63 [00:38<10:58, 11.16s/it]\u001b[A\n  8%|â–ˆâ–ˆâ–ˆâ–                                        | 5/63 [00:56<12:56, 13.39s/it]\u001b[A\n 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 6/63 [01:13<13:57, 14.69s/it]\u001b[A\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰                                       | 7/63 [01:24<12:33, 13.46s/it]\u001b[A\n 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                      | 8/63 [01:34<11:20, 12.37s/it]\u001b[A\n 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 9/63 [01:44<10:23, 11.55s/it]\u001b[A\n 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                    | 10/63 [01:54<09:45, 11.04s/it]\u001b[A\n 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                   | 11/63 [02:03<09:03, 10.46s/it]\u001b[A\n 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 12/63 [02:13<08:56, 10.52s/it]\u001b[A\n 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 13/63 [02:24<08:49, 10.60s/it]\u001b[A\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                 | 14/63 [02:40<10:02, 12.30s/it]\u001b[A\n 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 15/63 [02:51<09:29, 11.86s/it]\u001b[A\n 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                | 16/63 [03:08<10:30, 13.42s/it]\u001b[A\n 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                               | 17/63 [03:18<09:21, 12.21s/it]\u001b[A\n 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 18/63 [03:28<08:41, 11.58s/it]\u001b[A\n 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                              | 19/63 [03:40<08:33, 11.67s/it]\u001b[A\n 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                             | 20/63 [03:52<08:27, 11.81s/it]\u001b[A\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 21/63 [04:01<07:46, 11.10s/it]\u001b[A\n 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 22/63 [04:12<07:30, 10.99s/it]\u001b[A\n 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 23/63 [04:22<07:07, 10.69s/it]\u001b[A\n 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 24/63 [04:39<08:10, 12.57s/it]\u001b[A\n 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 25/63 [04:49<07:32, 11.92s/it]\u001b[A\n 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 26/63 [05:01<07:22, 11.97s/it]\u001b[A\n 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 27/63 [05:18<07:58, 13.28s/it]\u001b[A\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 28/63 [05:29<07:22, 12.64s/it]\u001b[A\n 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 29/63 [05:38<06:30, 11.48s/it]\u001b[A\n 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 30/63 [05:48<06:09, 11.21s/it]\u001b[A\n 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 31/63 [06:02<06:19, 11.85s/it]\u001b[A\n 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     | 32/63 [06:11<05:47, 11.21s/it]\u001b[A\n 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 33/63 [06:28<06:25, 12.84s/it]\u001b[A\n 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 34/63 [06:41<06:12, 12.86s/it]\u001b[A\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 35/63 [06:50<05:33, 11.90s/it]\u001b[A\n 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 36/63 [07:04<05:34, 12.40s/it]\u001b[A\n 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 37/63 [07:14<05:07, 11.81s/it]\u001b[A\n 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 38/63 [07:24<04:37, 11.09s/it]\u001b[A\n 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                | 39/63 [07:35<04:27, 11.15s/it]\u001b[A\n 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 40/63 [07:46<04:11, 10.94s/it]\u001b[A\n 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 41/63 [07:56<03:59, 10.89s/it]\u001b[A\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 42/63 [08:08<03:56, 11.24s/it]\u001b[A\n 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 43/63 [08:18<03:35, 10.79s/it]\u001b[A\n 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 44/63 [08:28<03:17, 10.38s/it]\u001b[A\n 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 45/63 [08:39<03:12, 10.68s/it]\u001b[A\n 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 46/63 [08:50<03:00, 10.65s/it]\u001b[A\n 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 47/63 [09:00<02:51, 10.71s/it]\u001b[A\n 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 48/63 [09:13<02:50, 11.38s/it]\u001b[A\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 49/63 [09:25<02:38, 11.33s/it]\u001b[A\n 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 50/63 [09:35<02:21, 10.92s/it]\u001b[A\n 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 51/63 [09:46<02:12, 11.02s/it]\u001b[A\n 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 52/63 [09:58<02:04, 11.31s/it]\u001b[A\n 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 53/63 [10:08<01:48, 10.88s/it]\u001b[A\n 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 54/63 [10:19<01:38, 10.90s/it]\u001b[A\n 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 55/63 [10:31<01:30, 11.33s/it]\u001b[A\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 56/63 [10:43<01:20, 11.54s/it]\u001b[A\n 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 57/63 [10:54<01:08, 11.43s/it]\u001b[A\n 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 58/63 [11:06<00:57, 11.45s/it]\u001b[A\n 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 59/63 [11:16<00:44, 11.12s/it]\u001b[A\n 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 60/63 [11:25<00:31, 10.61s/it]\u001b[A\n 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 61/63 [11:34<00:19,  9.90s/it]\u001b[A\n 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 62/63 [11:42<00:09,  9.31s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.2742122709751129, 'eval_wer': 0.5871745880105087, 'eval_runtime': 744.9587, 'eval_samples_per_second': 1.342, 'eval_steps_per_second': 0.085, 'epoch': 1.0}\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 1250/3750 [59:51<1:17:04,  1.85s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [11:57<00:00, 11.12s/it]\u001b[A\n{'loss': 0.1211, 'learning_rate': 6.7378378378378384e-06, 'epoch': 1.01}        \u001b[A\n{'loss': 0.2474, 'learning_rate': 6.7108108108108115e-06, 'epoch': 1.02}        \n{'loss': 0.3476, 'learning_rate': 6.683783783783784e-06, 'epoch': 1.02}         \n{'loss': 0.1734, 'learning_rate': 6.656756756756757e-06, 'epoch': 1.03}         \n{'loss': 0.4522, 'learning_rate': 6.62972972972973e-06, 'epoch': 1.04}          \n{'loss': 0.1258, 'learning_rate': 6.602702702702704e-06, 'epoch': 1.05}         \n{'loss': 0.2488, 'learning_rate': 6.575675675675676e-06, 'epoch': 1.06}         \n{'loss': 0.3202, 'learning_rate': 6.548648648648649e-06, 'epoch': 1.06}         \n{'loss': 0.1711, 'learning_rate': 6.521621621621622e-06, 'epoch': 1.07}         \n{'loss': 0.4825, 'learning_rate': 6.494594594594595e-06, 'epoch': 1.08}         \n{'loss': 0.1251, 'learning_rate': 6.467567567567568e-06, 'epoch': 1.09}         \n{'loss': 0.2507, 'learning_rate': 6.4405405405405405e-06, 'epoch': 1.1}         \n{'loss': 0.3514, 'learning_rate': 6.4135135135135144e-06, 'epoch': 1.1}         \n{'loss': 0.1483, 'learning_rate': 6.3864864864864875e-06, 'epoch': 1.11}        \n{'loss': 0.4609, 'learning_rate': 6.359459459459461e-06, 'epoch': 1.12}         \n{'loss': 0.1611, 'learning_rate': 6.332432432432433e-06, 'epoch': 1.13}         \n{'loss': 0.2295, 'learning_rate': 6.305405405405406e-06, 'epoch': 1.14}         \n{'loss': 0.3026, 'learning_rate': 6.278378378378379e-06, 'epoch': 1.14}         \n{'loss': 0.1885, 'learning_rate': 6.251351351351352e-06, 'epoch': 1.15}         \n{'loss': 0.3895, 'learning_rate': 6.224324324324324e-06, 'epoch': 1.16}         \n{'loss': 0.1354, 'learning_rate': 6.197297297297298e-06, 'epoch': 1.17}         \n{'loss': 0.2162, 'learning_rate': 6.170270270270271e-06, 'epoch': 1.18}         \n{'loss': 0.3224, 'learning_rate': 6.143243243243244e-06, 'epoch': 1.18}         \n{'loss': 0.1665, 'learning_rate': 6.1162162162162165e-06, 'epoch': 1.19}        \n{'loss': 0.5007, 'learning_rate': 6.08918918918919e-06, 'epoch': 1.2}           \n{'loss': 0.1258, 'learning_rate': 6.062162162162163e-06, 'epoch': 1.21}         \n{'loss': 0.2618, 'learning_rate': 6.035135135135136e-06, 'epoch': 1.22}         \n{'loss': 0.3811, 'learning_rate': 6.008108108108108e-06, 'epoch': 1.22}         \n{'loss': 0.2118, 'learning_rate': 5.981081081081082e-06, 'epoch': 1.23}         \n{'loss': 0.4791, 'learning_rate': 5.954054054054055e-06, 'epoch': 1.24}         \n{'loss': 0.1338, 'learning_rate': 5.927027027027028e-06, 'epoch': 1.25}         \n{'loss': 0.2913, 'learning_rate': 5.9e-06, 'epoch': 1.26}                       \n{'loss': 0.2928, 'learning_rate': 5.872972972972973e-06, 'epoch': 1.26}         \n{'loss': 0.1417, 'learning_rate': 5.845945945945946e-06, 'epoch': 1.27}         \n{'loss': 0.4632, 'learning_rate': 5.81891891891892e-06, 'epoch': 1.28}          \n{'loss': 0.1445, 'learning_rate': 5.791891891891892e-06, 'epoch': 1.29}         \n{'loss': 0.2366, 'learning_rate': 5.764864864864866e-06, 'epoch': 1.3}          \n{'loss': 0.3582, 'learning_rate': 5.737837837837839e-06, 'epoch': 1.3}          \n{'loss': 0.1594, 'learning_rate': 5.710810810810812e-06, 'epoch': 1.31}         \n{'loss': 0.4582, 'learning_rate': 5.683783783783784e-06, 'epoch': 1.32}         \n{'loss': 0.1147, 'learning_rate': 5.656756756756757e-06, 'epoch': 1.33}         \n{'loss': 0.2713, 'learning_rate': 5.62972972972973e-06, 'epoch': 1.34}          \n{'loss': 0.3025, 'learning_rate': 5.602702702702704e-06, 'epoch': 1.34}         \n{'loss': 0.1499, 'learning_rate': 5.5756756756756754e-06, 'epoch': 1.35}        \n{'loss': 0.5222, 'learning_rate': 5.548648648648649e-06, 'epoch': 1.36}         \n{'loss': 0.1188, 'learning_rate': 5.521621621621622e-06, 'epoch': 1.37}         \n{'loss': 0.2608, 'learning_rate': 5.4945945945945955e-06, 'epoch': 1.38}        \n{'loss': 0.3454, 'learning_rate': 5.467567567567568e-06, 'epoch': 1.38}         \n{'loss': 0.1521, 'learning_rate': 5.440540540540541e-06, 'epoch': 1.39}         \n{'loss': 0.4562, 'learning_rate': 5.413513513513514e-06, 'epoch': 1.4}          \n{'loss': 0.1047, 'learning_rate': 5.386486486486488e-06, 'epoch': 1.41}         \n{'loss': 0.2438, 'learning_rate': 5.359459459459459e-06, 'epoch': 1.42}         \n{'loss': 0.3318, 'learning_rate': 5.332432432432433e-06, 'epoch': 1.42}         \n{'loss': 0.1479, 'learning_rate': 5.305405405405406e-06, 'epoch': 1.43}         \n{'loss': 0.4447, 'learning_rate': 5.278378378378379e-06, 'epoch': 1.44}         \n{'loss': 0.1337, 'learning_rate': 5.2513513513513514e-06, 'epoch': 1.45}        \n{'loss': 0.2493, 'learning_rate': 5.2243243243243245e-06, 'epoch': 1.46}        \n{'loss': 0.3546, 'learning_rate': 5.197297297297298e-06, 'epoch': 1.46}         \n{'loss': 0.1529, 'learning_rate': 5.1702702702702715e-06, 'epoch': 1.47}        \n{'loss': 0.4699, 'learning_rate': 5.143243243243244e-06, 'epoch': 1.48}         \n{'loss': 0.1036, 'learning_rate': 5.116216216216217e-06, 'epoch': 1.49}         \n{'loss': 0.2448, 'learning_rate': 5.08918918918919e-06, 'epoch': 1.5}           \n{'loss': 0.3467, 'learning_rate': 5.062162162162163e-06, 'epoch': 1.5}          \n{'loss': 0.1348, 'learning_rate': 5.035135135135135e-06, 'epoch': 1.51}         \n{'loss': 0.5118, 'learning_rate': 5.008108108108108e-06, 'epoch': 1.52}         \n{'loss': 0.106, 'learning_rate': 4.981081081081081e-06, 'epoch': 1.53}          \n{'loss': 0.2288, 'learning_rate': 4.954054054054054e-06, 'epoch': 1.54}         \n{'loss': 0.3456, 'learning_rate': 4.9270270270270274e-06, 'epoch': 1.54}        \n{'loss': 0.1576, 'learning_rate': 4.9000000000000005e-06, 'epoch': 1.55}        \n{'loss': 0.3892, 'learning_rate': 4.872972972972974e-06, 'epoch': 1.56}         \n{'loss': 0.1255, 'learning_rate': 4.845945945945947e-06, 'epoch': 1.57}         \n{'loss': 0.2905, 'learning_rate': 4.81891891891892e-06, 'epoch': 1.58}          \n{'loss': 0.3285, 'learning_rate': 4.791891891891892e-06, 'epoch': 1.58}         \n{'loss': 0.1379, 'learning_rate': 4.764864864864865e-06, 'epoch': 1.59}         \n{'loss': 0.4107, 'learning_rate': 4.737837837837838e-06, 'epoch': 1.6}          \n{'loss': 0.1504, 'learning_rate': 4.710810810810811e-06, 'epoch': 1.61}         \n{'loss': 0.3113, 'learning_rate': 4.683783783783784e-06, 'epoch': 1.62}         \n{'loss': 0.3566, 'learning_rate': 4.656756756756757e-06, 'epoch': 1.62}         \n{'loss': 0.1755, 'learning_rate': 4.62972972972973e-06, 'epoch': 1.63}          \n{'loss': 0.3649, 'learning_rate': 4.6027027027027035e-06, 'epoch': 1.64}        \n{'loss': 0.1547, 'learning_rate': 4.575675675675676e-06, 'epoch': 1.65}         \n{'loss': 0.2209, 'learning_rate': 4.54864864864865e-06, 'epoch': 1.66}          \n{'loss': 0.3001, 'learning_rate': 4.521621621621622e-06, 'epoch': 1.66}         \n{'loss': 0.1456, 'learning_rate': 4.494594594594595e-06, 'epoch': 1.67}         \n{'loss': 0.5349, 'learning_rate': 4.470270270270271e-06, 'epoch': 1.68}         \n{'loss': 0.102, 'learning_rate': 4.443243243243244e-06, 'epoch': 1.69}          \n{'loss': 0.2002, 'learning_rate': 4.416216216216217e-06, 'epoch': 1.7}          \n{'loss': 0.3338, 'learning_rate': 4.38918918918919e-06, 'epoch': 1.7}           \n{'loss': 0.1558, 'learning_rate': 4.362162162162162e-06, 'epoch': 1.71}         \n{'loss': 0.3401, 'learning_rate': 4.335135135135136e-06, 'epoch': 1.72}         \n{'loss': 0.0991, 'learning_rate': 4.308108108108108e-06, 'epoch': 1.73}         \n{'loss': 0.2439, 'learning_rate': 4.281081081081081e-06, 'epoch': 1.74}         \n{'loss': 0.3244, 'learning_rate': 4.254054054054054e-06, 'epoch': 1.74}         \n{'loss': 0.2138, 'learning_rate': 4.227027027027027e-06, 'epoch': 1.75}         \n{'loss': 0.404, 'learning_rate': 4.2000000000000004e-06, 'epoch': 1.76}         \n{'loss': 0.1004, 'learning_rate': 4.1729729729729735e-06, 'epoch': 1.77}        \n{'loss': 0.2491, 'learning_rate': 4.145945945945946e-06, 'epoch': 1.78}         \n{'loss': 0.3327, 'learning_rate': 4.11891891891892e-06, 'epoch': 1.78}          \n{'loss': 0.1302, 'learning_rate': 4.091891891891892e-06, 'epoch': 1.79}         \n{'loss': 0.3837, 'learning_rate': 4.064864864864865e-06, 'epoch': 1.8}          \n{'loss': 0.1215, 'learning_rate': 4.037837837837838e-06, 'epoch': 1.81}         \n{'loss': 0.263, 'learning_rate': 4.010810810810811e-06, 'epoch': 1.82}          \n{'loss': 0.3402, 'learning_rate': 3.983783783783784e-06, 'epoch': 1.82}         \n{'loss': 0.1814, 'learning_rate': 3.956756756756757e-06, 'epoch': 1.83}         \n{'loss': 0.4386, 'learning_rate': 3.9297297297297295e-06, 'epoch': 1.84}        \n{'loss': 0.1167, 'learning_rate': 3.902702702702703e-06, 'epoch': 1.85}         \n{'loss': 0.2326, 'learning_rate': 3.875675675675676e-06, 'epoch': 1.86}         \n{'loss': 0.3476, 'learning_rate': 3.848648648648649e-06, 'epoch': 1.86}         \n{'loss': 0.1637, 'learning_rate': 3.821621621621622e-06, 'epoch': 1.87}         \n{'loss': 0.4561, 'learning_rate': 3.794594594594595e-06, 'epoch': 1.88}         \n{'loss': 0.1208, 'learning_rate': 3.767567567567568e-06, 'epoch': 1.89}         \n{'loss': 0.2306, 'learning_rate': 3.740540540540541e-06, 'epoch': 1.9}          \n{'loss': 0.2789, 'learning_rate': 3.7135135135135136e-06, 'epoch': 1.9}         \n{'loss': 0.1785, 'learning_rate': 3.6864864864864867e-06, 'epoch': 1.91}        \n{'loss': 0.4433, 'learning_rate': 3.6594594594594598e-06, 'epoch': 1.92}        \n{'loss': 0.1517, 'learning_rate': 3.632432432432433e-06, 'epoch': 1.93}         \n{'loss': 0.2749, 'learning_rate': 3.6054054054054055e-06, 'epoch': 1.94}        \n{'loss': 0.357, 'learning_rate': 3.5783783783783785e-06, 'epoch': 1.94}         \n{'loss': 0.1535, 'learning_rate': 3.5513513513513516e-06, 'epoch': 1.95}        \n{'loss': 0.3971, 'learning_rate': 3.5243243243243247e-06, 'epoch': 1.96}        \n{'loss': 0.1472, 'learning_rate': 3.4972972972972973e-06, 'epoch': 1.97}        \n{'loss': 0.2005, 'learning_rate': 3.470270270270271e-06, 'epoch': 1.98}         \n{'loss': 0.3028, 'learning_rate': 3.4432432432432435e-06, 'epoch': 1.98}        \n{'loss': 0.1244, 'learning_rate': 3.4162162162162166e-06, 'epoch': 1.99}        \n{'loss': 0.4692, 'learning_rate': 3.389189189189189e-06, 'epoch': 2.0}          \n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 2500/3750 [1:47:12<38:29,  1.85s/it]\n  0%|                                                    | 0/63 [00:00<?, ?it/s]\u001b[A\n  3%|â–ˆâ–                                          | 2/63 [00:09<04:59,  4.92s/it]\u001b[A\n  5%|â–ˆâ–ˆ                                          | 3/63 [00:20<07:27,  7.46s/it]\u001b[A\n  6%|â–ˆâ–ˆâ–Š                                         | 4/63 [00:31<08:33,  8.70s/it]\u001b[A\n  8%|â–ˆâ–ˆâ–ˆâ–                                        | 5/63 [00:41<08:56,  9.24s/it]\u001b[A\n 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 6/63 [00:52<09:11,  9.68s/it]\u001b[A\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰                                       | 7/63 [01:03<09:21, 10.02s/it]\u001b[A\n 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                      | 8/63 [01:13<09:08,  9.97s/it]\u001b[A\n 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 9/63 [01:22<08:48,  9.78s/it]\u001b[A\n 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                    | 10/63 [01:32<08:45,  9.91s/it]\u001b[A\n 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                   | 11/63 [01:41<08:25,  9.72s/it]\u001b[A\n 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 12/63 [01:52<08:25,  9.92s/it]\u001b[A\n 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 13/63 [02:03<08:32, 10.25s/it]\u001b[A\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                 | 14/63 [02:19<09:51, 12.06s/it]\u001b[A\n 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 15/63 [02:31<09:43, 12.16s/it]\u001b[A\n 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                | 16/63 [02:43<09:16, 11.84s/it]\u001b[A\n 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                               | 17/63 [02:52<08:32, 11.15s/it]\u001b[A\n 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 18/63 [03:06<08:57, 11.94s/it]\u001b[A\n 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                              | 19/63 [03:18<08:44, 11.92s/it]\u001b[A\n 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                             | 20/63 [03:29<08:27, 11.81s/it]\u001b[A\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 21/63 [03:40<07:58, 11.39s/it]\u001b[A\n 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 22/63 [03:51<07:42, 11.27s/it]\u001b[A\n 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 23/63 [04:01<07:14, 10.86s/it]\u001b[A\n 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 24/63 [04:13<07:23, 11.38s/it]\u001b[A\n 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 25/63 [04:23<06:58, 11.02s/it]\u001b[A\n 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 26/63 [04:35<06:52, 11.14s/it]\u001b[A\n 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 27/63 [04:47<06:53, 11.48s/it]\u001b[A\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 28/63 [04:57<06:28, 11.11s/it]\u001b[A\n 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 29/63 [05:06<05:57, 10.52s/it]\u001b[A\n 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 30/63 [05:17<05:49, 10.60s/it]\u001b[A\n 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 31/63 [05:30<05:56, 11.14s/it]\u001b[A\n 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     | 32/63 [05:47<06:44, 13.06s/it]\u001b[A\n 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 33/63 [06:04<07:04, 14.15s/it]\u001b[A\n 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 34/63 [06:17<06:40, 13.81s/it]\u001b[A\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 35/63 [06:26<05:51, 12.55s/it]\u001b[A\n 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 36/63 [06:39<05:40, 12.59s/it]\u001b[A\n 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 37/63 [06:50<05:13, 12.07s/it]\u001b[A\n 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 38/63 [06:59<04:41, 11.24s/it]\u001b[A\n 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                | 39/63 [07:10<04:23, 11.00s/it]\u001b[A\n 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 40/63 [07:22<04:20, 11.35s/it]\u001b[A\n 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 41/63 [07:33<04:05, 11.17s/it]\u001b[A\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 42/63 [07:44<03:58, 11.34s/it]\u001b[A\n 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 43/63 [07:59<04:08, 12.44s/it]\u001b[A\n 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 44/63 [08:14<04:08, 13.10s/it]\u001b[A\n 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 45/63 [08:25<03:44, 12.49s/it]\u001b[A\n 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 46/63 [08:36<03:22, 11.89s/it]\u001b[A\n 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 47/63 [08:47<03:05, 11.60s/it]\u001b[A\n 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 48/63 [08:58<02:51, 11.45s/it]\u001b[A\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 49/63 [09:09<02:38, 11.31s/it]\u001b[A\n 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 50/63 [09:18<02:20, 10.83s/it]\u001b[A\n 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 51/63 [09:30<02:11, 10.94s/it]\u001b[A\n 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 52/63 [09:41<02:03, 11.21s/it]\u001b[A\n 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 53/63 [09:51<01:48, 10.84s/it]\u001b[A\n 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 54/63 [10:02<01:37, 10.80s/it]\u001b[A\n 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 55/63 [10:14<01:30, 11.28s/it]\u001b[A\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 56/63 [10:28<01:24, 12.02s/it]\u001b[A\n 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 57/63 [10:39<01:09, 11.57s/it]\u001b[A\n 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 58/63 [10:50<00:57, 11.52s/it]\u001b[A\n 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 59/63 [11:01<00:45, 11.38s/it]\u001b[A\n 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 60/63 [11:13<00:34, 11.65s/it]\u001b[A\n 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 61/63 [11:22<00:21, 10.58s/it]\u001b[A\n 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 62/63 [11:30<00:09,  9.94s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.25855952501296997, 'eval_wer': 0.539765942202054, 'eval_runtime': 716.7943, 'eval_samples_per_second': 1.395, 'eval_steps_per_second': 0.088, 'epoch': 2.0}\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 2500/3750 [1:59:09<38:29,  1.85s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [11:38<00:00,  9.21s/it]\u001b[A\n{'loss': 0.0864, 'learning_rate': 3.3621621621621627e-06, 'epoch': 2.01}        \u001b[A\n{'loss': 0.1643, 'learning_rate': 3.3351351351351353e-06, 'epoch': 2.02}        \n{'loss': 0.2695, 'learning_rate': 3.3081081081081084e-06, 'epoch': 2.02}        \n{'loss': 0.1298, 'learning_rate': 3.281081081081081e-06, 'epoch': 2.03}         \n{'loss': 0.393, 'learning_rate': 3.2540540540540546e-06, 'epoch': 2.04}         \n{'loss': 0.0753, 'learning_rate': 3.227027027027027e-06, 'epoch': 2.05}         \n{'loss': 0.192, 'learning_rate': 3.2000000000000003e-06, 'epoch': 2.06}         \n{'loss': 0.2885, 'learning_rate': 3.172972972972973e-06, 'epoch': 2.06}         \n{'loss': 0.1062, 'learning_rate': 3.1459459459459464e-06, 'epoch': 2.07}        \n{'loss': 0.3711, 'learning_rate': 3.118918918918919e-06, 'epoch': 2.08}         \n{'loss': 0.0856, 'learning_rate': 3.091891891891892e-06, 'epoch': 2.09}         \n{'loss': 0.1854, 'learning_rate': 3.064864864864865e-06, 'epoch': 2.1}          \n{'loss': 0.3301, 'learning_rate': 3.0378378378378383e-06, 'epoch': 2.1}         \n{'loss': 0.1363, 'learning_rate': 3.010810810810811e-06, 'epoch': 2.11}         \n{'loss': 0.4113, 'learning_rate': 2.983783783783784e-06, 'epoch': 2.12}         \n{'loss': 0.1037, 'learning_rate': 2.9567567567567567e-06, 'epoch': 2.13}        \n{'loss': 0.1965, 'learning_rate': 2.92972972972973e-06, 'epoch': 2.14}          \n{'loss': 0.278, 'learning_rate': 2.902702702702703e-06, 'epoch': 2.14}          \n{'loss': 0.127, 'learning_rate': 2.875675675675676e-06, 'epoch': 2.15}          \n{'loss': 0.3774, 'learning_rate': 2.8486486486486485e-06, 'epoch': 2.16}        \n{'loss': 0.0787, 'learning_rate': 2.821621621621622e-06, 'epoch': 2.17}         \n{'loss': 0.1789, 'learning_rate': 2.7945945945945947e-06, 'epoch': 2.18}        \n{'loss': 0.3123, 'learning_rate': 2.7675675675675677e-06, 'epoch': 2.18}        \n{'loss': 0.1262, 'learning_rate': 2.7405405405405404e-06, 'epoch': 2.19}        \n{'loss': 0.3884, 'learning_rate': 2.713513513513514e-06, 'epoch': 2.2}          \n{'loss': 0.0688, 'learning_rate': 2.6864864864864865e-06, 'epoch': 2.21}        \n{'loss': 0.1924, 'learning_rate': 2.6594594594594596e-06, 'epoch': 2.22}        \n{'loss': 0.3024, 'learning_rate': 2.6324324324324322e-06, 'epoch': 2.22}        \n{'loss': 0.1154, 'learning_rate': 2.6054054054054057e-06, 'epoch': 2.23}        \n{'loss': 0.3498, 'learning_rate': 2.5783783783783784e-06, 'epoch': 2.24}        \n{'loss': 0.09, 'learning_rate': 2.5513513513513515e-06, 'epoch': 2.25}          \n{'loss': 0.19, 'learning_rate': 2.524324324324324e-06, 'epoch': 2.26}           \n{'loss': 0.2734, 'learning_rate': 2.4972972972972976e-06, 'epoch': 2.26}        \n{'loss': 0.13, 'learning_rate': 2.4702702702702707e-06, 'epoch': 2.27}          \n{'loss': 0.4626, 'learning_rate': 2.4432432432432433e-06, 'epoch': 2.28}        \n{'loss': 0.0753, 'learning_rate': 2.4162162162162164e-06, 'epoch': 2.29}        \n{'loss': 0.2001, 'learning_rate': 2.3891891891891895e-06, 'epoch': 2.3}         \n{'loss': 0.3219, 'learning_rate': 2.3621621621621625e-06, 'epoch': 2.3}         \n{'loss': 0.1329, 'learning_rate': 2.335135135135135e-06, 'epoch': 2.31}         \n{'loss': 0.3328, 'learning_rate': 2.3081081081081082e-06, 'epoch': 2.32}        \n{'loss': 0.0801, 'learning_rate': 2.2810810810810813e-06, 'epoch': 2.33}        \n{'loss': 0.2073, 'learning_rate': 2.2540540540540544e-06, 'epoch': 2.34}        \n{'loss': 0.2244, 'learning_rate': 2.227027027027027e-06, 'epoch': 2.34}         \n{'loss': 0.0971, 'learning_rate': 2.2e-06, 'epoch': 2.35}                       \n{'loss': 0.4116, 'learning_rate': 2.172972972972973e-06, 'epoch': 2.36}         \n{'loss': 0.0647, 'learning_rate': 2.1459459459459463e-06, 'epoch': 2.37}        \n{'loss': 0.1541, 'learning_rate': 2.118918918918919e-06, 'epoch': 2.38}         \n{'loss': 0.2517, 'learning_rate': 2.091891891891892e-06, 'epoch': 2.38}         \n{'loss': 0.098, 'learning_rate': 2.064864864864865e-06, 'epoch': 2.39}          \n{'loss': 0.3377, 'learning_rate': 2.037837837837838e-06, 'epoch': 2.4}          \n{'loss': 0.0898, 'learning_rate': 2.0108108108108108e-06, 'epoch': 2.41}        \n{'loss': 0.1781, 'learning_rate': 1.983783783783784e-06, 'epoch': 2.42}         \n{'loss': 0.2754, 'learning_rate': 1.956756756756757e-06, 'epoch': 2.42}         \n{'loss': 0.1224, 'learning_rate': 1.92972972972973e-06, 'epoch': 2.43}          \n{'loss': 0.3222, 'learning_rate': 1.9027027027027028e-06, 'epoch': 2.44}        \n{'loss': 0.0943, 'learning_rate': 1.875675675675676e-06, 'epoch': 2.45}         \n{'loss': 0.2085, 'learning_rate': 1.8486486486486488e-06, 'epoch': 2.46}        \n{'loss': 0.2773, 'learning_rate': 1.8216216216216218e-06, 'epoch': 2.46}        \n{'loss': 0.115, 'learning_rate': 1.7945945945945947e-06, 'epoch': 2.47}         \n{'loss': 0.3899, 'learning_rate': 1.7675675675675678e-06, 'epoch': 2.48}        \n{'loss': 0.0685, 'learning_rate': 1.7405405405405406e-06, 'epoch': 2.49}        \n{'loss': 0.149, 'learning_rate': 1.7135135135135137e-06, 'epoch': 2.5}          \n{'loss': 0.1916, 'learning_rate': 1.6864864864864866e-06, 'epoch': 2.5}         \n{'loss': 0.0913, 'learning_rate': 1.6594594594594596e-06, 'epoch': 2.51}        \n{'loss': 0.3644, 'learning_rate': 1.6324324324324325e-06, 'epoch': 2.52}        \n{'loss': 0.0778, 'learning_rate': 1.6054054054054056e-06, 'epoch': 2.53}        \n{'loss': 0.2165, 'learning_rate': 1.5783783783783784e-06, 'epoch': 2.54}        \n{'loss': 0.2567, 'learning_rate': 1.5513513513513515e-06, 'epoch': 2.54}        \n{'loss': 0.1271, 'learning_rate': 1.5243243243243244e-06, 'epoch': 2.55}        \n{'loss': 0.3561, 'learning_rate': 1.4972972972972974e-06, 'epoch': 2.56}        \n{'loss': 0.0745, 'learning_rate': 1.4702702702702703e-06, 'epoch': 2.57}        \n{'loss': 0.1663, 'learning_rate': 1.4432432432432434e-06, 'epoch': 2.58}        \n{'loss': 0.2656, 'learning_rate': 1.4162162162162162e-06, 'epoch': 2.58}        \n{'loss': 0.1093, 'learning_rate': 1.3891891891891893e-06, 'epoch': 2.59}        \n{'loss': 0.3962, 'learning_rate': 1.3621621621621622e-06, 'epoch': 2.6}         \n{'loss': 0.073, 'learning_rate': 1.3351351351351352e-06, 'epoch': 2.61}         \n{'loss': 0.1928, 'learning_rate': 1.308108108108108e-06, 'epoch': 2.62}         \n{'loss': 0.2848, 'learning_rate': 1.2810810810810812e-06, 'epoch': 2.62}        \n{'loss': 0.099, 'learning_rate': 1.254054054054054e-06, 'epoch': 2.63}          \n{'loss': 0.3096, 'learning_rate': 1.227027027027027e-06, 'epoch': 2.64}         \n{'loss': 0.0777, 'learning_rate': 1.2000000000000002e-06, 'epoch': 2.65}        \n{'loss': 0.1413, 'learning_rate': 1.172972972972973e-06, 'epoch': 2.66}         \n{'loss': 0.2275, 'learning_rate': 1.145945945945946e-06, 'epoch': 2.66}         \n{'loss': 0.1013, 'learning_rate': 1.118918918918919e-06, 'epoch': 2.67}         \n{'loss': 0.3662, 'learning_rate': 1.091891891891892e-06, 'epoch': 2.68}         \n{'loss': 0.1025, 'learning_rate': 1.0648648648648649e-06, 'epoch': 2.69}        \n{'loss': 0.2241, 'learning_rate': 1.037837837837838e-06, 'epoch': 2.7}          \n{'loss': 0.2568, 'learning_rate': 1.0108108108108108e-06, 'epoch': 2.7}         \n{'loss': 0.1022, 'learning_rate': 9.837837837837839e-07, 'epoch': 2.71}         \n{'loss': 0.3698, 'learning_rate': 9.567567567567567e-07, 'epoch': 2.72}         \n{'loss': 0.0896, 'learning_rate': 9.297297297297297e-07, 'epoch': 2.73}         \n{'loss': 0.1745, 'learning_rate': 9.027027027027027e-07, 'epoch': 2.74}         \n{'loss': 0.307, 'learning_rate': 8.756756756756756e-07, 'epoch': 2.74}          \n{'loss': 0.1207, 'learning_rate': 8.486486486486486e-07, 'epoch': 2.75}         \n{'loss': 0.4096, 'learning_rate': 8.216216216216217e-07, 'epoch': 2.76}         \n{'loss': 0.113, 'learning_rate': 7.945945945945946e-07, 'epoch': 2.77}          \n{'loss': 0.1625, 'learning_rate': 7.675675675675676e-07, 'epoch': 2.78}         \n{'loss': 0.2797, 'learning_rate': 7.405405405405407e-07, 'epoch': 2.78}         \n{'loss': 0.0858, 'learning_rate': 7.135135135135136e-07, 'epoch': 2.79}         \n{'loss': 0.3883, 'learning_rate': 6.864864864864866e-07, 'epoch': 2.8}          \n{'loss': 0.0713, 'learning_rate': 6.594594594594596e-07, 'epoch': 2.81}         \n{'loss': 0.1283, 'learning_rate': 6.324324324324325e-07, 'epoch': 2.82}         \n{'loss': 0.2606, 'learning_rate': 6.054054054054054e-07, 'epoch': 2.82}         \n{'loss': 0.1228, 'learning_rate': 5.783783783783784e-07, 'epoch': 2.83}         \n{'loss': 0.3231, 'learning_rate': 5.513513513513514e-07, 'epoch': 2.84}         \n{'loss': 0.0856, 'learning_rate': 5.243243243243244e-07, 'epoch': 2.85}         \n{'loss': 0.2075, 'learning_rate': 4.972972972972974e-07, 'epoch': 2.86}         \n{'loss': 0.2664, 'learning_rate': 4.702702702702703e-07, 'epoch': 2.86}         \n{'loss': 0.1004, 'learning_rate': 4.4324324324324325e-07, 'epoch': 2.87}        \n{'loss': 0.3807, 'learning_rate': 4.162162162162162e-07, 'epoch': 2.88}         \n{'loss': 0.0752, 'learning_rate': 3.891891891891892e-07, 'epoch': 2.89}         \n{'loss': 0.1687, 'learning_rate': 3.6216216216216214e-07, 'epoch': 2.9}         \n{'loss': 0.2923, 'learning_rate': 3.3513513513513516e-07, 'epoch': 2.9}         \n{'loss': 0.0758, 'learning_rate': 3.0810810810810813e-07, 'epoch': 2.91}        \n{'loss': 0.3403, 'learning_rate': 2.810810810810811e-07, 'epoch': 2.92}         \n{'loss': 0.0923, 'learning_rate': 2.5405405405405406e-07, 'epoch': 2.93}        \n{'loss': 0.2616, 'learning_rate': 2.2702702702702705e-07, 'epoch': 2.94}        \n{'loss': 0.2361, 'learning_rate': 2.0000000000000002e-07, 'epoch': 2.94}        \n{'loss': 0.129, 'learning_rate': 1.7297297297297298e-07, 'epoch': 2.95}         \n{'loss': 0.3727, 'learning_rate': 1.4594594594594595e-07, 'epoch': 2.96}        \n{'loss': 0.0719, 'learning_rate': 1.1891891891891893e-07, 'epoch': 2.97}        \n{'loss': 0.162, 'learning_rate': 9.189189189189189e-08, 'epoch': 2.98}          \n{'loss': 0.2612, 'learning_rate': 6.486486486486487e-08, 'epoch': 2.98}         \n{'loss': 0.1019, 'learning_rate': 3.783783783783784e-08, 'epoch': 2.99}         \n{'loss': 0.4059, 'learning_rate': 1.0810810810810811e-08, 'epoch': 3.0}         \n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [2:46:20<00:00,  1.85s/it]\n  0%|                                                    | 0/63 [00:00<?, ?it/s]\u001b[A\n  3%|â–ˆâ–                                          | 2/63 [00:09<04:56,  4.86s/it]\u001b[A\n  5%|â–ˆâ–ˆ                                          | 3/63 [00:20<07:24,  7.40s/it]\u001b[A\n  6%|â–ˆâ–ˆâ–Š                                         | 4/63 [00:32<08:57,  9.10s/it]\u001b[A\n  8%|â–ˆâ–ˆâ–ˆâ–                                        | 5/63 [00:42<09:04,  9.38s/it]\u001b[A\n 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 6/63 [00:54<09:43, 10.23s/it]\u001b[A\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰                                       | 7/63 [01:05<09:55, 10.64s/it]\u001b[A\n 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                      | 8/63 [01:16<09:37, 10.49s/it]\u001b[A\n 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 9/63 [01:25<09:09, 10.18s/it]\u001b[A\n 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                    | 10/63 [01:36<09:08, 10.34s/it]\u001b[A\n 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                   | 11/63 [01:45<08:38,  9.98s/it]\u001b[A\n 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 12/63 [01:55<08:28,  9.97s/it]\u001b[A\n 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 13/63 [02:05<08:23, 10.06s/it]\u001b[A\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                 | 14/63 [02:16<08:24, 10.30s/it]\u001b[A\n 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 15/63 [02:26<08:08, 10.19s/it]\u001b[A\n 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                | 16/63 [02:38<08:23, 10.72s/it]\u001b[A\n 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                               | 17/63 [02:48<07:58, 10.41s/it]\u001b[A\n 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 18/63 [02:57<07:39, 10.21s/it]\u001b[A\n 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                              | 19/63 [03:15<09:13, 12.57s/it]\u001b[A\n 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                             | 20/63 [03:27<08:48, 12.30s/it]\u001b[A\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 21/63 [03:37<08:02, 11.48s/it]\u001b[A\n 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 22/63 [03:48<07:53, 11.54s/it]\u001b[A\n 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 23/63 [03:59<07:28, 11.20s/it]\u001b[A\n 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 24/63 [04:10<07:19, 11.27s/it]\u001b[A\n 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 25/63 [04:21<07:03, 11.15s/it]\u001b[A\n 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 26/63 [04:32<06:53, 11.19s/it]\u001b[A\n 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 27/63 [04:42<06:26, 10.72s/it]\u001b[A\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 28/63 [04:53<06:19, 10.85s/it]\u001b[A\n 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 29/63 [05:02<05:52, 10.36s/it]\u001b[A\n 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 30/63 [05:13<05:46, 10.50s/it]\u001b[A\n 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 31/63 [05:27<06:03, 11.37s/it]\u001b[A\n 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     | 32/63 [05:36<05:36, 10.87s/it]\u001b[A\n 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 33/63 [05:47<05:24, 10.80s/it]\u001b[A\n 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 34/63 [05:59<05:28, 11.33s/it]\u001b[A\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 35/63 [06:09<05:03, 10.83s/it]\u001b[A\n 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 36/63 [06:22<05:12, 11.58s/it]\u001b[A\n 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 37/63 [06:33<04:50, 11.16s/it]\u001b[A\n 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 38/63 [06:42<04:25, 10.61s/it]\u001b[A\n 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                | 39/63 [06:53<04:20, 10.86s/it]\u001b[A\n 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 40/63 [07:04<04:06, 10.70s/it]\u001b[A\n 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 41/63 [07:14<03:55, 10.70s/it]\u001b[A\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 42/63 [07:27<03:58, 11.35s/it]\u001b[A\n 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 43/63 [07:37<03:36, 10.85s/it]\u001b[A\n 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 44/63 [07:46<03:18, 10.43s/it]\u001b[A\n 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 45/63 [07:58<03:12, 10.71s/it]\u001b[A\n 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 46/63 [08:09<03:02, 10.73s/it]\u001b[A\n 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 47/63 [08:20<02:53, 10.82s/it]\u001b[A\n 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 48/63 [08:31<02:43, 10.90s/it]\u001b[A\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 49/63 [08:42<02:35, 11.09s/it]\u001b[A\n 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 50/63 [08:52<02:19, 10.73s/it]\u001b[A\n 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 51/63 [09:03<02:10, 10.84s/it]\u001b[A\n 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 52/63 [09:15<02:02, 11.15s/it]\u001b[A\n 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 53/63 [09:25<01:48, 10.81s/it]\u001b[A\n 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 54/63 [09:36<01:37, 10.85s/it]\u001b[A\n 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 55/63 [09:49<01:30, 11.36s/it]\u001b[A\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 56/63 [10:01<01:22, 11.75s/it]\u001b[A\n 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 57/63 [10:12<01:09, 11.56s/it]\u001b[A\n 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 58/63 [10:24<00:57, 11.50s/it]\u001b[A\n 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 59/63 [10:34<00:44, 11.12s/it]\u001b[A\n 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 60/63 [10:44<00:32, 10.93s/it]\u001b[A\n 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 61/63 [11:00<00:24, 12.17s/it]\u001b[A\n 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 62/63 [11:09<00:11, 11.23s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.25590652227401733, 'eval_wer': 0.5291378074994029, 'eval_runtime': 695.2844, 'eval_samples_per_second': 1.438, 'eval_steps_per_second': 0.091, 'epoch': 3.0}\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [2:57:55<00:00,  1.85s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [11:16<00:00, 10.04s/it]\u001b[A\n{'train_runtime': 10689.4081, 'train_samples_per_second': 2.807, 'train_steps_per_second': 0.351, 'train_loss': 0.27922073413530984, 'epoch': 3.0}\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [2:58:09<00:00,  2.85s/it]\n***** train metrics *****\n  epoch                    =        3.0\n  train_loss               =     0.2792\n  train_runtime            = 2:58:09.40\n  train_samples            =      10000\n  train_samples_per_second =      2.807\n  train_steps_per_second   =      0.351\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [11:12<00:00, 10.68s/it]\n***** eval metrics *****\n  epoch                   =        3.0\n  eval_loss               =     0.2559\n  eval_runtime            = 0:11:30.05\n  eval_samples            =       1000\n  eval_samples_per_second =      1.449\n  eval_steps_per_second   =      0.091\n  eval_wer                =     0.5291\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}