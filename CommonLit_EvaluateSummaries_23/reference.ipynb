{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":3904.532124,"end_time":"2023-07-12T20:02:00.433169","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-07-12T18:56:55.901045","version":"2.4.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"006eacd4366548cfaa9ddf17fee7e90f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0594d57c99124f29bafb370ca2e8d00e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_38d0446019cd422ca77ee83d6091c33e","placeholder":"​","style":"IPY_MODEL_46481dcde16d4c5a8460ce1dce5bb39a","value":" 7165/7165 [00:02&lt;00:00, 3155.90it/s]"}},"061613ba502c46ed9ef12aa14404c656":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20c4fc3ba85445c6958a54189c1fbad3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2313a820078f410f84b25bd35b3565d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"26eba1b0bfad4f1d86e8a8209759a5d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_20c4fc3ba85445c6958a54189c1fbad3","placeholder":"​","style":"IPY_MODEL_7aefb7d32bcc43f289602a2fd71e1146","value":"Downloading (…)lve/main/config.json: 100%"}},"2735154e616a469da8524bc5dcd92d1f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2c91894e264e411a8edea0bebaeec93b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ce22175883048b8a5cad99ebe886800":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e60e876f8054470a29d3249763df1fd","placeholder":"​","style":"IPY_MODEL_eacb98d83fc742959faf04c9f8b54818","value":" 52.0/52.0 [00:00&lt;00:00, 3.55kB/s]"}},"38d0446019cd422ca77ee83d6091c33e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e60e876f8054470a29d3249763df1fd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"444e95d1eed548bbb64339cfda3c9e30":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fef5ba86e9f48f6bb664be8d3a60dda","placeholder":"​","style":"IPY_MODEL_2c91894e264e411a8edea0bebaeec93b","value":" 579/579 [00:00&lt;00:00, 42.2kB/s]"}},"46481dcde16d4c5a8460ce1dce5bb39a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"48f7b44c9bf9414dab534c2150236b00":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_524158d9d5bd4d98ba1d3a6320baa641","IPY_MODEL_c9617f1f10a94797b60fb8df656e0218","IPY_MODEL_2ce22175883048b8a5cad99ebe886800"],"layout":"IPY_MODEL_f34c8285422c43ea9e298e05748650ef"}},"524158d9d5bd4d98ba1d3a6320baa641":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9bc6d658e23424c8a553d3f1b373544","placeholder":"​","style":"IPY_MODEL_8578fe21650643748145c6ce56e6a8ac","value":"Downloading (…)okenizer_config.json: 100%"}},"53b4bff155dd4fda9b49d1d0ad08a6a6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55008084828c4de3bf617f59873c767d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc9c3cd212ee4e35b1d87e8529ece826","placeholder":"​","style":"IPY_MODEL_afd9c754cb8242f9a5eded75e4c6bf2d","value":" 371M/371M [00:01&lt;00:00, 367MB/s]"}},"556b72893c4e441c922c57510d54dc17":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"557f7b1f7e814a8c95848bb7e7c48f12":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e72f6946520d4172bac02beee324fc05","IPY_MODEL_78b271b1d17e4d608a474439c19cd0f8","IPY_MODEL_55008084828c4de3bf617f59873c767d"],"layout":"IPY_MODEL_99f53794c1f544f886415080b4f552a5"}},"5e97b6e3c00e4167a68588003e9365e4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"610e0a406e504081add04d4593a582ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6fef5ba86e9f48f6bb664be8d3a60dda":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"722eb83c79424b3a869d42c5197e61ce":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"731f0254f3744a148c9c1144330ddaac":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"734bb350e7cf48129acd7d95613d0f1e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_26eba1b0bfad4f1d86e8a8209759a5d6","IPY_MODEL_7d036870db624916be81d8ab66085893","IPY_MODEL_444e95d1eed548bbb64339cfda3c9e30"],"layout":"IPY_MODEL_5e97b6e3c00e4167a68588003e9365e4"}},"77d54f716e884156a93bd0e33a8d357a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78b271b1d17e4d608a474439c19cd0f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff4308df7fa7413aa4d62b80b3d360be","max":371146213,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f5c94ce411754b6a9478153e95eaa375","value":371146213}},"7aefb7d32bcc43f289602a2fd71e1146":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7d036870db624916be81d8ab66085893":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4dc646ca52b4d15be0388764a62a530","max":579,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f0335159e02f4bcbaf0ed03e81b37c82","value":579}},"7e668f44f0ab4b0a8314e810d8f5ada8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_731f0254f3744a148c9c1144330ddaac","max":7165,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2735154e616a469da8524bc5dcd92d1f","value":7165}},"837c01c3a677493ca40dab52648f05f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbd0160422974822b71a4458b8dc3b5c","placeholder":"​","style":"IPY_MODEL_b9aa58ad31bc4fab9ff421ddd8b5795c","value":"Downloading spm.model: 100%"}},"8578fe21650643748145c6ce56e6a8ac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"99f53794c1f544f886415080b4f552a5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4dc646ca52b4d15be0388764a62a530":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab33fd12dc2a4879a06d12e32c87a1a5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"afd9c754cb8242f9a5eded75e4c6bf2d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b1ee5646c8cb4297a03c2fdef62ac648":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_837c01c3a677493ca40dab52648f05f0","IPY_MODEL_efe88537d2b34c4f9c5ac3bedc2e7427","IPY_MODEL_c4a568f9b9f44c368e4b7ee349780c0f"],"layout":"IPY_MODEL_ab33fd12dc2a4879a06d12e32c87a1a5"}},"b9aa58ad31bc4fab9ff421ddd8b5795c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b9bc6d658e23424c8a553d3f1b373544":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc9c3cd212ee4e35b1d87e8529ece826":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bee56aa1633b4f4cbeaf9455c8f7ce65":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4a568f9b9f44c368e4b7ee349780c0f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53b4bff155dd4fda9b49d1d0ad08a6a6","placeholder":"​","style":"IPY_MODEL_e133e30189934de28ac2ec5746d63e50","value":" 2.46M/2.46M [00:00&lt;00:00, 53.1MB/s]"}},"c9617f1f10a94797b60fb8df656e0218":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_77d54f716e884156a93bd0e33a8d357a","max":52,"min":0,"orientation":"horizontal","style":"IPY_MODEL_610e0a406e504081add04d4593a582ca","value":52}},"d85d809ca57b47858875e6ac84c140c1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbd0160422974822b71a4458b8dc3b5c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e133e30189934de28ac2ec5746d63e50":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3cf208b38db4fdd896dc68af00a03a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bee56aa1633b4f4cbeaf9455c8f7ce65","placeholder":"​","style":"IPY_MODEL_2313a820078f410f84b25bd35b3565d2","value":"100%"}},"e72f6946520d4172bac02beee324fc05":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_722eb83c79424b3a869d42c5197e61ce","placeholder":"​","style":"IPY_MODEL_006eacd4366548cfaa9ddf17fee7e90f","value":"Downloading pytorch_model.bin: 100%"}},"eacb98d83fc742959faf04c9f8b54818":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ecb9e26c57f1481fad163de50959f8a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e3cf208b38db4fdd896dc68af00a03a2","IPY_MODEL_7e668f44f0ab4b0a8314e810d8f5ada8","IPY_MODEL_0594d57c99124f29bafb370ca2e8d00e"],"layout":"IPY_MODEL_061613ba502c46ed9ef12aa14404c656"}},"efe88537d2b34c4f9c5ac3bedc2e7427":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d85d809ca57b47858875e6ac84c140c1","max":2464616,"min":0,"orientation":"horizontal","style":"IPY_MODEL_556b72893c4e441c922c57510d54dc17","value":2464616}},"f0335159e02f4bcbaf0ed03e81b37c82":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f34c8285422c43ea9e298e05748650ef":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5c94ce411754b6a9478153e95eaa375":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ff4308df7fa7413aa4d62b80b3d360be":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About this notebook\n- This notebook is a modified version of the PyTorch pipeline from Y.Nakama's starter NLP notebook from Feedback Prize 3 competition [here](https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train). Don't forget to upvote his work!\n- Inference notebook is [here](https://www.kaggle.com/mohammad2012191/debertav3-pytorch-baseline-inference-cv-0-467)","metadata":{"papermill":{"duration":0.008353,"end_time":"2023-07-12T18:57:05.687020","exception":false,"start_time":"2023-07-12T18:57:05.678667","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Directory settings","metadata":{"papermill":{"duration":0.007406,"end_time":"2023-07-12T18:57:05.702253","exception":false,"start_time":"2023-07-12T18:57:05.694847","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Directory settings\n# ====================================================\nimport os\n\nOUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)","metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:57:05.719772Z","iopub.status.busy":"2023-07-12T18:57:05.719332Z","iopub.status.idle":"2023-07-12T18:57:05.732122Z","shell.execute_reply":"2023-07-12T18:57:05.731292Z"},"papermill":{"duration":0.023884,"end_time":"2023-07-12T18:57:05.734092","exception":false,"start_time":"2023-07-12T18:57:05.710208","status":"completed"},"tags":[]},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# CFG","metadata":{"papermill":{"duration":0.007611,"end_time":"2023-07-12T18:57:05.749173","exception":false,"start_time":"2023-07-12T18:57:05.741562","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    wandb=False\n    competition='FB3'\n    _wandb_kernel='nakama'\n    debug=False\n    apex=True\n    print_freq=20\n    num_workers=4\n    model=\"microsoft/deberta-v3-base\"\n    gradient_checkpointing=True\n    scheduler='cosine' # ['linear', 'cosine']\n    batch_scheduler=True\n    num_cycles=0.5\n    num_warmup_steps=0\n    epochs=4\n    encoder_lr=2e-5\n    decoder_lr=2e-5\n    min_lr=1e-6\n    eps=1e-6\n    betas=(0.9, 0.999)\n    batch_size=8\n    max_len=512\n    weight_decay=0.01\n    gradient_accumulation_steps=1\n    max_grad_norm=1000\n    target_cols=['content', 'wording']\n    seed=42\n    n_fold=4\n    trn_fold=[0, 1, 2, 3]\n    train=True\n    \nif CFG.debug:\n    CFG.epochs = 2\n    CFG.trn_fold = [0]","metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:57:05.768495Z","iopub.status.busy":"2023-07-12T18:57:05.768214Z","iopub.status.idle":"2023-07-12T18:57:05.775499Z","shell.execute_reply":"2023-07-12T18:57:05.774494Z"},"papermill":{"duration":0.018034,"end_time":"2023-07-12T18:57:05.777672","exception":false,"start_time":"2023-07-12T18:57:05.759638","status":"completed"},"tags":[]},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# wandb\n# ====================================================\nif CFG.wandb:\n    \n    import wandb\n\n    try:\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n        secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n        wandb.login(key=secret_value_0)\n        anony = None\n    except:\n        anony = \"must\"\n        print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n\n\n    def class2dict(f):\n        return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n\n    run = wandb.init(project='FB3-Public', \n                     name=CFG.model,\n                     config=class2dict(CFG),\n                     group=CFG.model,\n                     job_type=\"train\",\n                     anonymous=anony)","metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:57:05.795181Z","iopub.status.busy":"2023-07-12T18:57:05.793692Z","iopub.status.idle":"2023-07-12T18:57:05.801265Z","shell.execute_reply":"2023-07-12T18:57:05.800361Z"},"papermill":{"duration":0.017945,"end_time":"2023-07-12T18:57:05.803221","exception":false,"start_time":"2023-07-12T18:57:05.785276","status":"completed"},"tags":[]},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Library","metadata":{"papermill":{"duration":0.007388,"end_time":"2023-07-12T18:57:05.818090","exception":false,"start_time":"2023-07-12T18:57:05.810702","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Library\n# ====================================================\nimport os\nimport gc\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nos.system('pip install iterative-stratification==0.1.7')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\nos.system('pip install -q transformers')\nos.system('pip install -q tokenizers')\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n%env TOKENIZERS_PARALLELISM=true\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:57:05.834229Z","iopub.status.busy":"2023-07-12T18:57:05.833930Z","iopub.status.idle":"2023-07-12T18:57:49.763324Z","shell.execute_reply":"2023-07-12T18:57:49.762133Z"},"papermill":{"duration":43.940064,"end_time":"2023-07-12T18:57:49.765590","exception":false,"start_time":"2023-07-12T18:57:05.825526","status":"completed"},"tags":[]},"execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":"Collecting iterative-stratification==0.1.7\n\n  Downloading iterative_stratification-0.1.7-py3-none-any.whl (8.5 kB)\n\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from iterative-stratification==0.1.7) (1.23.5)\n\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from iterative-stratification==0.1.7) (1.11.1)\n\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from iterative-stratification==0.1.7) (1.2.2)\n\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->iterative-stratification==0.1.7) (1.2.0)\n\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->iterative-stratification==0.1.7) (3.1.0)\n\nInstalling collected packages: iterative-stratification\n\nSuccessfully installed iterative-stratification-0.1.7\n\ntokenizers.__version__: 0.13.3\n\ntransformers.__version__: 4.30.2\n\nenv: TOKENIZERS_PARALLELISM=true\n"}]},{"cell_type":"markdown","source":"# Utils","metadata":{"papermill":{"duration":0.007785,"end_time":"2023-07-12T18:57:49.781404","exception":false,"start_time":"2023-07-12T18:57:49.773619","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Utils\n# ====================================================\ndef MCRMSE(y_trues, y_preds):\n    scores = []\n    idxes = y_trues.shape[1]\n    for i in range(idxes):\n        y_true = y_trues[:,i]\n        y_pred = y_preds[:,i]\n        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n        scores.append(score)\n    mcrmse_score = np.mean(scores)\n    return mcrmse_score, scores\n\n\ndef get_score(y_trues, y_preds):\n    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n    return mcrmse_score, scores\n\n\ndef get_logger(filename=OUTPUT_DIR+'train'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()\n\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:57:49.798713Z","iopub.status.busy":"2023-07-12T18:57:49.798142Z","iopub.status.idle":"2023-07-12T18:57:49.818606Z","shell.execute_reply":"2023-07-12T18:57:49.817748Z"},"papermill":{"duration":0.031391,"end_time":"2023-07-12T18:57:49.820629","exception":false,"start_time":"2023-07-12T18:57:49.789238","status":"completed"},"tags":[]},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading","metadata":{"papermill":{"duration":0.007898,"end_time":"2023-07-12T18:57:49.836243","exception":false,"start_time":"2023-07-12T18:57:49.828345","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Data Loading\n# ====================================================\ntrain = pd.read_csv('../input/commonlit-evaluate-student-summaries/summaries_train.csv')\ntest = pd.read_csv('../input/commonlit-evaluate-student-summaries/summaries_test.csv')\nsubmission = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/sample_submission.csv')\n\nprint(f\"train.shape: {train.shape}\")\ndisplay(train.head())\nprint(f\"test.shape: {test.shape}\")\ndisplay(test.head())\nprint(f\"submission.shape: {submission.shape}\")\ndisplay(submission.head())","metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:57:49.853252Z","iopub.status.busy":"2023-07-12T18:57:49.852568Z","iopub.status.idle":"2023-07-12T18:57:49.979501Z","shell.execute_reply":"2023-07-12T18:57:49.978626Z"},"papermill":{"duration":0.141389,"end_time":"2023-07-12T18:57:49.985355","exception":false,"start_time":"2023-07-12T18:57:49.843966","status":"completed"},"tags":[]},"execution_count":6,"outputs":[{"name":"stdout","output_type":"stream","text":"train.shape: (7165, 5)\n"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>prompt_id</th>\n","      <th>text</th>\n","      <th>content</th>\n","      <th>wording</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000e8c3c7ddb</td>\n","      <td>814d6b</td>\n","      <td>The third wave was an experimentto see how peo...</td>\n","      <td>0.205683</td>\n","      <td>0.380538</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0020ae56ffbf</td>\n","      <td>ebad26</td>\n","      <td>They would rub it up with soda to make the sme...</td>\n","      <td>-0.548304</td>\n","      <td>0.506755</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>004e978e639e</td>\n","      <td>3b9047</td>\n","      <td>In Egypt, there were many occupations and soci...</td>\n","      <td>3.128928</td>\n","      <td>4.231226</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>005ab0199905</td>\n","      <td>3b9047</td>\n","      <td>The highest class was Pharaohs these people we...</td>\n","      <td>-0.210614</td>\n","      <td>-0.471415</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0070c9e7af47</td>\n","      <td>814d6b</td>\n","      <td>The Third Wave developed  rapidly because the ...</td>\n","      <td>3.272894</td>\n","      <td>3.219757</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     student_id prompt_id                                               text   content   wording\n","0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...  0.205683  0.380538\n","1  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme... -0.548304  0.506755\n","2  004e978e639e    3b9047  In Egypt, there were many occupations and soci...  3.128928  4.231226\n","3  005ab0199905    3b9047  The highest class was Pharaohs these people we... -0.210614 -0.471415\n","4  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...  3.272894  3.219757"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"test.shape: (4, 3)\n"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>prompt_id</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000000ffffff</td>\n","      <td>abc123</td>\n","      <td>Example text 1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>111111eeeeee</td>\n","      <td>def789</td>\n","      <td>Example text 2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>222222cccccc</td>\n","      <td>abc123</td>\n","      <td>Example text 3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>333333dddddd</td>\n","      <td>def789</td>\n","      <td>Example text 4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     student_id prompt_id            text\n","0  000000ffffff    abc123  Example text 1\n","1  111111eeeeee    def789  Example text 2\n","2  222222cccccc    abc123  Example text 3\n","3  333333dddddd    def789  Example text 4"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"submission.shape: (4, 3)\n"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>content</th>\n","      <th>wording</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000000ffffff</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>111111eeeeee</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>222222cccccc</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>333333dddddd</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     student_id  content  wording\n","0  000000ffffff      0.0      0.0\n","1  111111eeeeee      0.0      0.0\n","2  222222cccccc      0.0      0.0\n","3  333333dddddd      0.0      0.0"]},"metadata":{}}]},{"cell_type":"markdown","source":"# CV split","metadata":{"papermill":{"duration":0.00864,"end_time":"2023-07-12T18:57:50.003107","exception":false,"start_time":"2023-07-12T18:57:49.994467","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# CV split\n# ====================================================\nFold = MultilabelStratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\nfor n, (train_index, val_index) in enumerate(Fold.split(train, train[CFG.target_cols])):\n    train.loc[val_index, 'fold'] = int(n)\ntrain['fold'] = train['fold'].astype(int)\ndisplay(train.groupby('fold').size())","metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:57:50.021734Z","iopub.status.busy":"2023-07-12T18:57:50.021448Z","iopub.status.idle":"2023-07-12T18:57:50.332582Z","shell.execute_reply":"2023-07-12T18:57:50.331686Z"},"papermill":{"duration":0.322909,"end_time":"2023-07-12T18:57:50.334680","exception":false,"start_time":"2023-07-12T18:57:50.011771","status":"completed"},"tags":[]},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":["fold\n","0    1791\n","1    1791\n","2    1792\n","3    1791\n","dtype: int64"]},"metadata":{}}]},{"cell_type":"code","source":"if CFG.debug:\n    display(train.groupby('fold').size())\n    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n    display(train.groupby('fold').size())","metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:57:50.353718Z","iopub.status.busy":"2023-07-12T18:57:50.353443Z","iopub.status.idle":"2023-07-12T18:57:50.358865Z","shell.execute_reply":"2023-07-12T18:57:50.357891Z"},"papermill":{"duration":0.017361,"end_time":"2023-07-12T18:57:50.360934","exception":false,"start_time":"2023-07-12T18:57:50.343573","status":"completed"},"tags":[]},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# tokenizer","metadata":{"papermill":{"duration":0.008559,"end_time":"2023-07-12T18:57:50.378324","exception":false,"start_time":"2023-07-12T18:57:50.369765","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\ntokenizer = AutoTokenizer.from_pretrained(CFG.model)\ntokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\nCFG.tokenizer = tokenizer","metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:57:50.397779Z","iopub.status.busy":"2023-07-12T18:57:50.396980Z","iopub.status.idle":"2023-07-12T18:57:52.770741Z","shell.execute_reply":"2023-07-12T18:57:52.769710Z"},"papermill":{"duration":2.385784,"end_time":"2023-07-12T18:57:52.773141","exception":false,"start_time":"2023-07-12T18:57:50.387357","status":"completed"},"tags":[]},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"48f7b44c9bf9414dab534c2150236b00","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"734bb350e7cf48129acd7d95613d0f1e","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b1ee5646c8cb4297a03c2fdef62ac648","version_major":2,"version_minor":0},"text/plain":["Downloading spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"}]},{"cell_type":"markdown","source":"# Dataset","metadata":{"papermill":{"duration":0.009431,"end_time":"2023-07-12T18:57:52.792340","exception":false,"start_time":"2023-07-12T18:57:52.782909","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Define max_len\n# ====================================================\nlengths = []\ntk0 = tqdm(train['text'].fillna(\"\").values, total=len(train))\nfor text in tk0:\n    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n    lengths.append(length)\nCFG.max_len = max(lengths) + 2 # cls & sep\nLOGGER.info(f\"max_len: {CFG.max_len}\")","metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:57:52.812516Z","iopub.status.busy":"2023-07-12T18:57:52.812190Z","iopub.status.idle":"2023-07-12T18:57:55.168561Z","shell.execute_reply":"2023-07-12T18:57:55.167631Z"},"papermill":{"duration":2.369739,"end_time":"2023-07-12T18:57:55.171507","exception":false,"start_time":"2023-07-12T18:57:52.801768","status":"completed"},"tags":[]},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ecb9e26c57f1481fad163de50959f8a1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/7165 [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"max_len: 822\n"}]},{"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\ndef prepare_input(cfg, text):\n    inputs = cfg.tokenizer.encode_plus(\n        text, \n        return_tensors=None, \n        add_special_tokens=True, \n        max_length=CFG.max_len,\n        pad_to_max_length=True,\n        truncation=True\n    )\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\n\nclass TrainDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n        self.labels = df[cfg.target_cols].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        label = torch.tensor(self.labels[item], dtype=torch.float)\n        return inputs, label\n    \n\ndef collate(inputs):\n    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n    for k, v in inputs.items():\n        inputs[k] = inputs[k][:,:mask_len]\n    return inputs","metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:57:55.192740Z","iopub.status.busy":"2023-07-12T18:57:55.192458Z","iopub.status.idle":"2023-07-12T18:57:55.202840Z","shell.execute_reply":"2023-07-12T18:57:55.201949Z"},"papermill":{"duration":0.023109,"end_time":"2023-07-12T18:57:55.204841","exception":false,"start_time":"2023-07-12T18:57:55.181732","status":"completed"},"tags":[]},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"papermill":{"duration":0.009564,"end_time":"2023-07-12T18:57:55.224029","exception":false,"start_time":"2023-07-12T18:57:55.214465","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Model\n# ====================================================\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n    \n\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n            self.config.hidden_dropout = 0.\n            self.config.hidden_dropout_prob = 0.\n            self.config.attention_dropout = 0.\n            self.config.attention_probs_dropout_prob = 0.\n            LOGGER.info(self.config)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel(self.config)\n        if self.cfg.gradient_checkpointing:\n            self.model.gradient_checkpointing_enable()\n        self.pool = MeanPooling()\n        self.fc = nn.Linear(self.config.hidden_size, 2)\n        self._init_weights(self.fc)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(feature)\n        return output","metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:57:55.244581Z","iopub.status.busy":"2023-07-12T18:57:55.244302Z","iopub.status.idle":"2023-07-12T18:57:55.259175Z","shell.execute_reply":"2023-07-12T18:57:55.258355Z"},"papermill":{"duration":0.027453,"end_time":"2023-07-12T18:57:55.261060","exception":false,"start_time":"2023-07-12T18:57:55.233607","status":"completed"},"tags":[]},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Loss","metadata":{"papermill":{"duration":0.009443,"end_time":"2023-07-12T18:57:55.280318","exception":false,"start_time":"2023-07-12T18:57:55.270875","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Loss\n# ====================================================\nclass RMSELoss(nn.Module):\n    def __init__(self, reduction='mean', eps=1e-9):\n        super().__init__()\n        self.mse = nn.MSELoss(reduction='none')\n        self.reduction = reduction\n        self.eps = eps\n\n    def forward(self, y_pred, y_true):\n        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n        if self.reduction == 'none':\n            loss = loss\n        elif self.reduction == 'sum':\n            loss = loss.sum()\n        elif self.reduction == 'mean':\n            loss = loss.mean()\n        return loss","metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:57:55.301603Z","iopub.status.busy":"2023-07-12T18:57:55.300802Z","iopub.status.idle":"2023-07-12T18:57:55.307792Z","shell.execute_reply":"2023-07-12T18:57:55.306966Z"},"papermill":{"duration":0.019731,"end_time":"2023-07-12T18:57:55.309674","exception":false,"start_time":"2023-07-12T18:57:55.289943","status":"completed"},"tags":[]},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Helpler functions","metadata":{"papermill":{"duration":0.009512,"end_time":"2023-07-12T18:57:55.328765","exception":false,"start_time":"2023-07-12T18:57:55.319253","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Helper functions\n# ====================================================\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n\n\ndef train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n    model.train()\n    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n    losses = AverageMeter()\n    start = end = time.time()\n    global_step = 0\n    for step, (inputs, labels) in enumerate(train_loader):\n        inputs = collate(inputs)\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        with torch.cuda.amp.autocast(enabled=CFG.apex):\n            y_preds = model(inputs)\n            loss = criterion(y_preds, labels)\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss / CFG.gradient_accumulation_steps\n        losses.update(loss.item(), batch_size)\n        scaler.scale(loss).backward()\n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            global_step += 1\n            if CFG.batch_scheduler:\n                scheduler.step()\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n            print('Epoch: [{0}][{1}/{2}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  'Grad: {grad_norm:.4f}  '\n                  'LR: {lr:.8f}  '\n                  .format(epoch+1, step, len(train_loader), \n                          remain=timeSince(start, float(step+1)/len(train_loader)),\n                          loss=losses,\n                          grad_norm=grad_norm,\n                          lr=scheduler.get_lr()[0]))\n        if CFG.wandb:\n            wandb.log({f\"[fold{fold}] loss\": losses.val,\n                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n    return losses.avg\n\n\ndef valid_fn(valid_loader, model, criterion, device):\n    losses = AverageMeter()\n    model.eval()\n    preds = []\n    start = end = time.time()\n    for step, (inputs, labels) in enumerate(valid_loader):\n        inputs = collate(inputs)\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        with torch.no_grad():\n            y_preds = model(inputs)\n            loss = criterion(y_preds, labels)\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss / CFG.gradient_accumulation_steps\n        losses.update(loss.item(), batch_size)\n        preds.append(y_preds.to('cpu').numpy())\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n            print('EVAL: [{0}/{1}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  .format(step, len(valid_loader),\n                          loss=losses,\n                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n    predictions = np.concatenate(preds)\n    return losses.avg, predictions","metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:57:55.349280Z","iopub.status.busy":"2023-07-12T18:57:55.349032Z","iopub.status.idle":"2023-07-12T18:57:55.369160Z","shell.execute_reply":"2023-07-12T18:57:55.368229Z"},"papermill":{"duration":0.032719,"end_time":"2023-07-12T18:57:55.371176","exception":false,"start_time":"2023-07-12T18:57:55.338457","status":"completed"},"tags":[]},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# train loop","metadata":{"papermill":{"duration":0.009719,"end_time":"2023-07-12T18:57:55.390452","exception":false,"start_time":"2023-07-12T18:57:55.380733","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# train loop\n# ====================================================\ndef train_loop(folds, fold):\n    \n    LOGGER.info(f\"========== fold: {fold} training ==========\")\n\n    # ====================================================\n    # loader\n    # ====================================================\n    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n    valid_labels = valid_folds[CFG.target_cols].values\n    \n    train_dataset = TrainDataset(CFG, train_folds)\n    valid_dataset = TrainDataset(CFG, valid_folds)\n\n    train_loader = DataLoader(train_dataset,\n                              batch_size=CFG.batch_size,\n                              shuffle=True,\n                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset,\n                              batch_size=CFG.batch_size * 2,\n                              shuffle=False,\n                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n\n    # ====================================================\n    # model & optimizer\n    # ====================================================\n    model = CustomModel(CFG, config_path=None, pretrained=True)\n    torch.save(model.config, OUTPUT_DIR+'config.pth')\n    model.to(device)\n    \n    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n        param_optimizer = list(model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n             'lr': encoder_lr, 'weight_decay': weight_decay},\n            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n             'lr': encoder_lr, 'weight_decay': 0.0},\n            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n             'lr': decoder_lr, 'weight_decay': 0.0}\n        ]\n        return optimizer_parameters\n\n    optimizer_parameters = get_optimizer_params(model,\n                                                encoder_lr=CFG.encoder_lr, \n                                                decoder_lr=CFG.decoder_lr,\n                                                weight_decay=CFG.weight_decay)\n    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n    \n    # ====================================================\n    # scheduler\n    # ====================================================\n    def get_scheduler(cfg, optimizer, num_train_steps):\n        if cfg.scheduler == 'linear':\n            scheduler = get_linear_schedule_with_warmup(\n                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n            )\n        elif cfg.scheduler == 'cosine':\n            scheduler = get_cosine_schedule_with_warmup(\n                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n            )\n        return scheduler\n    \n    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n\n    # ====================================================\n    # loop\n    # ====================================================\n    criterion = nn.SmoothL1Loss(reduction='mean') # RMSELoss(reduction=\"mean\")\n    \n    best_score = np.inf\n\n    for epoch in range(CFG.epochs):\n\n        start_time = time.time()\n\n        # train\n        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n\n        # eval\n        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n        \n        # scoring\n        score, scores = get_score(valid_labels, predictions)\n\n        elapsed = time.time() - start_time\n\n        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {scores}')\n        if CFG.wandb:\n            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n                       f\"[fold{fold}] avg_train_loss\": avg_loss, \n                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n                       f\"[fold{fold}] score\": score})\n        \n        if best_score > score:\n            best_score = score\n            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n            torch.save({'model': model.state_dict(),\n                        'predictions': predictions},\n                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n\n    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n                             map_location=torch.device('cpu'))['predictions']\n    valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return valid_folds","metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:57:55.411092Z","iopub.status.busy":"2023-07-12T18:57:55.410830Z","iopub.status.idle":"2023-07-12T18:57:55.429905Z","shell.execute_reply":"2023-07-12T18:57:55.428925Z"},"papermill":{"duration":0.032097,"end_time":"2023-07-12T18:57:55.432214","exception":false,"start_time":"2023-07-12T18:57:55.400117","status":"completed"},"tags":[]},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    \n    def get_result(oof_df):\n        labels = oof_df[CFG.target_cols].values\n        preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n        score, scores = get_score(labels, preds)\n        LOGGER.info(f'Score: {score:<.4f}  Scores: {scores}')\n    \n    if CFG.train:\n        oof_df = pd.DataFrame()\n        for fold in range(CFG.n_fold):\n            if fold in CFG.trn_fold:\n                _oof_df = train_loop(train, fold)\n                oof_df = pd.concat([oof_df, _oof_df])\n                LOGGER.info(f\"========== fold: {fold} result ==========\")\n                get_result(_oof_df)\n        oof_df = oof_df.reset_index(drop=True)\n        LOGGER.info(f\"========== CV ==========\")\n        get_result(oof_df)\n        oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')\n        \n    if CFG.wandb:\n        wandb.finish()","metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:57:55.452565Z","iopub.status.busy":"2023-07-12T18:57:55.452309Z","iopub.status.idle":"2023-07-12T20:01:56.828686Z","shell.execute_reply":"2023-07-12T20:01:56.827757Z"},"papermill":{"duration":3841.388987,"end_time":"2023-07-12T20:01:56.830869","exception":false,"start_time":"2023-07-12T18:57:55.441882","status":"completed"},"tags":[]},"execution_count":16,"outputs":[{"name":"stderr","output_type":"stream","text":"========== fold: 0 training ==========\n\nDebertaV2Config {\n\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n\n  \"attention_dropout\": 0.0,\n\n  \"attention_probs_dropout_prob\": 0.0,\n\n  \"hidden_act\": \"gelu\",\n\n  \"hidden_dropout\": 0.0,\n\n  \"hidden_dropout_prob\": 0.0,\n\n  \"hidden_size\": 768,\n\n  \"initializer_range\": 0.02,\n\n  \"intermediate_size\": 3072,\n\n  \"layer_norm_eps\": 1e-07,\n\n  \"max_position_embeddings\": 512,\n\n  \"max_relative_positions\": -1,\n\n  \"model_type\": \"deberta-v2\",\n\n  \"norm_rel_ebd\": \"layer_norm\",\n\n  \"num_attention_heads\": 12,\n\n  \"num_hidden_layers\": 12,\n\n  \"output_hidden_states\": true,\n\n  \"pad_token_id\": 0,\n\n  \"pooler_dropout\": 0,\n\n  \"pooler_hidden_act\": \"gelu\",\n\n  \"pooler_hidden_size\": 768,\n\n  \"pos_att_type\": [\n\n    \"p2c\",\n\n    \"c2p\"\n\n  ],\n\n  \"position_biased_input\": false,\n\n  \"position_buckets\": 256,\n\n  \"relative_attention\": true,\n\n  \"share_att_key\": true,\n\n  \"transformers_version\": \"4.30.2\",\n\n  \"type_vocab_size\": 0,\n\n  \"vocab_size\": 128100\n\n}\n\n\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"557f7b1f7e814a8c95848bb7e7c48f12","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n\n- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n\n- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"},{"name":"stdout","output_type":"stream","text":"Epoch: [1][0/671] Elapsed 0m 2s (remain 26m 47s) Loss: 0.9646(0.9646) Grad: inf  LR: 0.00002000  \n\nEpoch: [1][20/671] Elapsed 0m 8s (remain 4m 23s) Loss: 0.7998(0.5248) Grad: 28478.7754  LR: 0.00002000  \n\nEpoch: [1][40/671] Elapsed 0m 14s (remain 3m 45s) Loss: 0.2424(0.4071) Grad: 26163.5176  LR: 0.00001999  \n\nEpoch: [1][60/671] Elapsed 0m 20s (remain 3m 27s) Loss: 0.2787(0.3422) Grad: 30125.7031  LR: 0.00001997  \n\nEpoch: [1][80/671] Elapsed 0m 28s (remain 3m 25s) Loss: 0.2080(0.3032) Grad: 30702.1699  LR: 0.00001996  \n\nEpoch: [1][100/671] Elapsed 0m 35s (remain 3m 18s) Loss: 0.1903(0.2720) Grad: 30658.7305  LR: 0.00001993  \n\nEpoch: [1][120/671] Elapsed 0m 41s (remain 3m 6s) Loss: 0.1274(0.2548) Grad: 12393.4619  LR: 0.00001990  \n\nEpoch: [1][140/671] Elapsed 0m 46s (remain 2m 55s) Loss: 0.2329(0.2422) Grad: 41183.6797  LR: 0.00001986  \n\nEpoch: [1][160/671] Elapsed 0m 53s (remain 2m 48s) Loss: 0.0709(0.2325) Grad: 9546.2188  LR: 0.00001982  \n\nEpoch: [1][180/671] Elapsed 0m 59s (remain 2m 41s) Loss: 0.1923(0.2267) Grad: 21018.9883  LR: 0.00001978  \n\nEpoch: [1][200/671] Elapsed 1m 5s (remain 2m 33s) Loss: 0.1392(0.2207) Grad: 10820.1074  LR: 0.00001973  \n\nEpoch: [1][220/671] Elapsed 1m 12s (remain 2m 27s) Loss: 0.0639(0.2165) Grad: 6965.9819  LR: 0.00001967  \n\nEpoch: [1][240/671] Elapsed 1m 19s (remain 2m 21s) Loss: 0.2696(0.2132) Grad: 47814.3008  LR: 0.00001961  \n\nEpoch: [1][260/671] Elapsed 1m 26s (remain 2m 15s) Loss: 0.1246(0.2103) Grad: 14586.1914  LR: 0.00001954  \n\nEpoch: [1][280/671] Elapsed 1m 33s (remain 2m 9s) Loss: 0.0736(0.2060) Grad: 20197.8945  LR: 0.00001947  \n\nEpoch: [1][300/671] Elapsed 1m 40s (remain 2m 3s) Loss: 0.1025(0.2060) Grad: 11347.4414  LR: 0.00001939  \n\nEpoch: [1][320/671] Elapsed 1m 46s (remain 1m 56s) Loss: 0.1503(0.2021) Grad: 18520.9277  LR: 0.00001930  \n\nEpoch: [1][340/671] Elapsed 1m 53s (remain 1m 49s) Loss: 0.1379(0.1981) Grad: 8649.6084  LR: 0.00001922  \n\nEpoch: [1][360/671] Elapsed 1m 59s (remain 1m 42s) Loss: 0.0373(0.1957) Grad: 10815.1738  LR: 0.00001912  \n\nEpoch: [1][380/671] Elapsed 2m 6s (remain 1m 36s) Loss: 0.2179(0.1936) Grad: 15655.8535  LR: 0.00001902  \n\nEpoch: [1][400/671] Elapsed 2m 12s (remain 1m 29s) Loss: 0.3204(0.1920) Grad: 27709.6250  LR: 0.00001892  \n\nEpoch: [1][420/671] Elapsed 2m 18s (remain 1m 22s) Loss: 0.1672(0.1889) Grad: 19120.5820  LR: 0.00001881  \n\nEpoch: [1][440/671] Elapsed 2m 25s (remain 1m 15s) Loss: 0.1120(0.1870) Grad: 13903.9180  LR: 0.00001870  \n\nEpoch: [1][460/671] Elapsed 2m 31s (remain 1m 8s) Loss: 0.1238(0.1837) Grad: 17840.0762  LR: 0.00001858  \n\nEpoch: [1][480/671] Elapsed 2m 37s (remain 1m 2s) Loss: 0.0641(0.1814) Grad: 16637.5020  LR: 0.00001846  \n\nEpoch: [1][500/671] Elapsed 2m 43s (remain 0m 55s) Loss: 0.1489(0.1797) Grad: 31889.7930  LR: 0.00001833  \n\nEpoch: [1][520/671] Elapsed 2m 50s (remain 0m 49s) Loss: 0.0987(0.1777) Grad: 15577.1318  LR: 0.00001820  \n\nEpoch: [1][540/671] Elapsed 2m 57s (remain 0m 42s) Loss: 0.1102(0.1774) Grad: 20488.1406  LR: 0.00001807  \n\nEpoch: [1][560/671] Elapsed 3m 5s (remain 0m 36s) Loss: 0.1205(0.1766) Grad: 22280.1758  LR: 0.00001792  \n\nEpoch: [1][580/671] Elapsed 3m 11s (remain 0m 29s) Loss: 0.0988(0.1751) Grad: 11365.1924  LR: 0.00001778  \n\nEpoch: [1][600/671] Elapsed 3m 18s (remain 0m 23s) Loss: 0.1085(0.1735) Grad: 13042.6055  LR: 0.00001763  \n\nEpoch: [1][620/671] Elapsed 3m 23s (remain 0m 16s) Loss: 0.0887(0.1715) Grad: 19292.6465  LR: 0.00001748  \n\nEpoch: [1][640/671] Elapsed 3m 30s (remain 0m 9s) Loss: 0.1587(0.1702) Grad: 32716.1250  LR: 0.00001732  \n\nEpoch: [1][660/671] Elapsed 3m 37s (remain 0m 3s) Loss: 0.1088(0.1687) Grad: 18942.0859  LR: 0.00001716  \n\nEpoch: [1][670/671] Elapsed 3m 40s (remain 0m 0s) Loss: 0.0644(0.1678) Grad: 5016.1094  LR: 0.00001708  \n\nEVAL: [0/112] Elapsed 0m 0s (remain 0m 55s) Loss: 0.1176(0.1176) \n\nEVAL: [20/112] Elapsed 0m 3s (remain 0m 16s) Loss: 0.1357(0.1258) \n\nEVAL: [40/112] Elapsed 0m 7s (remain 0m 12s) Loss: 0.1006(0.1217) \n\nEVAL: [60/112] Elapsed 0m 10s (remain 0m 9s) Loss: 0.0900(0.1208) \n\nEVAL: [80/112] Elapsed 0m 14s (remain 0m 5s) Loss: 0.1109(0.1165) \n\nEVAL: [100/112] Elapsed 0m 17s (remain 0m 1s) Loss: 0.2338(0.1197) \n"},{"name":"stderr","output_type":"stream","text":"Epoch 1 - avg_train_loss: 0.1678  avg_val_loss: 0.1190  time: 240s\n\nEpoch 1 - Score: 0.4971  Scores: [0.434792971075031, 0.5593829924712586]\n\nEpoch 1 - Save Best Score: 0.4971 Model\n"},{"name":"stdout","output_type":"stream","text":"EVAL: [111/112] Elapsed 0m 19s (remain 0m 0s) Loss: 0.0866(0.1190) \n\nEpoch: [2][0/671] Elapsed 0m 0s (remain 6m 22s) Loss: 0.0417(0.0417) Grad: inf  LR: 0.00001707  \n\nEpoch: [2][20/671] Elapsed 0m 7s (remain 3m 42s) Loss: 0.0600(0.0982) Grad: 29621.4844  LR: 0.00001690  \n\nEpoch: [2][40/671] Elapsed 0m 13s (remain 3m 21s) Loss: 0.0369(0.0898) Grad: 19586.9434  LR: 0.00001673  \n\nEpoch: [2][60/671] Elapsed 0m 19s (remain 3m 11s) Loss: 0.0643(0.0868) Grad: 31344.2812  LR: 0.00001656  \n\nEpoch: [2][80/671] Elapsed 0m 25s (remain 3m 6s) Loss: 0.1410(0.0883) Grad: 55203.1875  LR: 0.00001638  \n\nEpoch: [2][100/671] Elapsed 0m 32s (remain 3m 1s) Loss: 0.1262(0.0878) Grad: 31514.5215  LR: 0.00001620  \n\nEpoch: [2][120/671] Elapsed 0m 38s (remain 2m 56s) Loss: 0.0645(0.0897) Grad: 22231.8867  LR: 0.00001601  \n\nEpoch: [2][140/671] Elapsed 0m 45s (remain 2m 51s) Loss: 0.2540(0.0916) Grad: 44957.6992  LR: 0.00001582  \n\nEpoch: [2][160/671] Elapsed 0m 53s (remain 2m 48s) Loss: 0.0924(0.0922) Grad: 36994.6914  LR: 0.00001563  \n\nEpoch: [2][180/671] Elapsed 0m 59s (remain 2m 40s) Loss: 0.0534(0.0913) Grad: 9785.5830  LR: 0.00001544  \n\nEpoch: [2][200/671] Elapsed 1m 6s (remain 2m 34s) Loss: 0.1020(0.0912) Grad: 37688.2070  LR: 0.00001524  \n\nEpoch: [2][220/671] Elapsed 1m 11s (remain 2m 26s) Loss: 0.1074(0.0901) Grad: 29767.8340  LR: 0.00001504  \n\nEpoch: [2][240/671] Elapsed 1m 18s (remain 2m 19s) Loss: 0.0670(0.0899) Grad: 17784.3105  LR: 0.00001483  \n\nEpoch: [2][260/671] Elapsed 1m 24s (remain 2m 12s) Loss: 0.0597(0.0895) Grad: 18848.1680  LR: 0.00001463  \n\nEpoch: [2][280/671] Elapsed 1m 31s (remain 2m 6s) Loss: 0.0922(0.0905) Grad: 26937.9883  LR: 0.00001442  \n\nEpoch: [2][300/671] Elapsed 1m 37s (remain 2m 0s) Loss: 0.1577(0.0912) Grad: 47989.8984  LR: 0.00001421  \n\nEpoch: [2][320/671] Elapsed 1m 45s (remain 1m 55s) Loss: 0.1663(0.0919) Grad: 34194.0391  LR: 0.00001399  \n\nEpoch: [2][340/671] Elapsed 1m 52s (remain 1m 48s) Loss: 0.1278(0.0920) Grad: 28388.3105  LR: 0.00001378  \n\nEpoch: [2][360/671] Elapsed 1m 58s (remain 1m 42s) Loss: 0.1203(0.0913) Grad: 28096.2168  LR: 0.00001356  \n\nEpoch: [2][380/671] Elapsed 2m 5s (remain 1m 35s) Loss: 0.1512(0.0906) Grad: 17531.1641  LR: 0.00001334  \n\nEpoch: [2][400/671] Elapsed 2m 11s (remain 1m 28s) Loss: 0.0949(0.0905) Grad: 28686.5859  LR: 0.00001312  \n\nEpoch: [2][420/671] Elapsed 2m 17s (remain 1m 21s) Loss: 0.0764(0.0906) Grad: 22038.8691  LR: 0.00001290  \n\nEpoch: [2][440/671] Elapsed 2m 23s (remain 1m 15s) Loss: 0.0675(0.0903) Grad: 35618.3516  LR: 0.00001267  \n\nEpoch: [2][460/671] Elapsed 2m 30s (remain 1m 8s) Loss: 0.0793(0.0903) Grad: 40129.1758  LR: 0.00001245  \n\nEpoch: [2][480/671] Elapsed 2m 36s (remain 1m 2s) Loss: 0.0539(0.0897) Grad: 22999.6250  LR: 0.00001222  \n\nEpoch: [2][500/671] Elapsed 2m 43s (remain 0m 55s) Loss: 0.0442(0.0897) Grad: 10750.8232  LR: 0.00001199  \n\nEpoch: [2][520/671] Elapsed 2m 49s (remain 0m 48s) Loss: 0.0680(0.0895) Grad: 26574.3789  LR: 0.00001176  \n\nEpoch: [2][540/671] Elapsed 2m 55s (remain 0m 42s) Loss: 0.1016(0.0894) Grad: 16179.1748  LR: 0.00001153  \n\nEpoch: [2][560/671] Elapsed 3m 2s (remain 0m 35s) Loss: 0.0626(0.0896) Grad: 28599.1543  LR: 0.00001130  \n\nEpoch: [2][580/671] Elapsed 3m 9s (remain 0m 29s) Loss: 0.1581(0.0897) Grad: 48308.9258  LR: 0.00001107  \n\nEpoch: [2][600/671] Elapsed 3m 15s (remain 0m 22s) Loss: 0.1056(0.0895) Grad: 33606.2188  LR: 0.00001083  \n\nEpoch: [2][620/671] Elapsed 3m 22s (remain 0m 16s) Loss: 0.0722(0.0893) Grad: 29667.6367  LR: 0.00001060  \n\nEpoch: [2][640/671] Elapsed 3m 29s (remain 0m 9s) Loss: 0.0821(0.0893) Grad: 23048.6641  LR: 0.00001037  \n\nEpoch: [2][660/671] Elapsed 3m 35s (remain 0m 3s) Loss: 0.0899(0.0890) Grad: 27375.8086  LR: 0.00001013  \n\nEpoch: [2][670/671] Elapsed 3m 38s (remain 0m 0s) Loss: 0.1330(0.0892) Grad: 46034.8242  LR: 0.00001002  \n\nEVAL: [0/112] Elapsed 0m 0s (remain 0m 48s) Loss: 0.1335(0.1335) \n\nEVAL: [20/112] Elapsed 0m 3s (remain 0m 16s) Loss: 0.1297(0.1202) \n\nEVAL: [40/112] Elapsed 0m 7s (remain 0m 12s) Loss: 0.1037(0.1157) \n\nEVAL: [60/112] Elapsed 0m 10s (remain 0m 9s) Loss: 0.0860(0.1146) \n\nEVAL: [80/112] Elapsed 0m 14s (remain 0m 5s) Loss: 0.1089(0.1120) \n\nEVAL: [100/112] Elapsed 0m 17s (remain 0m 1s) Loss: 0.2391(0.1140) \n"},{"name":"stderr","output_type":"stream","text":"Epoch 2 - avg_train_loss: 0.0892  avg_val_loss: 0.1140  time: 239s\n\nEpoch 2 - Score: 0.4824  Scores: [0.41742258865130405, 0.5474339073594124]\n\nEpoch 2 - Save Best Score: 0.4824 Model\n"},{"name":"stdout","output_type":"stream","text":"EVAL: [111/112] Elapsed 0m 19s (remain 0m 0s) Loss: 0.0640(0.1140) \n\nEpoch: [3][0/671] Elapsed 0m 0s (remain 10m 4s) Loss: 0.1634(0.1634) Grad: inf  LR: 0.00001001  \n\nEpoch: [3][20/671] Elapsed 0m 7s (remain 3m 46s) Loss: 0.0455(0.0645) Grad: 54268.9961  LR: 0.00000977  \n\nEpoch: [3][40/671] Elapsed 0m 12s (remain 3m 15s) Loss: 0.0349(0.0579) Grad: 25245.5430  LR: 0.00000954  \n\nEpoch: [3][60/671] Elapsed 0m 19s (remain 3m 19s) Loss: 0.0735(0.0610) Grad: 39862.5742  LR: 0.00000930  \n\nEpoch: [3][80/671] Elapsed 0m 25s (remain 3m 8s) Loss: 0.0419(0.0610) Grad: 53565.8984  LR: 0.00000907  \n\nEpoch: [3][100/671] Elapsed 0m 32s (remain 3m 4s) Loss: 0.0460(0.0617) Grad: 42450.5000  LR: 0.00000884  \n\nEpoch: [3][120/671] Elapsed 0m 38s (remain 2m 55s) Loss: 0.1108(0.0632) Grad: 56406.2656  LR: 0.00000861  \n\nEpoch: [3][140/671] Elapsed 0m 45s (remain 2m 49s) Loss: 0.0758(0.0654) Grad: 49386.3594  LR: 0.00000838  \n\nEpoch: [3][160/671] Elapsed 0m 51s (remain 2m 43s) Loss: 0.0246(0.0645) Grad: 20020.1035  LR: 0.00000815  \n\nEpoch: [3][180/671] Elapsed 0m 57s (remain 2m 36s) Loss: 0.0582(0.0640) Grad: 49812.6680  LR: 0.00000792  \n\nEpoch: [3][200/671] Elapsed 1m 3s (remain 2m 28s) Loss: 0.0391(0.0653) Grad: 36167.9883  LR: 0.00000769  \n\nEpoch: [3][220/671] Elapsed 1m 9s (remain 2m 22s) Loss: 0.0465(0.0647) Grad: 30447.4258  LR: 0.00000746  \n\nEpoch: [3][240/671] Elapsed 1m 16s (remain 2m 16s) Loss: 0.0701(0.0639) Grad: 59512.8516  LR: 0.00000724  \n\nEpoch: [3][260/671] Elapsed 1m 23s (remain 2m 10s) Loss: 0.0678(0.0637) Grad: 41530.7578  LR: 0.00000701  \n\nEpoch: [3][280/671] Elapsed 1m 29s (remain 2m 4s) Loss: 0.0420(0.0634) Grad: 49700.9336  LR: 0.00000679  \n\nEpoch: [3][300/671] Elapsed 1m 36s (remain 1m 58s) Loss: 0.0481(0.0627) Grad: 62914.6758  LR: 0.00000657  \n\nEpoch: [3][320/671] Elapsed 1m 42s (remain 1m 51s) Loss: 0.0278(0.0621) Grad: 35430.4141  LR: 0.00000635  \n\nEpoch: [3][340/671] Elapsed 1m 48s (remain 1m 45s) Loss: 0.0686(0.0621) Grad: 76858.1016  LR: 0.00000613  \n\nEpoch: [3][360/671] Elapsed 1m 55s (remain 1m 39s) Loss: 0.0417(0.0622) Grad: 41757.1445  LR: 0.00000592  \n\nEpoch: [3][380/671] Elapsed 2m 2s (remain 1m 32s) Loss: 0.0722(0.0622) Grad: 49645.1445  LR: 0.00000571  \n\nEpoch: [3][400/671] Elapsed 2m 8s (remain 1m 26s) Loss: 0.1122(0.0619) Grad: 51881.7227  LR: 0.00000550  \n\nEpoch: [3][420/671] Elapsed 2m 16s (remain 1m 20s) Loss: 0.0664(0.0618) Grad: 33553.2578  LR: 0.00000529  \n\nEpoch: [3][440/671] Elapsed 2m 23s (remain 1m 14s) Loss: 0.0616(0.0613) Grad: 71838.2188  LR: 0.00000508  \n\nEpoch: [3][460/671] Elapsed 2m 28s (remain 1m 7s) Loss: 0.0655(0.0613) Grad: 54401.0312  LR: 0.00000488  \n\nEpoch: [3][480/671] Elapsed 2m 36s (remain 1m 1s) Loss: 0.0333(0.0610) Grad: 32430.2227  LR: 0.00000468  \n\nEpoch: [3][500/671] Elapsed 2m 42s (remain 0m 55s) Loss: 0.0233(0.0605) Grad: 20489.7344  LR: 0.00000449  \n\nEpoch: [3][520/671] Elapsed 2m 49s (remain 0m 48s) Loss: 0.0986(0.0605) Grad: 64104.4688  LR: 0.00000429  \n\nEpoch: [3][540/671] Elapsed 2m 56s (remain 0m 42s) Loss: 0.0516(0.0602) Grad: 53748.5156  LR: 0.00000410  \n\nEpoch: [3][560/671] Elapsed 3m 3s (remain 0m 35s) Loss: 0.0754(0.0600) Grad: 80114.3906  LR: 0.00000392  \n\nEpoch: [3][580/671] Elapsed 3m 9s (remain 0m 29s) Loss: 0.0488(0.0598) Grad: 79979.9219  LR: 0.00000373  \n\nEpoch: [3][600/671] Elapsed 3m 15s (remain 0m 22s) Loss: 0.0634(0.0595) Grad: 35595.6523  LR: 0.00000355  \n\nEpoch: [3][620/671] Elapsed 3m 22s (remain 0m 16s) Loss: 0.0740(0.0594) Grad: 58776.7969  LR: 0.00000337  \n\nEpoch: [3][640/671] Elapsed 3m 28s (remain 0m 9s) Loss: 0.0368(0.0592) Grad: 21513.2129  LR: 0.00000320  \n\nEpoch: [3][660/671] Elapsed 3m 34s (remain 0m 3s) Loss: 0.0578(0.0594) Grad: 52879.6602  LR: 0.00000303  \n\nEpoch: [3][670/671] Elapsed 3m 36s (remain 0m 0s) Loss: 0.0800(0.0595) Grad: 108818.9453  LR: 0.00000295  \n\nEVAL: [0/112] Elapsed 0m 0s (remain 0m 50s) Loss: 0.1049(0.1049) \n\nEVAL: [20/112] Elapsed 0m 3s (remain 0m 16s) Loss: 0.1238(0.1148) \n\nEVAL: [40/112] Elapsed 0m 7s (remain 0m 12s) Loss: 0.0932(0.1089) \n\nEVAL: [60/112] Elapsed 0m 10s (remain 0m 9s) Loss: 0.0727(0.1069) \n\nEVAL: [80/112] Elapsed 0m 14s (remain 0m 5s) Loss: 0.1122(0.1037) \n\nEVAL: [100/112] Elapsed 0m 17s (remain 0m 1s) Loss: 0.2375(0.1057) \n"},{"name":"stderr","output_type":"stream","text":"Epoch 3 - avg_train_loss: 0.0595  avg_val_loss: 0.1057  time: 237s\n\nEpoch 3 - Score: 0.4624  Scores: [0.3935299661762431, 0.531308450676688]\n\nEpoch 3 - Save Best Score: 0.4624 Model\n"},{"name":"stdout","output_type":"stream","text":"EVAL: [111/112] Elapsed 0m 19s (remain 0m 0s) Loss: 0.0697(0.1057) \n\nEpoch: [4][0/671] Elapsed 0m 0s (remain 4m 51s) Loss: 0.0252(0.0252) Grad: 142049.8438  LR: 0.00000294  \n\nEpoch: [4][20/671] Elapsed 0m 6s (remain 3m 22s) Loss: 0.0447(0.0446) Grad: 39098.4492  LR: 0.00000278  \n\nEpoch: [4][40/671] Elapsed 0m 12s (remain 3m 8s) Loss: 0.0436(0.0454) Grad: 42670.3906  LR: 0.00000262  \n\nEpoch: [4][60/671] Elapsed 0m 18s (remain 3m 6s) Loss: 0.0422(0.0443) Grad: 54295.9375  LR: 0.00000246  \n\nEpoch: [4][80/671] Elapsed 0m 26s (remain 3m 11s) Loss: 0.0558(0.0476) Grad: 40335.4180  LR: 0.00000231  \n\nEpoch: [4][100/671] Elapsed 0m 33s (remain 3m 9s) Loss: 0.0428(0.0486) Grad: 55368.7305  LR: 0.00000216  \n\nEpoch: [4][120/671] Elapsed 0m 39s (remain 3m 0s) Loss: 0.0745(0.0481) Grad: 56571.7188  LR: 0.00000202  \n\nEpoch: [4][140/671] Elapsed 0m 46s (remain 2m 54s) Loss: 0.0464(0.0480) Grad: 31220.4102  LR: 0.00000188  \n\nEpoch: [4][160/671] Elapsed 0m 52s (remain 2m 44s) Loss: 0.0371(0.0480) Grad: 39173.7539  LR: 0.00000175  \n\nEpoch: [4][180/671] Elapsed 0m 58s (remain 2m 38s) Loss: 0.0295(0.0484) Grad: 22424.2773  LR: 0.00000162  \n\nEpoch: [4][200/671] Elapsed 1m 6s (remain 2m 34s) Loss: 0.0928(0.0482) Grad: 64336.9453  LR: 0.00000149  \n\nEpoch: [4][220/671] Elapsed 1m 12s (remain 2m 28s) Loss: 0.0919(0.0483) Grad: 88266.2188  LR: 0.00000137  \n\nEpoch: [4][240/671] Elapsed 1m 18s (remain 2m 20s) Loss: 0.0200(0.0477) Grad: 38456.7109  LR: 0.00000125  \n\nEpoch: [4][260/671] Elapsed 1m 24s (remain 2m 13s) Loss: 0.0442(0.0476) Grad: 27408.7852  LR: 0.00000114  \n\nEpoch: [4][280/671] Elapsed 1m 31s (remain 2m 6s) Loss: 0.0365(0.0470) Grad: 34963.8672  LR: 0.00000104  \n\nEpoch: [4][300/671] Elapsed 1m 37s (remain 1m 59s) Loss: 0.0322(0.0466) Grad: 40142.6680  LR: 0.00000094  \n\nEpoch: [4][320/671] Elapsed 1m 44s (remain 1m 53s) Loss: 0.0321(0.0465) Grad: 27884.9355  LR: 0.00000084  \n\nEpoch: [4][340/671] Elapsed 1m 51s (remain 1m 47s) Loss: 0.0292(0.0466) Grad: 27157.8770  LR: 0.00000075  \n\nEpoch: [4][360/671] Elapsed 1m 59s (remain 1m 42s) Loss: 0.0717(0.0469) Grad: 41308.8945  LR: 0.00000066  \n\nEpoch: [4][380/671] Elapsed 2m 6s (remain 1m 36s) Loss: 0.0553(0.0469) Grad: 48471.3789  LR: 0.00000058  \n\nEpoch: [4][400/671] Elapsed 2m 12s (remain 1m 29s) Loss: 0.0574(0.0471) Grad: 50572.4883  LR: 0.00000051  \n\nEpoch: [4][420/671] Elapsed 2m 18s (remain 1m 22s) Loss: 0.0165(0.0471) Grad: 30754.1348  LR: 0.00000043  \n\nEpoch: [4][440/671] Elapsed 2m 24s (remain 1m 15s) Loss: 0.0212(0.0469) Grad: 23711.5684  LR: 0.00000037  \n\nEpoch: [4][460/671] Elapsed 2m 32s (remain 1m 9s) Loss: 0.0261(0.0471) Grad: 27299.2598  LR: 0.00000031  \n\nEpoch: [4][480/671] Elapsed 2m 39s (remain 1m 2s) Loss: 0.0622(0.0468) Grad: 45907.9844  LR: 0.00000025  \n\nEpoch: [4][500/671] Elapsed 2m 45s (remain 0m 56s) Loss: 0.0418(0.0470) Grad: 47871.2227  LR: 0.00000020  \n\nEpoch: [4][520/671] Elapsed 2m 51s (remain 0m 49s) Loss: 0.0345(0.0467) Grad: 34668.3555  LR: 0.00000016  \n\nEpoch: [4][540/671] Elapsed 2m 57s (remain 0m 42s) Loss: 0.0395(0.0467) Grad: 32882.3867  LR: 0.00000012  \n\nEpoch: [4][560/671] Elapsed 3m 3s (remain 0m 35s) Loss: 0.1072(0.0466) Grad: 109605.6797  LR: 0.00000009  \n\nEpoch: [4][580/671] Elapsed 3m 9s (remain 0m 29s) Loss: 0.0315(0.0465) Grad: 32308.6367  LR: 0.00000006  \n\nEpoch: [4][600/671] Elapsed 3m 16s (remain 0m 22s) Loss: 0.0289(0.0466) Grad: 20577.5840  LR: 0.00000004  \n\nEpoch: [4][620/671] Elapsed 3m 22s (remain 0m 16s) Loss: 0.0506(0.0466) Grad: 32449.8945  LR: 0.00000002  \n\nEpoch: [4][640/671] Elapsed 3m 28s (remain 0m 9s) Loss: 0.0531(0.0466) Grad: 27140.9785  LR: 0.00000001  \n\nEpoch: [4][660/671] Elapsed 3m 35s (remain 0m 3s) Loss: 0.0671(0.0467) Grad: 43356.1992  LR: 0.00000000  \n\nEpoch: [4][670/671] Elapsed 3m 38s (remain 0m 0s) Loss: 0.0327(0.0466) Grad: 51699.3945  LR: 0.00000000  \n\nEVAL: [0/112] Elapsed 0m 0s (remain 0m 54s) Loss: 0.1038(0.1038) \n\nEVAL: [20/112] Elapsed 0m 3s (remain 0m 16s) Loss: 0.1288(0.1150) \n\nEVAL: [40/112] Elapsed 0m 7s (remain 0m 12s) Loss: 0.0901(0.1091) \n\nEVAL: [60/112] Elapsed 0m 10s (remain 0m 9s) Loss: 0.0789(0.1067) \n\nEVAL: [80/112] Elapsed 0m 14s (remain 0m 5s) Loss: 0.1001(0.1042) \n\nEVAL: [100/112] Elapsed 0m 17s (remain 0m 1s) Loss: 0.2580(0.1062) \n"},{"name":"stderr","output_type":"stream","text":"Epoch 4 - avg_train_loss: 0.0466  avg_val_loss: 0.1059  time: 238s\n\nEpoch 4 - Score: 0.4623  Scores: [0.39076072702467185, 0.5337408548430956]\n\nEpoch 4 - Save Best Score: 0.4623 Model\n"},{"name":"stdout","output_type":"stream","text":"EVAL: [111/112] Elapsed 0m 19s (remain 0m 0s) Loss: 0.0766(0.1059) \n"},{"name":"stderr","output_type":"stream","text":"========== fold: 0 result ==========\n\nScore: 0.4623  Scores: [0.39076072702467185, 0.5337408548430956]\n\n========== fold: 1 training ==========\n\nDebertaV2Config {\n\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n\n  \"attention_dropout\": 0.0,\n\n  \"attention_probs_dropout_prob\": 0.0,\n\n  \"hidden_act\": \"gelu\",\n\n  \"hidden_dropout\": 0.0,\n\n  \"hidden_dropout_prob\": 0.0,\n\n  \"hidden_size\": 768,\n\n  \"initializer_range\": 0.02,\n\n  \"intermediate_size\": 3072,\n\n  \"layer_norm_eps\": 1e-07,\n\n  \"max_position_embeddings\": 512,\n\n  \"max_relative_positions\": -1,\n\n  \"model_type\": \"deberta-v2\",\n\n  \"norm_rel_ebd\": \"layer_norm\",\n\n  \"num_attention_heads\": 12,\n\n  \"num_hidden_layers\": 12,\n\n  \"output_hidden_states\": true,\n\n  \"pad_token_id\": 0,\n\n  \"pooler_dropout\": 0,\n\n  \"pooler_hidden_act\": \"gelu\",\n\n  \"pooler_hidden_size\": 768,\n\n  \"pos_att_type\": [\n\n    \"p2c\",\n\n    \"c2p\"\n\n  ],\n\n  \"position_biased_input\": false,\n\n  \"position_buckets\": 256,\n\n  \"relative_attention\": true,\n\n  \"share_att_key\": true,\n\n  \"transformers_version\": \"4.30.2\",\n\n  \"type_vocab_size\": 0,\n\n  \"vocab_size\": 128100\n\n}\n\n\n\nSome weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n\n- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n\n- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"},{"name":"stdout","output_type":"stream","text":"Epoch: [1][0/671] Elapsed 0m 0s (remain 5m 3s) Loss: 0.3736(0.3736) Grad: inf  LR: 0.00002000  \n\nEpoch: [1][20/671] Elapsed 0m 7s (remain 3m 44s) Loss: 0.2846(0.3195) Grad: 105162.5156  LR: 0.00002000  \n\nEpoch: [1][40/671] Elapsed 0m 13s (remain 3m 30s) Loss: 0.2152(0.2704) Grad: 20428.9180  LR: 0.00001999  \n\nEpoch: [1][60/671] Elapsed 0m 20s (remain 3m 28s) Loss: 0.2072(0.2503) Grad: 33920.7930  LR: 0.00001997  \n\nEpoch: [1][80/671] Elapsed 0m 26s (remain 3m 16s) Loss: 0.2529(0.2445) Grad: 33269.1680  LR: 0.00001996  \n\nEpoch: [1][100/671] Elapsed 0m 33s (remain 3m 8s) Loss: 0.0958(0.2258) Grad: 60973.0508  LR: 0.00001993  \n\nEpoch: [1][120/671] Elapsed 0m 39s (remain 2m 57s) Loss: 0.3469(0.2235) Grad: 105130.7188  LR: 0.00001990  \n\nEpoch: [1][140/671] Elapsed 0m 45s (remain 2m 50s) Loss: 0.0516(0.2140) Grad: 30114.9688  LR: 0.00001986  \n\nEpoch: [1][160/671] Elapsed 0m 51s (remain 2m 42s) Loss: 0.1913(0.2071) Grad: 109001.5312  LR: 0.00001982  \n\nEpoch: [1][180/671] Elapsed 0m 57s (remain 2m 35s) Loss: 0.2000(0.2014) Grad: 46242.2031  LR: 0.00001978  \n\nEpoch: [1][200/671] Elapsed 1m 3s (remain 2m 29s) Loss: 0.1002(0.1977) Grad: 44423.7188  LR: 0.00001973  \n\nEpoch: [1][220/671] Elapsed 1m 9s (remain 2m 21s) Loss: 0.1374(0.1925) Grad: 28959.3008  LR: 0.00001967  \n\nEpoch: [1][240/671] Elapsed 1m 15s (remain 2m 15s) Loss: 0.1609(0.1906) Grad: 38404.4961  LR: 0.00001961  \n\nEpoch: [1][260/671] Elapsed 1m 21s (remain 2m 8s) Loss: 0.0498(0.1863) Grad: 31047.2949  LR: 0.00001954  \n\nEpoch: [1][280/671] Elapsed 1m 27s (remain 2m 1s) Loss: 0.1712(0.1846) Grad: 62743.7344  LR: 0.00001947  \n\nEpoch: [1][300/671] Elapsed 1m 33s (remain 1m 55s) Loss: 0.2500(0.1810) Grad: 39303.9453  LR: 0.00001939  \n\nEpoch: [1][320/671] Elapsed 1m 39s (remain 1m 49s) Loss: 0.1203(0.1813) Grad: 36901.4102  LR: 0.00001930  \n\nEpoch: [1][340/671] Elapsed 1m 46s (remain 1m 43s) Loss: 0.1280(0.1780) Grad: 49385.8828  LR: 0.00001922  \n\nEpoch: [1][360/671] Elapsed 1m 53s (remain 1m 37s) Loss: 0.1159(0.1748) Grad: 30973.5371  LR: 0.00001912  \n\nEpoch: [1][380/671] Elapsed 1m 59s (remain 1m 31s) Loss: 0.2507(0.1729) Grad: 99568.1797  LR: 0.00001902  \n\nEpoch: [1][400/671] Elapsed 2m 6s (remain 1m 25s) Loss: 0.0847(0.1718) Grad: 24893.9707  LR: 0.00001892  \n\nEpoch: [1][420/671] Elapsed 2m 13s (remain 1m 19s) Loss: 0.4025(0.1697) Grad: 61419.8750  LR: 0.00001881  \n\nEpoch: [1][440/671] Elapsed 2m 20s (remain 1m 13s) Loss: 0.1838(0.1703) Grad: 50183.1367  LR: 0.00001870  \n\nEpoch: [1][460/671] Elapsed 2m 27s (remain 1m 7s) Loss: 0.2023(0.1692) Grad: 50300.9766  LR: 0.00001858  \n\nEpoch: [1][480/671] Elapsed 2m 33s (remain 1m 0s) Loss: 0.1454(0.1689) Grad: 33434.8789  LR: 0.00001846  \n\nEpoch: [1][500/671] Elapsed 2m 39s (remain 0m 54s) Loss: 0.1176(0.1667) Grad: 52424.1523  LR: 0.00001833  \n\nEpoch: [1][520/671] Elapsed 2m 46s (remain 0m 47s) Loss: 0.1423(0.1657) Grad: 68076.0938  LR: 0.00001820  \n\nEpoch: [1][540/671] Elapsed 2m 52s (remain 0m 41s) Loss: 0.1546(0.1645) Grad: 77686.5156  LR: 0.00001807  \n\nEpoch: [1][560/671] Elapsed 2m 59s (remain 0m 35s) Loss: 0.1071(0.1633) Grad: 35459.2695  LR: 0.00001792  \n\nEpoch: [1][580/671] Elapsed 3m 7s (remain 0m 29s) Loss: 0.1003(0.1621) Grad: 15777.8955  LR: 0.00001778  \n\nEpoch: [1][600/671] Elapsed 3m 13s (remain 0m 22s) Loss: 0.1094(0.1618) Grad: 19149.0762  LR: 0.00001763  \n\nEpoch: [1][620/671] Elapsed 3m 20s (remain 0m 16s) Loss: 0.1325(0.1601) Grad: 42166.9414  LR: 0.00001748  \n\nEpoch: [1][640/671] Elapsed 3m 26s (remain 0m 9s) Loss: 0.0946(0.1591) Grad: 41629.8398  LR: 0.00001732  \n\nEpoch: [1][660/671] Elapsed 3m 32s (remain 0m 3s) Loss: 0.1537(0.1581) Grad: 45076.0625  LR: 0.00001716  \n\nEpoch: [1][670/671] Elapsed 3m 36s (remain 0m 0s) Loss: 0.0932(0.1575) Grad: 36530.1406  LR: 0.00001708  \n\nEVAL: [0/112] Elapsed 0m 0s (remain 0m 55s) Loss: 0.1393(0.1393) \n\nEVAL: [20/112] Elapsed 0m 4s (remain 0m 17s) Loss: 0.1139(0.1144) \n\nEVAL: [40/112] Elapsed 0m 7s (remain 0m 13s) Loss: 0.0996(0.1211) \n\nEVAL: [60/112] Elapsed 0m 11s (remain 0m 9s) Loss: 0.1250(0.1202) \n\nEVAL: [80/112] Elapsed 0m 14s (remain 0m 5s) Loss: 0.0854(0.1225) \n\nEVAL: [100/112] Elapsed 0m 17s (remain 0m 1s) Loss: 0.1335(0.1249) \n"},{"name":"stderr","output_type":"stream","text":"Epoch 1 - avg_train_loss: 0.1575  avg_val_loss: 0.1239  time: 237s\n\nEpoch 1 - Score: 0.5013  Scores: [0.43166249366631043, 0.570917939046402]\n\nEpoch 1 - Save Best Score: 0.5013 Model\n"},{"name":"stdout","output_type":"stream","text":"EVAL: [111/112] Elapsed 0m 19s (remain 0m 0s) Loss: 0.0882(0.1239) \n\nEpoch: [2][0/671] Elapsed 0m 0s (remain 7m 48s) Loss: 0.1656(0.1656) Grad: inf  LR: 0.00001707  \n\nEpoch: [2][20/671] Elapsed 0m 7s (remain 3m 45s) Loss: 0.0863(0.1051) Grad: 102126.2266  LR: 0.00001690  \n\nEpoch: [2][40/671] Elapsed 0m 15s (remain 4m 0s) Loss: 0.0575(0.0938) Grad: 44754.5508  LR: 0.00001673  \n\nEpoch: [2][60/671] Elapsed 0m 22s (remain 3m 47s) Loss: 0.1011(0.0914) Grad: 51946.6992  LR: 0.00001656  \n\nEpoch: [2][80/671] Elapsed 0m 28s (remain 3m 27s) Loss: 0.0573(0.0931) Grad: 46262.1328  LR: 0.00001638  \n\nEpoch: [2][100/671] Elapsed 0m 34s (remain 3m 16s) Loss: 0.0417(0.0920) Grad: 21955.0703  LR: 0.00001620  \n\nEpoch: [2][120/671] Elapsed 0m 40s (remain 3m 4s) Loss: 0.1000(0.0921) Grad: 33674.2422  LR: 0.00001601  \n\nEpoch: [2][140/671] Elapsed 0m 46s (remain 2m 54s) Loss: 0.0553(0.0930) Grad: 16432.9316  LR: 0.00001582  \n\nEpoch: [2][160/671] Elapsed 0m 53s (remain 2m 48s) Loss: 0.0671(0.0922) Grad: 16763.8301  LR: 0.00001563  \n\nEpoch: [2][180/671] Elapsed 0m 59s (remain 2m 41s) Loss: 0.0407(0.0924) Grad: 32436.5898  LR: 0.00001544  \n\nEpoch: [2][200/671] Elapsed 1m 5s (remain 2m 33s) Loss: 0.1033(0.0944) Grad: 35608.3867  LR: 0.00001524  \n\nEpoch: [2][220/671] Elapsed 1m 12s (remain 2m 27s) Loss: 0.0666(0.0946) Grad: 20028.5781  LR: 0.00001504  \n\nEpoch: [2][240/671] Elapsed 1m 18s (remain 2m 19s) Loss: 0.2038(0.0946) Grad: 43778.0430  LR: 0.00001483  \n\nEpoch: [2][260/671] Elapsed 1m 24s (remain 2m 12s) Loss: 0.0668(0.0950) Grad: 39152.2578  LR: 0.00001463  \n\nEpoch: [2][280/671] Elapsed 1m 30s (remain 2m 6s) Loss: 0.0604(0.0956) Grad: 24468.4297  LR: 0.00001442  \n\nEpoch: [2][300/671] Elapsed 1m 37s (remain 2m 0s) Loss: 0.1802(0.0964) Grad: 40901.4531  LR: 0.00001421  \n\nEpoch: [2][320/671] Elapsed 1m 44s (remain 1m 53s) Loss: 0.0553(0.0967) Grad: 58157.8164  LR: 0.00001399  \n\nEpoch: [2][340/671] Elapsed 1m 51s (remain 1m 47s) Loss: 0.1237(0.0960) Grad: 39284.6484  LR: 0.00001378  \n\nEpoch: [2][360/671] Elapsed 1m 56s (remain 1m 40s) Loss: 0.1137(0.0964) Grad: 40220.7578  LR: 0.00001356  \n\nEpoch: [2][380/671] Elapsed 2m 2s (remain 1m 33s) Loss: 0.0643(0.0968) Grad: 46838.5195  LR: 0.00001334  \n\nEpoch: [2][400/671] Elapsed 2m 8s (remain 1m 26s) Loss: 0.1282(0.0975) Grad: 41718.9531  LR: 0.00001312  \n\nEpoch: [2][420/671] Elapsed 2m 14s (remain 1m 19s) Loss: 0.1513(0.0980) Grad: 53689.3594  LR: 0.00001290  \n\nEpoch: [2][440/671] Elapsed 2m 20s (remain 1m 13s) Loss: 0.0669(0.0976) Grad: 31881.4492  LR: 0.00001267  \n\nEpoch: [2][460/671] Elapsed 2m 28s (remain 1m 7s) Loss: 0.2258(0.0989) Grad: 87564.7891  LR: 0.00001245  \n\nEpoch: [2][480/671] Elapsed 2m 35s (remain 1m 1s) Loss: 0.0569(0.0988) Grad: 17276.5898  LR: 0.00001222  \n\nEpoch: [2][500/671] Elapsed 2m 43s (remain 0m 55s) Loss: 0.1117(0.0992) Grad: 55042.2578  LR: 0.00001199  \n\nEpoch: [2][520/671] Elapsed 2m 49s (remain 0m 48s) Loss: 0.1755(0.0987) Grad: 53480.5156  LR: 0.00001176  \n\nEpoch: [2][540/671] Elapsed 2m 54s (remain 0m 41s) Loss: 0.1169(0.0984) Grad: 36272.1406  LR: 0.00001153  \n\nEpoch: [2][560/671] Elapsed 3m 1s (remain 0m 35s) Loss: 0.1211(0.0988) Grad: 48222.6406  LR: 0.00001130  \n\nEpoch: [2][580/671] Elapsed 3m 8s (remain 0m 29s) Loss: 0.0437(0.0991) Grad: 11216.6504  LR: 0.00001107  \n\nEpoch: [2][600/671] Elapsed 3m 14s (remain 0m 22s) Loss: 0.1616(0.0998) Grad: 59615.7500  LR: 0.00001083  \n\nEpoch: [2][620/671] Elapsed 3m 20s (remain 0m 16s) Loss: 0.0959(0.0995) Grad: 68232.9141  LR: 0.00001060  \n\nEpoch: [2][640/671] Elapsed 3m 26s (remain 0m 9s) Loss: 0.0798(0.0990) Grad: 59233.0312  LR: 0.00001037  \n\nEpoch: [2][660/671] Elapsed 3m 33s (remain 0m 3s) Loss: 0.1043(0.0988) Grad: 57563.8008  LR: 0.00001013  \n\nEpoch: [2][670/671] Elapsed 3m 36s (remain 0m 0s) Loss: 0.0979(0.0988) Grad: 39550.7383  LR: 0.00001002  \n\nEVAL: [0/112] Elapsed 0m 0s (remain 1m 5s) Loss: 0.1474(0.1474) \n\nEVAL: [20/112] Elapsed 0m 4s (remain 0m 17s) Loss: 0.1026(0.1035) \n\nEVAL: [40/112] Elapsed 0m 8s (remain 0m 13s) Loss: 0.1077(0.1050) \n\nEVAL: [60/112] Elapsed 0m 11s (remain 0m 9s) Loss: 0.1029(0.1051) \n\nEVAL: [80/112] Elapsed 0m 14s (remain 0m 5s) Loss: 0.0706(0.1068) \n\nEVAL: [100/112] Elapsed 0m 17s (remain 0m 1s) Loss: 0.0957(0.1069) \n"},{"name":"stderr","output_type":"stream","text":"Epoch 2 - avg_train_loss: 0.0988  avg_val_loss: 0.1065  time: 237s\n\nEpoch 2 - Score: 0.4612  Scores: [0.3915531404985714, 0.5308464326548927]\n\nEpoch 2 - Save Best Score: 0.4612 Model\n"},{"name":"stdout","output_type":"stream","text":"EVAL: [111/112] Elapsed 0m 19s (remain 0m 0s) Loss: 0.1183(0.1065) \n\nEpoch: [3][0/671] Elapsed 0m 0s (remain 4m 42s) Loss: 0.0707(0.0707) Grad: inf  LR: 0.00001001  \n\nEpoch: [3][20/671] Elapsed 0m 7s (remain 3m 41s) Loss: 0.1105(0.0723) Grad: 54441.4062  LR: 0.00000977  \n\nEpoch: [3][40/671] Elapsed 0m 15s (remain 3m 51s) Loss: 0.0377(0.0703) Grad: 54212.7969  LR: 0.00000954  \n\nEpoch: [3][60/671] Elapsed 0m 21s (remain 3m 31s) Loss: 0.0927(0.0698) Grad: 84356.9453  LR: 0.00000930  \n\nEpoch: [3][80/671] Elapsed 0m 28s (remain 3m 29s) Loss: 0.0635(0.0684) Grad: 30714.3145  LR: 0.00000907  \n\nEpoch: [3][100/671] Elapsed 0m 34s (remain 3m 14s) Loss: 0.1304(0.0682) Grad: 45489.7852  LR: 0.00000884  \n\nEpoch: [3][120/671] Elapsed 0m 40s (remain 3m 5s) Loss: 0.0542(0.0713) Grad: 60328.2070  LR: 0.00000861  \n\nEpoch: [3][140/671] Elapsed 0m 47s (remain 2m 58s) Loss: 0.0655(0.0693) Grad: 75994.6250  LR: 0.00000838  \n\nEpoch: [3][160/671] Elapsed 0m 54s (remain 2m 51s) Loss: 0.0791(0.0699) Grad: 72124.6250  LR: 0.00000815  \n\nEpoch: [3][180/671] Elapsed 0m 59s (remain 2m 42s) Loss: 0.0557(0.0694) Grad: 23328.2793  LR: 0.00000792  \n\nEpoch: [3][200/671] Elapsed 1m 6s (remain 2m 34s) Loss: 0.0725(0.0697) Grad: 24929.1660  LR: 0.00000769  \n\nEpoch: [3][220/671] Elapsed 1m 12s (remain 2m 27s) Loss: 0.0767(0.0697) Grad: 49541.1992  LR: 0.00000746  \n\nEpoch: [3][240/671] Elapsed 1m 19s (remain 2m 21s) Loss: 0.0448(0.0695) Grad: 28478.8555  LR: 0.00000724  \n\nEpoch: [3][260/671] Elapsed 1m 25s (remain 2m 14s) Loss: 0.0429(0.0686) Grad: 16625.3477  LR: 0.00000701  \n\nEpoch: [3][280/671] Elapsed 1m 31s (remain 2m 6s) Loss: 0.0333(0.0675) Grad: 13458.2793  LR: 0.00000679  \n\nEpoch: [3][300/671] Elapsed 1m 38s (remain 2m 0s) Loss: 0.1086(0.0666) Grad: 20999.4980  LR: 0.00000657  \n\nEpoch: [3][320/671] Elapsed 1m 44s (remain 1m 53s) Loss: 0.0536(0.0663) Grad: 21077.6133  LR: 0.00000635  \n\nEpoch: [3][340/671] Elapsed 1m 49s (remain 1m 46s) Loss: 0.0850(0.0654) Grad: 24668.1641  LR: 0.00000613  \n\nEpoch: [3][360/671] Elapsed 1m 56s (remain 1m 39s) Loss: 0.0876(0.0651) Grad: 23497.2852  LR: 0.00000592  \n\nEpoch: [3][380/671] Elapsed 2m 2s (remain 1m 33s) Loss: 0.0230(0.0652) Grad: 10035.5723  LR: 0.00000571  \n\nEpoch: [3][400/671] Elapsed 2m 9s (remain 1m 26s) Loss: 0.0678(0.0655) Grad: 32253.8965  LR: 0.00000550  \n\nEpoch: [3][420/671] Elapsed 2m 14s (remain 1m 19s) Loss: 0.0726(0.0658) Grad: 30177.4922  LR: 0.00000529  \n\nEpoch: [3][440/671] Elapsed 2m 22s (remain 1m 14s) Loss: 0.0434(0.0660) Grad: 11487.3770  LR: 0.00000508  \n\nEpoch: [3][460/671] Elapsed 2m 29s (remain 1m 7s) Loss: 0.1337(0.0670) Grad: 22679.4219  LR: 0.00000488  \n\nEpoch: [3][480/671] Elapsed 2m 35s (remain 1m 1s) Loss: 0.0912(0.0667) Grad: 45787.8984  LR: 0.00000468  \n\nEpoch: [3][500/671] Elapsed 2m 42s (remain 0m 55s) Loss: 0.0278(0.0662) Grad: 18631.1641  LR: 0.00000449  \n\nEpoch: [3][520/671] Elapsed 2m 49s (remain 0m 48s) Loss: 0.0487(0.0660) Grad: 21717.9531  LR: 0.00000429  \n\nEpoch: [3][540/671] Elapsed 2m 56s (remain 0m 42s) Loss: 0.0779(0.0659) Grad: 16088.4033  LR: 0.00000410  \n\nEpoch: [3][560/671] Elapsed 3m 2s (remain 0m 35s) Loss: 0.0405(0.0656) Grad: 12885.3926  LR: 0.00000392  \n\nEpoch: [3][580/671] Elapsed 3m 9s (remain 0m 29s) Loss: 0.0947(0.0655) Grad: 30270.0605  LR: 0.00000373  \n\nEpoch: [3][600/671] Elapsed 3m 14s (remain 0m 22s) Loss: 0.0677(0.0650) Grad: 21928.0547  LR: 0.00000355  \n\nEpoch: [3][620/671] Elapsed 3m 21s (remain 0m 16s) Loss: 0.0345(0.0647) Grad: 17943.8730  LR: 0.00000337  \n\nEpoch: [3][640/671] Elapsed 3m 28s (remain 0m 9s) Loss: 0.0581(0.0650) Grad: 33634.9844  LR: 0.00000320  \n\nEpoch: [3][660/671] Elapsed 3m 36s (remain 0m 3s) Loss: 0.0774(0.0653) Grad: 28844.4980  LR: 0.00000303  \n\nEpoch: [3][670/671] Elapsed 3m 39s (remain 0m 0s) Loss: 0.0217(0.0649) Grad: 14611.4756  LR: 0.00000295  \n\nEVAL: [0/112] Elapsed 0m 0s (remain 0m 56s) Loss: 0.1211(0.1211) \n\nEVAL: [20/112] Elapsed 0m 4s (remain 0m 17s) Loss: 0.0938(0.1054) \n\nEVAL: [40/112] Elapsed 0m 7s (remain 0m 13s) Loss: 0.0903(0.1053) \n\nEVAL: [60/112] Elapsed 0m 11s (remain 0m 9s) Loss: 0.1105(0.1051) \n\nEVAL: [80/112] Elapsed 0m 14s (remain 0m 5s) Loss: 0.0792(0.1081) \n\nEVAL: [100/112] Elapsed 0m 17s (remain 0m 1s) Loss: 0.1163(0.1089) \n"},{"name":"stderr","output_type":"stream","text":"Epoch 3 - avg_train_loss: 0.0649  avg_val_loss: 0.1087  time: 239s\n\nEpoch 3 - Score: 0.4667  Scores: [0.39778638570107927, 0.5356141443866703]\n"},{"name":"stdout","output_type":"stream","text":"EVAL: [111/112] Elapsed 0m 19s (remain 0m 0s) Loss: 0.1208(0.1087) \n\nEpoch: [4][0/671] Elapsed 0m 0s (remain 5m 1s) Loss: 0.0442(0.0442) Grad: 155564.0469  LR: 0.00000294  \n\nEpoch: [4][20/671] Elapsed 0m 6s (remain 3m 31s) Loss: 0.0399(0.0493) Grad: 27813.7480  LR: 0.00000278  \n\nEpoch: [4][40/671] Elapsed 0m 13s (remain 3m 33s) Loss: 0.0749(0.0525) Grad: 37908.5430  LR: 0.00000262  \n\nEpoch: [4][60/671] Elapsed 0m 20s (remain 3m 21s) Loss: 0.0278(0.0495) Grad: 53335.8125  LR: 0.00000246  \n\nEpoch: [4][80/671] Elapsed 0m 26s (remain 3m 11s) Loss: 0.0444(0.0485) Grad: 28541.8750  LR: 0.00000231  \n\nEpoch: [4][100/671] Elapsed 0m 33s (remain 3m 6s) Loss: 0.0223(0.0469) Grad: 44563.9805  LR: 0.00000216  \n\nEpoch: [4][120/671] Elapsed 0m 38s (remain 2m 54s) Loss: 0.1206(0.0463) Grad: 39751.7969  LR: 0.00000202  \n\nEpoch: [4][140/671] Elapsed 0m 46s (remain 2m 53s) Loss: 0.0434(0.0471) Grad: 44738.1641  LR: 0.00000188  \n\nEpoch: [4][160/671] Elapsed 0m 52s (remain 2m 46s) Loss: 0.0490(0.0456) Grad: 51420.9102  LR: 0.00000175  \n\nEpoch: [4][180/671] Elapsed 0m 58s (remain 2m 39s) Loss: 0.0431(0.0465) Grad: 66933.7812  LR: 0.00000162  \n\nEpoch: [4][200/671] Elapsed 1m 5s (remain 2m 33s) Loss: 0.0353(0.0474) Grad: 40461.7383  LR: 0.00000149  \n\nEpoch: [4][220/671] Elapsed 1m 12s (remain 2m 27s) Loss: 0.0480(0.0472) Grad: 44298.2344  LR: 0.00000137  \n\nEpoch: [4][240/671] Elapsed 1m 20s (remain 2m 23s) Loss: 0.0221(0.0466) Grad: 26124.3379  LR: 0.00000125  \n\nEpoch: [4][260/671] Elapsed 1m 25s (remain 2m 14s) Loss: 0.0318(0.0463) Grad: 24285.0469  LR: 0.00000114  \n\nEpoch: [4][280/671] Elapsed 1m 31s (remain 2m 7s) Loss: 0.0170(0.0456) Grad: 22145.5488  LR: 0.00000104  \n\nEpoch: [4][300/671] Elapsed 1m 39s (remain 2m 2s) Loss: 0.0351(0.0455) Grad: 52915.6055  LR: 0.00000094  \n\nEpoch: [4][320/671] Elapsed 1m 45s (remain 1m 54s) Loss: 0.0496(0.0458) Grad: 51717.6172  LR: 0.00000084  \n\nEpoch: [4][340/671] Elapsed 1m 51s (remain 1m 47s) Loss: 0.0431(0.0454) Grad: 52532.2422  LR: 0.00000075  \n\nEpoch: [4][360/671] Elapsed 1m 58s (remain 1m 41s) Loss: 0.0422(0.0452) Grad: 24290.1875  LR: 0.00000066  \n\nEpoch: [4][380/671] Elapsed 2m 4s (remain 1m 34s) Loss: 0.0257(0.0451) Grad: 44134.4844  LR: 0.00000058  \n\nEpoch: [4][400/671] Elapsed 2m 11s (remain 1m 28s) Loss: 0.0461(0.0449) Grad: 41486.7578  LR: 0.00000051  \n\nEpoch: [4][420/671] Elapsed 2m 17s (remain 1m 21s) Loss: 0.0478(0.0451) Grad: 46205.5586  LR: 0.00000043  \n\nEpoch: [4][440/671] Elapsed 2m 24s (remain 1m 15s) Loss: 0.0218(0.0449) Grad: 44734.9023  LR: 0.00000037  \n\nEpoch: [4][460/671] Elapsed 2m 29s (remain 1m 8s) Loss: 0.0330(0.0448) Grad: 26417.2090  LR: 0.00000031  \n\nEpoch: [4][480/671] Elapsed 2m 36s (remain 1m 1s) Loss: 0.0676(0.0448) Grad: 73879.8359  LR: 0.00000025  \n\nEpoch: [4][500/671] Elapsed 2m 43s (remain 0m 55s) Loss: 0.0541(0.0449) Grad: 35427.7539  LR: 0.00000020  \n\nEpoch: [4][520/671] Elapsed 2m 50s (remain 0m 49s) Loss: 0.0318(0.0446) Grad: 39735.9414  LR: 0.00000016  \n\nEpoch: [4][540/671] Elapsed 2m 57s (remain 0m 42s) Loss: 0.0400(0.0447) Grad: 48978.9492  LR: 0.00000012  \n\nEpoch: [4][560/671] Elapsed 3m 3s (remain 0m 35s) Loss: 0.0408(0.0447) Grad: 59795.5820  LR: 0.00000009  \n\nEpoch: [4][580/671] Elapsed 3m 9s (remain 0m 29s) Loss: 0.0302(0.0446) Grad: 22544.0078  LR: 0.00000006  \n\nEpoch: [4][600/671] Elapsed 3m 17s (remain 0m 23s) Loss: 0.0292(0.0446) Grad: 47944.0781  LR: 0.00000004  \n\nEpoch: [4][620/671] Elapsed 3m 23s (remain 0m 16s) Loss: 0.0890(0.0446) Grad: 67818.0000  LR: 0.00000002  \n\nEpoch: [4][640/671] Elapsed 3m 29s (remain 0m 9s) Loss: 0.0590(0.0445) Grad: 44484.7188  LR: 0.00000001  \n\nEpoch: [4][660/671] Elapsed 3m 36s (remain 0m 3s) Loss: 0.0378(0.0443) Grad: 51790.6328  LR: 0.00000000  \n\nEpoch: [4][670/671] Elapsed 3m 39s (remain 0m 0s) Loss: 0.0302(0.0441) Grad: 39502.9883  LR: 0.00000000  \n\nEVAL: [0/112] Elapsed 0m 0s (remain 0m 50s) Loss: 0.1368(0.1368) \n\nEVAL: [20/112] Elapsed 0m 4s (remain 0m 17s) Loss: 0.0929(0.1035) \n\nEVAL: [40/112] Elapsed 0m 7s (remain 0m 13s) Loss: 0.0877(0.1037) \n\nEVAL: [60/112] Elapsed 0m 11s (remain 0m 9s) Loss: 0.1079(0.1033) \n\nEVAL: [80/112] Elapsed 0m 14s (remain 0m 5s) Loss: 0.0747(0.1066) \n\nEVAL: [100/112] Elapsed 0m 17s (remain 0m 1s) Loss: 0.1156(0.1068) \n"},{"name":"stderr","output_type":"stream","text":"Epoch 4 - avg_train_loss: 0.0441  avg_val_loss: 0.1065  time: 240s\n\nEpoch 4 - Score: 0.4618  Scores: [0.39123384322580024, 0.5323950354920308]\n"},{"name":"stdout","output_type":"stream","text":"EVAL: [111/112] Elapsed 0m 19s (remain 0m 0s) Loss: 0.1185(0.1065) \n"},{"name":"stderr","output_type":"stream","text":"========== fold: 1 result ==========\n\nScore: 0.4612  Scores: [0.3915531404985714, 0.5308464326548927]\n\n========== fold: 2 training ==========\n\nDebertaV2Config {\n\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n\n  \"attention_dropout\": 0.0,\n\n  \"attention_probs_dropout_prob\": 0.0,\n\n  \"hidden_act\": \"gelu\",\n\n  \"hidden_dropout\": 0.0,\n\n  \"hidden_dropout_prob\": 0.0,\n\n  \"hidden_size\": 768,\n\n  \"initializer_range\": 0.02,\n\n  \"intermediate_size\": 3072,\n\n  \"layer_norm_eps\": 1e-07,\n\n  \"max_position_embeddings\": 512,\n\n  \"max_relative_positions\": -1,\n\n  \"model_type\": \"deberta-v2\",\n\n  \"norm_rel_ebd\": \"layer_norm\",\n\n  \"num_attention_heads\": 12,\n\n  \"num_hidden_layers\": 12,\n\n  \"output_hidden_states\": true,\n\n  \"pad_token_id\": 0,\n\n  \"pooler_dropout\": 0,\n\n  \"pooler_hidden_act\": \"gelu\",\n\n  \"pooler_hidden_size\": 768,\n\n  \"pos_att_type\": [\n\n    \"p2c\",\n\n    \"c2p\"\n\n  ],\n\n  \"position_biased_input\": false,\n\n  \"position_buckets\": 256,\n\n  \"relative_attention\": true,\n\n  \"share_att_key\": true,\n\n  \"transformers_version\": \"4.30.2\",\n\n  \"type_vocab_size\": 0,\n\n  \"vocab_size\": 128100\n\n}\n\n\n\nSome weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n\n- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n\n- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"},{"name":"stdout","output_type":"stream","text":"Epoch: [1][0/671] Elapsed 0m 0s (remain 6m 2s) Loss: 0.6931(0.6931) Grad: inf  LR: 0.00002000  \n\nEpoch: [1][20/671] Elapsed 0m 6s (remain 3m 19s) Loss: 0.2606(0.3963) Grad: 34165.6211  LR: 0.00002000  \n\nEpoch: [1][40/671] Elapsed 0m 13s (remain 3m 30s) Loss: 0.1662(0.3288) Grad: 56741.6055  LR: 0.00001999  \n\nEpoch: [1][60/671] Elapsed 0m 19s (remain 3m 15s) Loss: 0.3094(0.2822) Grad: 78726.8750  LR: 0.00001997  \n\nEpoch: [1][80/671] Elapsed 0m 26s (remain 3m 13s) Loss: 0.2001(0.2586) Grad: 62206.6211  LR: 0.00001996  \n\nEpoch: [1][100/671] Elapsed 0m 32s (remain 3m 5s) Loss: 0.1039(0.2395) Grad: 25654.5723  LR: 0.00001993  \n\nEpoch: [1][120/671] Elapsed 0m 39s (remain 2m 59s) Loss: 0.1146(0.2303) Grad: 61796.9414  LR: 0.00001990  \n\nEpoch: [1][140/671] Elapsed 0m 45s (remain 2m 52s) Loss: 0.1640(0.2160) Grad: 53275.7969  LR: 0.00001986  \n\nEpoch: [1][160/671] Elapsed 0m 52s (remain 2m 46s) Loss: 0.1052(0.2122) Grad: 25878.7539  LR: 0.00001982  \n\nEpoch: [1][180/671] Elapsed 0m 58s (remain 2m 38s) Loss: 0.2591(0.2072) Grad: 49872.1016  LR: 0.00001978  \n\nEpoch: [1][200/671] Elapsed 1m 4s (remain 2m 31s) Loss: 0.1010(0.2006) Grad: 26904.5508  LR: 0.00001972  \n\nEpoch: [1][220/671] Elapsed 1m 11s (remain 2m 25s) Loss: 0.1129(0.1958) Grad: 35947.9023  LR: 0.00001967  \n\nEpoch: [1][240/671] Elapsed 1m 19s (remain 2m 21s) Loss: 0.0980(0.1919) Grad: 47777.4062  LR: 0.00001961  \n\nEpoch: [1][260/671] Elapsed 1m 25s (remain 2m 13s) Loss: 0.0940(0.1870) Grad: 51409.5078  LR: 0.00001954  \n\nEpoch: [1][280/671] Elapsed 1m 31s (remain 2m 7s) Loss: 0.1566(0.1846) Grad: 33526.7969  LR: 0.00001946  \n\nEpoch: [1][300/671] Elapsed 1m 37s (remain 2m 0s) Loss: 0.3461(0.1831) Grad: 78031.4141  LR: 0.00001939  \n\nEpoch: [1][320/671] Elapsed 1m 44s (remain 1m 54s) Loss: 0.0655(0.1804) Grad: 19130.2324  LR: 0.00001930  \n\nEpoch: [1][340/671] Elapsed 1m 51s (remain 1m 47s) Loss: 0.1405(0.1777) Grad: 65212.5234  LR: 0.00001922  \n\nEpoch: [1][360/671] Elapsed 1m 57s (remain 1m 40s) Loss: 0.3274(0.1756) Grad: 71953.4922  LR: 0.00001912  \n\nEpoch: [1][380/671] Elapsed 2m 4s (remain 1m 34s) Loss: 0.1001(0.1737) Grad: 34916.4805  LR: 0.00001902  \n\nEpoch: [1][400/671] Elapsed 2m 9s (remain 1m 27s) Loss: 0.1531(0.1713) Grad: 35301.2031  LR: 0.00001892  \n\nEpoch: [1][420/671] Elapsed 2m 15s (remain 1m 20s) Loss: 0.2184(0.1709) Grad: 62559.9258  LR: 0.00001881  \n\nEpoch: [1][440/671] Elapsed 2m 21s (remain 1m 13s) Loss: 0.1072(0.1691) Grad: 50622.0391  LR: 0.00001870  \n\nEpoch: [1][460/671] Elapsed 2m 27s (remain 1m 7s) Loss: 0.1028(0.1685) Grad: 35799.2070  LR: 0.00001858  \n\nEpoch: [1][480/671] Elapsed 2m 34s (remain 1m 1s) Loss: 0.0768(0.1695) Grad: 27116.2070  LR: 0.00001846  \n\nEpoch: [1][500/671] Elapsed 2m 40s (remain 0m 54s) Loss: 0.0722(0.1677) Grad: 13839.5449  LR: 0.00001833  \n\nEpoch: [1][520/671] Elapsed 2m 46s (remain 0m 48s) Loss: 0.1509(0.1659) Grad: 31069.2812  LR: 0.00001820  \n\nEpoch: [1][540/671] Elapsed 2m 52s (remain 0m 41s) Loss: 0.1581(0.1649) Grad: 31294.8633  LR: 0.00001806  \n\nEpoch: [1][560/671] Elapsed 2m 59s (remain 0m 35s) Loss: 0.0513(0.1632) Grad: 19802.5273  LR: 0.00001792  \n\nEpoch: [1][580/671] Elapsed 3m 6s (remain 0m 28s) Loss: 0.1063(0.1622) Grad: 32054.6113  LR: 0.00001778  \n\nEpoch: [1][600/671] Elapsed 3m 13s (remain 0m 22s) Loss: 0.0810(0.1611) Grad: 54156.0820  LR: 0.00001763  \n\nEpoch: [1][620/671] Elapsed 3m 19s (remain 0m 16s) Loss: 0.0644(0.1596) Grad: 29763.3867  LR: 0.00001748  \n\nEpoch: [1][640/671] Elapsed 3m 25s (remain 0m 9s) Loss: 0.1772(0.1586) Grad: 21396.3652  LR: 0.00001732  \n\nEpoch: [1][660/671] Elapsed 3m 33s (remain 0m 3s) Loss: 0.1642(0.1582) Grad: 68104.4609  LR: 0.00001716  \n\nEpoch: [1][670/671] Elapsed 3m 35s (remain 0m 0s) Loss: 0.1218(0.1573) Grad: 50127.5234  LR: 0.00001708  \n\nEVAL: [0/112] Elapsed 0m 0s (remain 0m 37s) Loss: 0.1970(0.1970) \n\nEVAL: [20/112] Elapsed 0m 3s (remain 0m 15s) Loss: 0.1234(0.1325) \n\nEVAL: [40/112] Elapsed 0m 7s (remain 0m 13s) Loss: 0.1402(0.1352) \n\nEVAL: [60/112] Elapsed 0m 10s (remain 0m 9s) Loss: 0.1746(0.1360) \n\nEVAL: [80/112] Elapsed 0m 15s (remain 0m 5s) Loss: 0.1527(0.1376) \n\nEVAL: [100/112] Elapsed 0m 18s (remain 0m 2s) Loss: 0.0777(0.1377) \n\nEVAL: [111/112] Elapsed 0m 20s (remain 0m 0s) Loss: 0.1636(0.1373) \n"},{"name":"stderr","output_type":"stream","text":"Epoch 1 - avg_train_loss: 0.1573  avg_val_loss: 0.1373  time: 237s\n\nEpoch 1 - Score: 0.5342  Scores: [0.5098404292781592, 0.5586359594049851]\n\nEpoch 1 - Save Best Score: 0.5342 Model\n"},{"name":"stdout","output_type":"stream","text":"Epoch: [2][0/671] Elapsed 0m 0s (remain 8m 22s) Loss: 0.0848(0.0848) Grad: inf  LR: 0.00001707  \n\nEpoch: [2][20/671] Elapsed 0m 5s (remain 2m 52s) Loss: 0.0518(0.1130) Grad: 73935.4141  LR: 0.00001690  \n\nEpoch: [2][40/671] Elapsed 0m 11s (remain 2m 53s) Loss: 0.0432(0.1026) Grad: 71008.0547  LR: 0.00001673  \n\nEpoch: [2][60/671] Elapsed 0m 17s (remain 2m 59s) Loss: 0.1234(0.1009) Grad: 52334.8047  LR: 0.00001655  \n\nEpoch: [2][80/671] Elapsed 0m 25s (remain 3m 4s) Loss: 0.0730(0.0945) Grad: 54970.8047  LR: 0.00001637  \n\nEpoch: [2][100/671] Elapsed 0m 31s (remain 3m 0s) Loss: 0.0806(0.0962) Grad: 67059.7734  LR: 0.00001619  \n\nEpoch: [2][120/671] Elapsed 0m 37s (remain 2m 50s) Loss: 0.1935(0.0968) Grad: 43097.0547  LR: 0.00001601  \n\nEpoch: [2][140/671] Elapsed 0m 43s (remain 2m 43s) Loss: 0.1307(0.0974) Grad: 66373.2656  LR: 0.00001582  \n\nEpoch: [2][160/671] Elapsed 0m 50s (remain 2m 38s) Loss: 0.0716(0.0968) Grad: 66120.9297  LR: 0.00001563  \n\nEpoch: [2][180/671] Elapsed 0m 55s (remain 2m 31s) Loss: 0.1005(0.0967) Grad: 51811.1836  LR: 0.00001543  \n\nEpoch: [2][200/671] Elapsed 1m 3s (remain 2m 28s) Loss: 0.0756(0.0966) Grad: 56121.5234  LR: 0.00001523  \n\nEpoch: [2][220/671] Elapsed 1m 9s (remain 2m 21s) Loss: 0.0436(0.0962) Grad: 37606.7148  LR: 0.00001503  \n\nEpoch: [2][240/671] Elapsed 1m 15s (remain 2m 14s) Loss: 0.0536(0.0942) Grad: 48155.7773  LR: 0.00001483  \n\nEpoch: [2][260/671] Elapsed 1m 22s (remain 2m 9s) Loss: 0.0390(0.0947) Grad: 52583.3047  LR: 0.00001462  \n\nEpoch: [2][280/671] Elapsed 1m 28s (remain 2m 2s) Loss: 0.1276(0.0932) Grad: 150875.2344  LR: 0.00001442  \n\nEpoch: [2][300/671] Elapsed 1m 34s (remain 1m 56s) Loss: 0.1255(0.0931) Grad: 74077.1172  LR: 0.00001420  \n\nEpoch: [2][320/671] Elapsed 1m 41s (remain 1m 50s) Loss: 0.0765(0.0931) Grad: 58276.6484  LR: 0.00001399  \n\nEpoch: [2][340/671] Elapsed 1m 48s (remain 1m 45s) Loss: 0.0978(0.0937) Grad: 61316.1758  LR: 0.00001378  \n\nEpoch: [2][360/671] Elapsed 1m 54s (remain 1m 38s) Loss: 0.0810(0.0938) Grad: 65452.6641  LR: 0.00001356  \n\nEpoch: [2][380/671] Elapsed 2m 1s (remain 1m 32s) Loss: 0.0643(0.0942) Grad: 39928.2930  LR: 0.00001334  \n\nEpoch: [2][400/671] Elapsed 2m 8s (remain 1m 26s) Loss: 0.1685(0.0939) Grad: 42735.4844  LR: 0.00001312  \n\nEpoch: [2][420/671] Elapsed 2m 14s (remain 1m 20s) Loss: 0.2016(0.0942) Grad: 64967.5547  LR: 0.00001289  \n\nEpoch: [2][440/671] Elapsed 2m 21s (remain 1m 13s) Loss: 0.1079(0.0944) Grad: 61141.8750  LR: 0.00001267  \n\nEpoch: [2][460/671] Elapsed 2m 27s (remain 1m 7s) Loss: 0.1544(0.0946) Grad: 45465.3945  LR: 0.00001244  \n\nEpoch: [2][480/671] Elapsed 2m 34s (remain 1m 0s) Loss: 0.1125(0.0950) Grad: 16124.9951  LR: 0.00001222  \n\nEpoch: [2][500/671] Elapsed 2m 39s (remain 0m 54s) Loss: 0.0912(0.0951) Grad: 37610.6680  LR: 0.00001199  \n\nEpoch: [2][520/671] Elapsed 2m 46s (remain 0m 47s) Loss: 0.0676(0.0958) Grad: 22954.5645  LR: 0.00001176  \n\nEpoch: [2][540/671] Elapsed 2m 52s (remain 0m 41s) Loss: 0.0745(0.0958) Grad: 26167.4043  LR: 0.00001153  \n\nEpoch: [2][560/671] Elapsed 2m 58s (remain 0m 35s) Loss: 0.1237(0.0954) Grad: 60449.0391  LR: 0.00001129  \n\nEpoch: [2][580/671] Elapsed 3m 4s (remain 0m 28s) Loss: 0.1233(0.0961) Grad: 43949.6875  LR: 0.00001106  \n\nEpoch: [2][600/671] Elapsed 3m 10s (remain 0m 22s) Loss: 0.0777(0.0957) Grad: 21299.2168  LR: 0.00001083  \n\nEpoch: [2][620/671] Elapsed 3m 18s (remain 0m 15s) Loss: 0.0978(0.0957) Grad: 56927.1758  LR: 0.00001060  \n\nEpoch: [2][640/671] Elapsed 3m 25s (remain 0m 9s) Loss: 0.0814(0.0955) Grad: 31113.0918  LR: 0.00001036  \n\nEpoch: [2][660/671] Elapsed 3m 32s (remain 0m 3s) Loss: 0.0828(0.0959) Grad: 27995.8027  LR: 0.00001013  \n\nEpoch: [2][670/671] Elapsed 3m 35s (remain 0m 0s) Loss: 0.3699(0.0964) Grad: 55356.5312  LR: 0.00001001  \n\nEVAL: [0/112] Elapsed 0m 0s (remain 0m 40s) Loss: 0.1548(0.1548) \n\nEVAL: [20/112] Elapsed 0m 3s (remain 0m 15s) Loss: 0.0988(0.1132) \n\nEVAL: [40/112] Elapsed 0m 7s (remain 0m 13s) Loss: 0.1254(0.1162) \n\nEVAL: [60/112] Elapsed 0m 11s (remain 0m 9s) Loss: 0.1468(0.1172) \n\nEVAL: [80/112] Elapsed 0m 15s (remain 0m 5s) Loss: 0.1246(0.1185) \n\nEVAL: [100/112] Elapsed 0m 18s (remain 0m 2s) Loss: 0.0628(0.1183) \n"},{"name":"stderr","output_type":"stream","text":"Epoch 2 - avg_train_loss: 0.0964  avg_val_loss: 0.1176  time: 237s\n\nEpoch 2 - Score: 0.4897  Scores: [0.4399299397146032, 0.5393973342294365]\n\nEpoch 2 - Save Best Score: 0.4897 Model\n"},{"name":"stdout","output_type":"stream","text":"EVAL: [111/112] Elapsed 0m 20s (remain 0m 0s) Loss: 0.1415(0.1176) \n\nEpoch: [3][0/671] Elapsed 0m 0s (remain 5m 48s) Loss: 0.0560(0.0560) Grad: inf  LR: 0.00001000  \n\nEpoch: [3][20/671] Elapsed 0m 6s (remain 3m 31s) Loss: 0.0780(0.0798) Grad: 46113.8086  LR: 0.00000977  \n\nEpoch: [3][40/671] Elapsed 0m 13s (remain 3m 24s) Loss: 0.0403(0.0825) Grad: 55498.3438  LR: 0.00000953  \n\nEpoch: [3][60/671] Elapsed 0m 20s (remain 3m 20s) Loss: 0.0898(0.0805) Grad: 72155.8672  LR: 0.00000930  \n\nEpoch: [3][80/671] Elapsed 0m 25s (remain 3m 9s) Loss: 0.0586(0.0755) Grad: 43661.3906  LR: 0.00000907  \n\nEpoch: [3][100/671] Elapsed 0m 33s (remain 3m 8s) Loss: 0.1726(0.0736) Grad: 76698.6875  LR: 0.00000883  \n\nEpoch: [3][120/671] Elapsed 0m 40s (remain 3m 2s) Loss: 0.0398(0.0734) Grad: 45022.7734  LR: 0.00000860  \n\nEpoch: [3][140/671] Elapsed 0m 46s (remain 2m 54s) Loss: 0.0639(0.0725) Grad: 77711.5781  LR: 0.00000837  \n\nEpoch: [3][160/671] Elapsed 0m 52s (remain 2m 45s) Loss: 0.0861(0.0714) Grad: 104927.7109  LR: 0.00000814  \n\nEpoch: [3][180/671] Elapsed 0m 57s (remain 2m 36s) Loss: 0.0414(0.0703) Grad: 36683.1484  LR: 0.00000791  \n\nEpoch: [3][200/671] Elapsed 1m 3s (remain 2m 27s) Loss: 0.0374(0.0696) Grad: 29078.9219  LR: 0.00000768  \n\nEpoch: [3][220/671] Elapsed 1m 9s (remain 2m 21s) Loss: 0.0231(0.0681) Grad: 24772.7773  LR: 0.00000746  \n\nEpoch: [3][240/671] Elapsed 1m 16s (remain 2m 15s) Loss: 0.0674(0.0690) Grad: 49219.4805  LR: 0.00000723  \n\nEpoch: [3][260/671] Elapsed 1m 22s (remain 2m 9s) Loss: 0.0566(0.0689) Grad: 47605.1211  LR: 0.00000701  \n\nEpoch: [3][280/671] Elapsed 1m 28s (remain 2m 3s) Loss: 0.0266(0.0685) Grad: 20761.6406  LR: 0.00000678  \n\nEpoch: [3][300/671] Elapsed 1m 36s (remain 1m 58s) Loss: 0.0890(0.0680) Grad: 26805.1621  LR: 0.00000656  \n\nEpoch: [3][320/671] Elapsed 1m 43s (remain 1m 52s) Loss: 0.0364(0.0681) Grad: 16516.3965  LR: 0.00000634  \n\nEpoch: [3][340/671] Elapsed 1m 49s (remain 1m 45s) Loss: 0.0482(0.0681) Grad: 34690.4844  LR: 0.00000613  \n\nEpoch: [3][360/671] Elapsed 1m 55s (remain 1m 39s) Loss: 0.0335(0.0677) Grad: 41778.2461  LR: 0.00000591  \n\nEpoch: [3][380/671] Elapsed 2m 1s (remain 1m 32s) Loss: 0.0658(0.0671) Grad: 51320.8359  LR: 0.00000570  \n\nEpoch: [3][400/671] Elapsed 2m 7s (remain 1m 25s) Loss: 0.0394(0.0670) Grad: 37931.2500  LR: 0.00000549  \n\nEpoch: [3][420/671] Elapsed 2m 13s (remain 1m 19s) Loss: 0.1785(0.0669) Grad: 99873.1328  LR: 0.00000528  \n\nEpoch: [3][440/671] Elapsed 2m 19s (remain 1m 12s) Loss: 0.0778(0.0672) Grad: 64687.8242  LR: 0.00000508  \n\nEpoch: [3][460/671] Elapsed 2m 25s (remain 1m 6s) Loss: 0.0299(0.0666) Grad: 32151.6465  LR: 0.00000488  \n\nEpoch: [3][480/671] Elapsed 2m 32s (remain 1m 0s) Loss: 0.0543(0.0667) Grad: 55080.1523  LR: 0.00000468  \n\nEpoch: [3][500/671] Elapsed 2m 39s (remain 0m 54s) Loss: 0.0585(0.0667) Grad: 32351.7656  LR: 0.00000448  \n\nEpoch: [3][520/671] Elapsed 2m 45s (remain 0m 47s) Loss: 0.1162(0.0668) Grad: 99743.8750  LR: 0.00000429  \n\nEpoch: [3][540/671] Elapsed 2m 51s (remain 0m 41s) Loss: 0.0299(0.0662) Grad: 29904.2207  LR: 0.00000410  \n\nEpoch: [3][560/671] Elapsed 2m 59s (remain 0m 35s) Loss: 0.0368(0.0658) Grad: 20545.5566  LR: 0.00000391  \n\nEpoch: [3][580/671] Elapsed 3m 6s (remain 0m 28s) Loss: 0.0355(0.0660) Grad: 35295.3516  LR: 0.00000372  \n\nEpoch: [3][600/671] Elapsed 3m 12s (remain 0m 22s) Loss: 0.1130(0.0661) Grad: 111826.3047  LR: 0.00000354  \n\nEpoch: [3][620/671] Elapsed 3m 20s (remain 0m 16s) Loss: 0.0612(0.0664) Grad: 30648.2695  LR: 0.00000337  \n\nEpoch: [3][640/671] Elapsed 3m 26s (remain 0m 9s) Loss: 0.0653(0.0667) Grad: 46212.9375  LR: 0.00000319  \n\nEpoch: [3][660/671] Elapsed 3m 32s (remain 0m 3s) Loss: 0.0492(0.0665) Grad: 34146.5039  LR: 0.00000302  \n\nEpoch: [3][670/671] Elapsed 3m 35s (remain 0m 0s) Loss: 0.0908(0.0665) Grad: 47213.5312  LR: 0.00000294  \n\nEVAL: [0/112] Elapsed 0m 0s (remain 0m 35s) Loss: 0.1339(0.1339) \n\nEVAL: [20/112] Elapsed 0m 3s (remain 0m 15s) Loss: 0.0732(0.1087) \n\nEVAL: [40/112] Elapsed 0m 7s (remain 0m 13s) Loss: 0.1188(0.1108) \n\nEVAL: [60/112] Elapsed 0m 10s (remain 0m 9s) Loss: 0.1173(0.1113) \n\nEVAL: [80/112] Elapsed 0m 14s (remain 0m 5s) Loss: 0.1153(0.1119) \n\nEVAL: [100/112] Elapsed 0m 18s (remain 0m 2s) Loss: 0.0739(0.1125) \n"},{"name":"stderr","output_type":"stream","text":"Epoch 3 - avg_train_loss: 0.0665  avg_val_loss: 0.1116  time: 236s\n\nEpoch 3 - Score: 0.4732  Scores: [0.4039846242234985, 0.5423233771004446]\n\nEpoch 3 - Save Best Score: 0.4732 Model\n"},{"name":"stdout","output_type":"stream","text":"EVAL: [111/112] Elapsed 0m 20s (remain 0m 0s) Loss: 0.1387(0.1116) \n\nEpoch: [4][0/671] Elapsed 0m 0s (remain 6m 36s) Loss: 0.0554(0.0554) Grad: 217263.3438  LR: 0.00000293  \n\nEpoch: [4][20/671] Elapsed 0m 7s (remain 3m 37s) Loss: 0.0668(0.0511) Grad: 34368.8945  LR: 0.00000277  \n\nEpoch: [4][40/671] Elapsed 0m 14s (remain 3m 37s) Loss: 0.0478(0.0514) Grad: 70164.7578  LR: 0.00000261  \n\nEpoch: [4][60/671] Elapsed 0m 20s (remain 3m 26s) Loss: 0.1097(0.0509) Grad: 46946.5352  LR: 0.00000245  \n\nEpoch: [4][80/671] Elapsed 0m 26s (remain 3m 13s) Loss: 0.0369(0.0492) Grad: 54206.7734  LR: 0.00000230  \n\nEpoch: [4][100/671] Elapsed 0m 32s (remain 3m 5s) Loss: 0.0309(0.0486) Grad: 26452.5020  LR: 0.00000216  \n\nEpoch: [4][120/671] Elapsed 0m 40s (remain 3m 3s) Loss: 0.0553(0.0490) Grad: 32940.1172  LR: 0.00000201  \n\nEpoch: [4][140/671] Elapsed 0m 47s (remain 2m 58s) Loss: 0.0641(0.0483) Grad: 28089.7461  LR: 0.00000187  \n\nEpoch: [4][160/671] Elapsed 0m 53s (remain 2m 48s) Loss: 0.0295(0.0482) Grad: 35731.7031  LR: 0.00000174  \n\nEpoch: [4][180/671] Elapsed 0m 59s (remain 2m 41s) Loss: 0.0436(0.0494) Grad: 51868.4102  LR: 0.00000161  \n\nEpoch: [4][200/671] Elapsed 1m 5s (remain 2m 33s) Loss: 0.0282(0.0488) Grad: 20692.6641  LR: 0.00000149  \n\nEpoch: [4][220/671] Elapsed 1m 12s (remain 2m 28s) Loss: 0.0566(0.0490) Grad: 58986.7656  LR: 0.00000137  \n\nEpoch: [4][240/671] Elapsed 1m 19s (remain 2m 21s) Loss: 0.0536(0.0496) Grad: 43153.3047  LR: 0.00000125  \n\nEpoch: [4][260/671] Elapsed 1m 25s (remain 2m 14s) Loss: 0.0601(0.0493) Grad: 46942.2891  LR: 0.00000114  \n\nEpoch: [4][280/671] Elapsed 1m 31s (remain 2m 7s) Loss: 0.0625(0.0496) Grad: 48075.6562  LR: 0.00000103  \n\nEpoch: [4][300/671] Elapsed 1m 38s (remain 2m 0s) Loss: 0.0257(0.0493) Grad: 17216.8418  LR: 0.00000093  \n\nEpoch: [4][320/671] Elapsed 1m 44s (remain 1m 54s) Loss: 0.0286(0.0491) Grad: 30784.9180  LR: 0.00000084  \n\nEpoch: [4][340/671] Elapsed 1m 51s (remain 1m 47s) Loss: 0.0288(0.0488) Grad: 36600.8281  LR: 0.00000074  \n\nEpoch: [4][360/671] Elapsed 1m 58s (remain 1m 41s) Loss: 0.0426(0.0486) Grad: 41968.8125  LR: 0.00000066  \n\nEpoch: [4][380/671] Elapsed 2m 4s (remain 1m 34s) Loss: 0.0329(0.0487) Grad: 47002.3359  LR: 0.00000058  \n\nEpoch: [4][400/671] Elapsed 2m 11s (remain 1m 28s) Loss: 0.0728(0.0491) Grad: 64512.6172  LR: 0.00000050  \n\nEpoch: [4][420/671] Elapsed 2m 17s (remain 1m 21s) Loss: 0.0442(0.0490) Grad: 33918.2812  LR: 0.00000043  \n\nEpoch: [4][440/671] Elapsed 2m 24s (remain 1m 15s) Loss: 0.0597(0.0494) Grad: 56331.9336  LR: 0.00000037  \n\nEpoch: [4][460/671] Elapsed 2m 31s (remain 1m 8s) Loss: 0.0224(0.0496) Grad: 31350.5840  LR: 0.00000031  \n\nEpoch: [4][480/671] Elapsed 2m 36s (remain 1m 2s) Loss: 0.0387(0.0492) Grad: 28978.6641  LR: 0.00000025  \n\nEpoch: [4][500/671] Elapsed 2m 43s (remain 0m 55s) Loss: 0.0313(0.0498) Grad: 27717.7793  LR: 0.00000020  \n\nEpoch: [4][520/671] Elapsed 2m 49s (remain 0m 48s) Loss: 0.0464(0.0499) Grad: 35978.1523  LR: 0.00000016  \n\nEpoch: [4][540/671] Elapsed 2m 55s (remain 0m 42s) Loss: 0.0448(0.0496) Grad: 47414.7148  LR: 0.00000012  \n\nEpoch: [4][560/671] Elapsed 3m 1s (remain 0m 35s) Loss: 0.1152(0.0498) Grad: 59926.6797  LR: 0.00000009  \n\nEpoch: [4][580/671] Elapsed 3m 7s (remain 0m 28s) Loss: 0.0522(0.0497) Grad: 44175.4180  LR: 0.00000006  \n\nEpoch: [4][600/671] Elapsed 3m 13s (remain 0m 22s) Loss: 0.0333(0.0499) Grad: 60760.8633  LR: 0.00000004  \n\nEpoch: [4][620/671] Elapsed 3m 19s (remain 0m 16s) Loss: 0.1502(0.0501) Grad: 100290.5703  LR: 0.00000002  \n\nEpoch: [4][640/671] Elapsed 3m 26s (remain 0m 9s) Loss: 0.0391(0.0500) Grad: 45346.4141  LR: 0.00000001  \n\nEpoch: [4][660/671] Elapsed 3m 32s (remain 0m 3s) Loss: 0.0259(0.0498) Grad: 25586.5117  LR: 0.00000000  \n\nEpoch: [4][670/671] Elapsed 3m 36s (remain 0m 0s) Loss: 0.1089(0.0501) Grad: 107264.1641  LR: 0.00000000  \n\nEVAL: [0/112] Elapsed 0m 0s (remain 0m 37s) Loss: 0.1328(0.1328) \n\nEVAL: [20/112] Elapsed 0m 3s (remain 0m 15s) Loss: 0.0749(0.1093) \n\nEVAL: [40/112] Elapsed 0m 7s (remain 0m 13s) Loss: 0.1154(0.1115) \n\nEVAL: [60/112] Elapsed 0m 11s (remain 0m 9s) Loss: 0.1252(0.1118) \n\nEVAL: [80/112] Elapsed 0m 15s (remain 0m 5s) Loss: 0.1123(0.1123) \n\nEVAL: [100/112] Elapsed 0m 18s (remain 0m 2s) Loss: 0.0753(0.1133) \n"},{"name":"stderr","output_type":"stream","text":"Epoch 4 - avg_train_loss: 0.0501  avg_val_loss: 0.1125  time: 237s\n\nEpoch 4 - Score: 0.4753  Scores: [0.4068700782138248, 0.543703168217574]\n"},{"name":"stdout","output_type":"stream","text":"EVAL: [111/112] Elapsed 0m 20s (remain 0m 0s) Loss: 0.1378(0.1125) \n"},{"name":"stderr","output_type":"stream","text":"========== fold: 2 result ==========\n\nScore: 0.4732  Scores: [0.4039846242234985, 0.5423233771004446]\n\n========== fold: 3 training ==========\n\nDebertaV2Config {\n\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n\n  \"attention_dropout\": 0.0,\n\n  \"attention_probs_dropout_prob\": 0.0,\n\n  \"hidden_act\": \"gelu\",\n\n  \"hidden_dropout\": 0.0,\n\n  \"hidden_dropout_prob\": 0.0,\n\n  \"hidden_size\": 768,\n\n  \"initializer_range\": 0.02,\n\n  \"intermediate_size\": 3072,\n\n  \"layer_norm_eps\": 1e-07,\n\n  \"max_position_embeddings\": 512,\n\n  \"max_relative_positions\": -1,\n\n  \"model_type\": \"deberta-v2\",\n\n  \"norm_rel_ebd\": \"layer_norm\",\n\n  \"num_attention_heads\": 12,\n\n  \"num_hidden_layers\": 12,\n\n  \"output_hidden_states\": true,\n\n  \"pad_token_id\": 0,\n\n  \"pooler_dropout\": 0,\n\n  \"pooler_hidden_act\": \"gelu\",\n\n  \"pooler_hidden_size\": 768,\n\n  \"pos_att_type\": [\n\n    \"p2c\",\n\n    \"c2p\"\n\n  ],\n\n  \"position_biased_input\": false,\n\n  \"position_buckets\": 256,\n\n  \"relative_attention\": true,\n\n  \"share_att_key\": true,\n\n  \"transformers_version\": \"4.30.2\",\n\n  \"type_vocab_size\": 0,\n\n  \"vocab_size\": 128100\n\n}\n\n\n\nSome weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n\n- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n\n- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"},{"name":"stdout","output_type":"stream","text":"Epoch: [1][0/671] Elapsed 0m 0s (remain 5m 29s) Loss: 0.5084(0.5084) Grad: inf  LR: 0.00002000  \n\nEpoch: [1][20/671] Elapsed 0m 6s (remain 3m 23s) Loss: 0.3439(0.4188) Grad: 32316.9844  LR: 0.00002000  \n\nEpoch: [1][40/671] Elapsed 0m 12s (remain 3m 15s) Loss: 0.2337(0.3669) Grad: 32981.0742  LR: 0.00001999  \n\nEpoch: [1][60/671] Elapsed 0m 20s (remain 3m 26s) Loss: 0.2587(0.3135) Grad: 57121.4492  LR: 0.00001997  \n\nEpoch: [1][80/671] Elapsed 0m 27s (remain 3m 22s) Loss: 0.2747(0.2782) Grad: 42730.0039  LR: 0.00001996  \n\nEpoch: [1][100/671] Elapsed 0m 34s (remain 3m 14s) Loss: 0.0847(0.2515) Grad: 22637.1074  LR: 0.00001993  \n\nEpoch: [1][120/671] Elapsed 0m 40s (remain 3m 6s) Loss: 0.2394(0.2343) Grad: 41158.3398  LR: 0.00001990  \n\nEpoch: [1][140/671] Elapsed 0m 47s (remain 2m 57s) Loss: 0.2182(0.2286) Grad: 33114.7539  LR: 0.00001986  \n\nEpoch: [1][160/671] Elapsed 0m 53s (remain 2m 48s) Loss: 0.0753(0.2203) Grad: 15545.2158  LR: 0.00001982  \n\nEpoch: [1][180/671] Elapsed 0m 59s (remain 2m 42s) Loss: 0.2113(0.2149) Grad: 48613.7148  LR: 0.00001978  \n\nEpoch: [1][200/671] Elapsed 1m 5s (remain 2m 32s) Loss: 0.0692(0.2086) Grad: 25713.2031  LR: 0.00001973  \n\nEpoch: [1][220/671] Elapsed 1m 12s (remain 2m 28s) Loss: 0.0918(0.2027) Grad: 23882.4902  LR: 0.00001967  \n\nEpoch: [1][240/671] Elapsed 1m 20s (remain 2m 23s) Loss: 0.0566(0.1961) Grad: 15803.3584  LR: 0.00001961  \n\nEpoch: [1][260/671] Elapsed 1m 27s (remain 2m 17s) Loss: 0.2164(0.1928) Grad: 40500.1484  LR: 0.00001954  \n\nEpoch: [1][280/671] Elapsed 1m 33s (remain 2m 9s) Loss: 0.1064(0.1914) Grad: 23155.2695  LR: 0.00001947  \n\nEpoch: [1][300/671] Elapsed 1m 39s (remain 2m 2s) Loss: 0.1145(0.1885) Grad: 34144.6875  LR: 0.00001939  \n\nEpoch: [1][320/671] Elapsed 1m 45s (remain 1m 55s) Loss: 0.0714(0.1841) Grad: 36255.5508  LR: 0.00001930  \n\nEpoch: [1][340/671] Elapsed 1m 52s (remain 1m 48s) Loss: 0.1572(0.1825) Grad: 67926.1172  LR: 0.00001922  \n\nEpoch: [1][360/671] Elapsed 1m 57s (remain 1m 40s) Loss: 0.0869(0.1796) Grad: 60044.6250  LR: 0.00001912  \n\nEpoch: [1][380/671] Elapsed 2m 3s (remain 1m 33s) Loss: 0.1582(0.1771) Grad: 31224.2051  LR: 0.00001902  \n\nEpoch: [1][400/671] Elapsed 2m 10s (remain 1m 27s) Loss: 0.1941(0.1760) Grad: 68273.1797  LR: 0.00001892  \n\nEpoch: [1][420/671] Elapsed 2m 16s (remain 1m 21s) Loss: 0.1572(0.1737) Grad: 26460.8203  LR: 0.00001881  \n\nEpoch: [1][440/671] Elapsed 2m 23s (remain 1m 15s) Loss: 0.0697(0.1724) Grad: 42177.4375  LR: 0.00001870  \n\nEpoch: [1][460/671] Elapsed 2m 29s (remain 1m 8s) Loss: 0.0594(0.1695) Grad: 30963.6992  LR: 0.00001858  \n\nEpoch: [1][480/671] Elapsed 2m 34s (remain 1m 1s) Loss: 0.0836(0.1670) Grad: 44377.8320  LR: 0.00001846  \n\nEpoch: [1][500/671] Elapsed 2m 41s (remain 0m 54s) Loss: 0.2002(0.1653) Grad: 37392.3203  LR: 0.00001833  \n\nEpoch: [1][520/671] Elapsed 2m 48s (remain 0m 48s) Loss: 0.1428(0.1646) Grad: 26442.5762  LR: 0.00001820  \n\nEpoch: [1][540/671] Elapsed 2m 53s (remain 0m 41s) Loss: 0.1759(0.1651) Grad: 49609.5586  LR: 0.00001807  \n\nEpoch: [1][560/671] Elapsed 3m 0s (remain 0m 35s) Loss: 0.1182(0.1646) Grad: 51130.2930  LR: 0.00001792  \n\nEpoch: [1][580/671] Elapsed 3m 6s (remain 0m 28s) Loss: 0.0779(0.1633) Grad: 20239.0371  LR: 0.00001778  \n\nEpoch: [1][600/671] Elapsed 3m 12s (remain 0m 22s) Loss: 0.0687(0.1618) Grad: 14535.0107  LR: 0.00001763  \n\nEpoch: [1][620/671] Elapsed 3m 18s (remain 0m 16s) Loss: 0.2075(0.1606) Grad: 43956.8984  LR: 0.00001748  \n\nEpoch: [1][640/671] Elapsed 3m 25s (remain 0m 9s) Loss: 0.1318(0.1593) Grad: 23492.7871  LR: 0.00001732  \n\nEpoch: [1][660/671] Elapsed 3m 31s (remain 0m 3s) Loss: 0.2804(0.1588) Grad: 63010.9141  LR: 0.00001716  \n\nEpoch: [1][670/671] Elapsed 3m 34s (remain 0m 0s) Loss: 0.1768(0.1583) Grad: 23660.0410  LR: 0.00001708  \n\nEVAL: [0/112] Elapsed 0m 0s (remain 0m 50s) Loss: 0.2777(0.2777) \n\nEVAL: [20/112] Elapsed 0m 3s (remain 0m 17s) Loss: 0.1905(0.1400) \n\nEVAL: [40/112] Elapsed 0m 7s (remain 0m 12s) Loss: 0.1536(0.1420) \n\nEVAL: [60/112] Elapsed 0m 11s (remain 0m 9s) Loss: 0.1753(0.1425) \n\nEVAL: [80/112] Elapsed 0m 14s (remain 0m 5s) Loss: 0.0874(0.1398) \n\nEVAL: [100/112] Elapsed 0m 18s (remain 0m 2s) Loss: 0.0956(0.1339) \n\nEVAL: [111/112] Elapsed 0m 20s (remain 0m 0s) Loss: 0.1480(0.1338) \n"},{"name":"stderr","output_type":"stream","text":"Epoch 1 - avg_train_loss: 0.1583  avg_val_loss: 0.1338  time: 236s\n\nEpoch 1 - Score: 0.5263  Scores: [0.4562947393159735, 0.5962788203038255]\n\nEpoch 1 - Save Best Score: 0.5263 Model\n"},{"name":"stdout","output_type":"stream","text":"Epoch: [2][0/671] Elapsed 0m 0s (remain 9m 14s) Loss: 0.2316(0.2316) Grad: inf  LR: 0.00001707  \n\nEpoch: [2][20/671] Elapsed 0m 5s (remain 3m 5s) Loss: 0.0684(0.1127) Grad: 49813.9297  LR: 0.00001690  \n\nEpoch: [2][40/671] Elapsed 0m 12s (remain 3m 5s) Loss: 0.0974(0.1068) Grad: 70137.3906  LR: 0.00001673  \n\nEpoch: [2][60/671] Elapsed 0m 18s (remain 3m 5s) Loss: 0.2525(0.1028) Grad: 77012.6641  LR: 0.00001656  \n\nEpoch: [2][80/671] Elapsed 0m 25s (remain 3m 3s) Loss: 0.1593(0.0998) Grad: 53601.2305  LR: 0.00001638  \n\nEpoch: [2][100/671] Elapsed 0m 31s (remain 3m 0s) Loss: 0.0811(0.0979) Grad: 39415.5273  LR: 0.00001620  \n\nEpoch: [2][120/671] Elapsed 0m 39s (remain 3m 0s) Loss: 0.1185(0.0982) Grad: 53806.4102  LR: 0.00001601  \n\nEpoch: [2][140/671] Elapsed 0m 46s (remain 2m 54s) Loss: 0.0256(0.0993) Grad: 23795.6113  LR: 0.00001582  \n\nEpoch: [2][160/671] Elapsed 0m 52s (remain 2m 46s) Loss: 0.0904(0.0971) Grad: 29049.5059  LR: 0.00001563  \n\nEpoch: [2][180/671] Elapsed 0m 58s (remain 2m 38s) Loss: 0.0726(0.0990) Grad: 30182.2402  LR: 0.00001544  \n\nEpoch: [2][200/671] Elapsed 1m 4s (remain 2m 30s) Loss: 0.1183(0.0972) Grad: 31786.4336  LR: 0.00001524  \n\nEpoch: [2][220/671] Elapsed 1m 10s (remain 2m 23s) Loss: 0.0174(0.0966) Grad: 7225.6084  LR: 0.00001504  \n\nEpoch: [2][240/671] Elapsed 1m 16s (remain 2m 16s) Loss: 0.1062(0.0964) Grad: 31112.8906  LR: 0.00001483  \n\nEpoch: [2][260/671] Elapsed 1m 22s (remain 2m 10s) Loss: 0.0285(0.0983) Grad: 10267.5752  LR: 0.00001463  \n\nEpoch: [2][280/671] Elapsed 1m 29s (remain 2m 3s) Loss: 0.0821(0.0983) Grad: 44674.7891  LR: 0.00001442  \n\nEpoch: [2][300/671] Elapsed 1m 36s (remain 1m 58s) Loss: 0.2288(0.0985) Grad: 38599.3789  LR: 0.00001421  \n\nEpoch: [2][320/671] Elapsed 1m 42s (remain 1m 52s) Loss: 0.1314(0.0980) Grad: 49829.9492  LR: 0.00001399  \n\nEpoch: [2][340/671] Elapsed 1m 49s (remain 1m 45s) Loss: 0.0978(0.0972) Grad: 50876.9062  LR: 0.00001378  \n\nEpoch: [2][360/671] Elapsed 1m 55s (remain 1m 39s) Loss: 0.1856(0.0976) Grad: 70250.5391  LR: 0.00001356  \n\nEpoch: [2][380/671] Elapsed 2m 2s (remain 1m 32s) Loss: 0.1352(0.0973) Grad: 59755.5977  LR: 0.00001334  \n\nEpoch: [2][400/671] Elapsed 2m 9s (remain 1m 27s) Loss: 0.0894(0.0972) Grad: 38853.1641  LR: 0.00001312  \n\nEpoch: [2][420/671] Elapsed 2m 15s (remain 1m 20s) Loss: 0.0277(0.0985) Grad: 9381.1504  LR: 0.00001290  \n\nEpoch: [2][440/671] Elapsed 2m 22s (remain 1m 14s) Loss: 0.0727(0.0989) Grad: 31349.7285  LR: 0.00001267  \n\nEpoch: [2][460/671] Elapsed 2m 28s (remain 1m 7s) Loss: 0.1156(0.0986) Grad: 36422.0273  LR: 0.00001245  \n\nEpoch: [2][480/671] Elapsed 2m 33s (remain 1m 0s) Loss: 0.1062(0.0988) Grad: 35274.2422  LR: 0.00001222  \n\nEpoch: [2][500/671] Elapsed 2m 39s (remain 0m 53s) Loss: 0.1180(0.0991) Grad: 44533.1641  LR: 0.00001199  \n\nEpoch: [2][520/671] Elapsed 2m 46s (remain 0m 47s) Loss: 0.0955(0.0999) Grad: 46356.5742  LR: 0.00001176  \n\nEpoch: [2][540/671] Elapsed 2m 51s (remain 0m 41s) Loss: 0.0687(0.0990) Grad: 31058.8750  LR: 0.00001153  \n\nEpoch: [2][560/671] Elapsed 2m 57s (remain 0m 34s) Loss: 0.1140(0.0985) Grad: 34466.7578  LR: 0.00001130  \n\nEpoch: [2][580/671] Elapsed 3m 5s (remain 0m 28s) Loss: 0.0874(0.0989) Grad: 18008.1270  LR: 0.00001107  \n\nEpoch: [2][600/671] Elapsed 3m 10s (remain 0m 22s) Loss: 0.1492(0.0991) Grad: 54992.7344  LR: 0.00001083  \n\nEpoch: [2][620/671] Elapsed 3m 17s (remain 0m 15s) Loss: 0.0782(0.0996) Grad: 45612.9844  LR: 0.00001060  \n\nEpoch: [2][640/671] Elapsed 3m 23s (remain 0m 9s) Loss: 0.0479(0.0995) Grad: 20558.8203  LR: 0.00001037  \n\nEpoch: [2][660/671] Elapsed 3m 31s (remain 0m 3s) Loss: 0.1248(0.0994) Grad: 30911.7891  LR: 0.00001013  \n\nEpoch: [2][670/671] Elapsed 3m 34s (remain 0m 0s) Loss: 0.1000(0.0994) Grad: 22173.7656  LR: 0.00001002  \n\nEVAL: [0/112] Elapsed 0m 0s (remain 0m 50s) Loss: 0.2255(0.2255) \n\nEVAL: [20/112] Elapsed 0m 3s (remain 0m 17s) Loss: 0.1745(0.1286) \n\nEVAL: [40/112] Elapsed 0m 7s (remain 0m 12s) Loss: 0.1351(0.1261) \n\nEVAL: [60/112] Elapsed 0m 11s (remain 0m 9s) Loss: 0.1207(0.1240) \n\nEVAL: [80/112] Elapsed 0m 14s (remain 0m 5s) Loss: 0.0827(0.1215) \n\nEVAL: [100/112] Elapsed 0m 18s (remain 0m 2s) Loss: 0.0601(0.1171) \n"},{"name":"stderr","output_type":"stream","text":"Epoch 2 - avg_train_loss: 0.0994  avg_val_loss: 0.1170  time: 236s\n\nEpoch 2 - Score: 0.4879  Scores: [0.41843396973978864, 0.5573273112109701]\n\nEpoch 2 - Save Best Score: 0.4879 Model\n"},{"name":"stdout","output_type":"stream","text":"EVAL: [111/112] Elapsed 0m 20s (remain 0m 0s) Loss: 0.0858(0.1170) \n\nEpoch: [3][0/671] Elapsed 0m 0s (remain 6m 39s) Loss: 0.0433(0.0433) Grad: inf  LR: 0.00001001  \n\nEpoch: [3][20/671] Elapsed 0m 7s (remain 4m 7s) Loss: 0.0578(0.0706) Grad: 65442.2656  LR: 0.00000977  \n\nEpoch: [3][40/671] Elapsed 0m 14s (remain 3m 47s) Loss: 0.0860(0.0658) Grad: 78936.7500  LR: 0.00000954  \n\nEpoch: [3][60/671] Elapsed 0m 21s (remain 3m 34s) Loss: 0.1080(0.0728) Grad: 68353.2656  LR: 0.00000930  \n\nEpoch: [3][80/671] Elapsed 0m 27s (remain 3m 19s) Loss: 0.0367(0.0704) Grad: 38981.8008  LR: 0.00000907  \n\nEpoch: [3][100/671] Elapsed 0m 33s (remain 3m 11s) Loss: 0.0673(0.0702) Grad: 47475.3594  LR: 0.00000884  \n\nEpoch: [3][120/671] Elapsed 0m 41s (remain 3m 7s) Loss: 0.0507(0.0705) Grad: 34966.3125  LR: 0.00000861  \n\nEpoch: [3][140/671] Elapsed 0m 48s (remain 3m 2s) Loss: 0.0944(0.0724) Grad: 76960.4297  LR: 0.00000838  \n\nEpoch: [3][160/671] Elapsed 0m 54s (remain 2m 52s) Loss: 0.0718(0.0719) Grad: 74934.3984  LR: 0.00000815  \n\nEpoch: [3][180/671] Elapsed 1m 0s (remain 2m 43s) Loss: 0.0404(0.0706) Grad: 21795.3652  LR: 0.00000792  \n\nEpoch: [3][200/671] Elapsed 1m 6s (remain 2m 35s) Loss: 0.0460(0.0690) Grad: 41007.7500  LR: 0.00000769  \n\nEpoch: [3][220/671] Elapsed 1m 12s (remain 2m 28s) Loss: 0.0290(0.0691) Grad: 48233.6289  LR: 0.00000746  \n\nEpoch: [3][240/671] Elapsed 1m 19s (remain 2m 22s) Loss: 0.0761(0.0688) Grad: 20409.6660  LR: 0.00000724  \n\nEpoch: [3][260/671] Elapsed 1m 25s (remain 2m 14s) Loss: 0.1640(0.0683) Grad: 115190.3359  LR: 0.00000701  \n\nEpoch: [3][280/671] Elapsed 1m 32s (remain 2m 7s) Loss: 0.0878(0.0682) Grad: 122111.1797  LR: 0.00000679  \n\nEpoch: [3][300/671] Elapsed 1m 37s (remain 2m 0s) Loss: 0.0231(0.0680) Grad: 23654.7480  LR: 0.00000657  \n\nEpoch: [3][320/671] Elapsed 1m 43s (remain 1m 53s) Loss: 0.0415(0.0676) Grad: 48915.5625  LR: 0.00000635  \n\nEpoch: [3][340/671] Elapsed 1m 50s (remain 1m 47s) Loss: 0.0179(0.0675) Grad: 35216.3398  LR: 0.00000613  \n\nEpoch: [3][360/671] Elapsed 1m 57s (remain 1m 40s) Loss: 0.0906(0.0672) Grad: 49477.7578  LR: 0.00000592  \n\nEpoch: [3][380/671] Elapsed 2m 2s (remain 1m 33s) Loss: 0.0431(0.0667) Grad: 48519.8008  LR: 0.00000571  \n\nEpoch: [3][400/671] Elapsed 2m 10s (remain 1m 27s) Loss: 0.0288(0.0664) Grad: 34499.1836  LR: 0.00000550  \n\nEpoch: [3][420/671] Elapsed 2m 15s (remain 1m 20s) Loss: 0.0545(0.0662) Grad: 72862.1172  LR: 0.00000529  \n\nEpoch: [3][440/671] Elapsed 2m 22s (remain 1m 14s) Loss: 0.0684(0.0661) Grad: 77192.6875  LR: 0.00000508  \n\nEpoch: [3][460/671] Elapsed 2m 27s (remain 1m 7s) Loss: 0.0425(0.0656) Grad: 18512.1055  LR: 0.00000488  \n\nEpoch: [3][480/671] Elapsed 2m 34s (remain 1m 1s) Loss: 0.0538(0.0655) Grad: 56365.3945  LR: 0.00000468  \n\nEpoch: [3][500/671] Elapsed 2m 40s (remain 0m 54s) Loss: 0.0613(0.0656) Grad: 34925.5312  LR: 0.00000449  \n\nEpoch: [3][520/671] Elapsed 2m 47s (remain 0m 48s) Loss: 0.0482(0.0652) Grad: 61619.7266  LR: 0.00000429  \n\nEpoch: [3][540/671] Elapsed 2m 53s (remain 0m 41s) Loss: 0.0671(0.0650) Grad: 48248.4805  LR: 0.00000410  \n\nEpoch: [3][560/671] Elapsed 2m 59s (remain 0m 35s) Loss: 0.0596(0.0647) Grad: 48330.2305  LR: 0.00000392  \n\nEpoch: [3][580/671] Elapsed 3m 6s (remain 0m 28s) Loss: 0.0960(0.0651) Grad: 50234.6250  LR: 0.00000373  \n\nEpoch: [3][600/671] Elapsed 3m 13s (remain 0m 22s) Loss: 0.0477(0.0647) Grad: 69886.7812  LR: 0.00000355  \n\nEpoch: [3][620/671] Elapsed 3m 19s (remain 0m 16s) Loss: 0.0868(0.0643) Grad: 51879.0625  LR: 0.00000337  \n\nEpoch: [3][640/671] Elapsed 3m 25s (remain 0m 9s) Loss: 0.0427(0.0642) Grad: 41329.9062  LR: 0.00000320  \n\nEpoch: [3][660/671] Elapsed 3m 31s (remain 0m 3s) Loss: 0.0899(0.0641) Grad: 38751.4727  LR: 0.00000303  \n\nEpoch: [3][670/671] Elapsed 3m 35s (remain 0m 0s) Loss: 0.0543(0.0642) Grad: 39844.4141  LR: 0.00000295  \n\nEVAL: [0/112] Elapsed 0m 0s (remain 0m 46s) Loss: 0.2109(0.2109) \n\nEVAL: [20/112] Elapsed 0m 3s (remain 0m 17s) Loss: 0.1638(0.1212) \n\nEVAL: [40/112] Elapsed 0m 7s (remain 0m 12s) Loss: 0.1053(0.1171) \n\nEVAL: [60/112] Elapsed 0m 11s (remain 0m 9s) Loss: 0.1183(0.1165) \n\nEVAL: [80/112] Elapsed 0m 14s (remain 0m 5s) Loss: 0.0869(0.1135) \n\nEVAL: [100/112] Elapsed 0m 18s (remain 0m 2s) Loss: 0.0702(0.1093) \n"},{"name":"stderr","output_type":"stream","text":"Epoch 3 - avg_train_loss: 0.0642  avg_val_loss: 0.1098  time: 237s\n\nEpoch 3 - Score: 0.4719  Scores: [0.40624884349553214, 0.5375453819913976]\n\nEpoch 3 - Save Best Score: 0.4719 Model\n"},{"name":"stdout","output_type":"stream","text":"EVAL: [111/112] Elapsed 0m 20s (remain 0m 0s) Loss: 0.0986(0.1098) \n\nEpoch: [4][0/671] Elapsed 0m 0s (remain 4m 56s) Loss: 0.0180(0.0180) Grad: 115517.8125  LR: 0.00000294  \n\nEpoch: [4][20/671] Elapsed 0m 5s (remain 3m 3s) Loss: 0.0458(0.0479) Grad: 47687.4180  LR: 0.00000278  \n\nEpoch: [4][40/671] Elapsed 0m 12s (remain 3m 11s) Loss: 0.0140(0.0496) Grad: 27247.0469  LR: 0.00000262  \n\nEpoch: [4][60/671] Elapsed 0m 18s (remain 3m 8s) Loss: 0.0307(0.0484) Grad: 29133.2383  LR: 0.00000246  \n\nEpoch: [4][80/671] Elapsed 0m 25s (remain 3m 6s) Loss: 0.0685(0.0490) Grad: 53436.0938  LR: 0.00000231  \n\nEpoch: [4][100/671] Elapsed 0m 32s (remain 3m 1s) Loss: 0.0135(0.0475) Grad: 26636.8750  LR: 0.00000216  \n\nEpoch: [4][120/671] Elapsed 0m 38s (remain 2m 55s) Loss: 0.0229(0.0470) Grad: 20958.5410  LR: 0.00000202  \n\nEpoch: [4][140/671] Elapsed 0m 44s (remain 2m 47s) Loss: 0.0207(0.0469) Grad: 32815.7383  LR: 0.00000188  \n\nEpoch: [4][160/671] Elapsed 0m 50s (remain 2m 39s) Loss: 0.0263(0.0468) Grad: 36014.0430  LR: 0.00000175  \n\nEpoch: [4][180/671] Elapsed 0m 56s (remain 2m 33s) Loss: 0.0492(0.0478) Grad: 60607.8516  LR: 0.00000162  \n\nEpoch: [4][200/671] Elapsed 1m 3s (remain 2m 29s) Loss: 0.0322(0.0481) Grad: 28514.0566  LR: 0.00000149  \n\nEpoch: [4][220/671] Elapsed 1m 10s (remain 2m 23s) Loss: 0.1519(0.0491) Grad: 52462.5039  LR: 0.00000137  \n\nEpoch: [4][240/671] Elapsed 1m 16s (remain 2m 16s) Loss: 0.0741(0.0490) Grad: 56045.6406  LR: 0.00000125  \n\nEpoch: [4][260/671] Elapsed 1m 22s (remain 2m 9s) Loss: 0.0776(0.0485) Grad: 65270.4141  LR: 0.00000114  \n\nEpoch: [4][280/671] Elapsed 1m 28s (remain 2m 2s) Loss: 0.0296(0.0488) Grad: 34378.8047  LR: 0.00000104  \n\nEpoch: [4][300/671] Elapsed 1m 36s (remain 1m 58s) Loss: 0.1008(0.0485) Grad: 57718.8984  LR: 0.00000094  \n\nEpoch: [4][320/671] Elapsed 1m 42s (remain 1m 51s) Loss: 0.0402(0.0485) Grad: 52917.7031  LR: 0.00000084  \n\nEpoch: [4][340/671] Elapsed 1m 48s (remain 1m 45s) Loss: 0.0550(0.0481) Grad: 39792.0625  LR: 0.00000075  \n\nEpoch: [4][360/671] Elapsed 1m 54s (remain 1m 38s) Loss: 0.0482(0.0482) Grad: 50129.8164  LR: 0.00000066  \n\nEpoch: [4][380/671] Elapsed 2m 0s (remain 1m 31s) Loss: 0.0591(0.0484) Grad: 31040.3477  LR: 0.00000058  \n\nEpoch: [4][400/671] Elapsed 2m 7s (remain 1m 26s) Loss: 0.0191(0.0485) Grad: 39523.5898  LR: 0.00000051  \n\nEpoch: [4][420/671] Elapsed 2m 13s (remain 1m 19s) Loss: 0.0341(0.0482) Grad: 47120.0586  LR: 0.00000043  \n\nEpoch: [4][440/671] Elapsed 2m 20s (remain 1m 13s) Loss: 0.0250(0.0479) Grad: 20488.4707  LR: 0.00000037  \n\nEpoch: [4][460/671] Elapsed 2m 27s (remain 1m 7s) Loss: 0.0464(0.0481) Grad: 34644.9180  LR: 0.00000031  \n\nEpoch: [4][480/671] Elapsed 2m 34s (remain 1m 0s) Loss: 0.0742(0.0481) Grad: 60837.1055  LR: 0.00000025  \n\nEpoch: [4][500/671] Elapsed 2m 39s (remain 0m 54s) Loss: 0.0267(0.0481) Grad: 31075.4121  LR: 0.00000020  \n\nEpoch: [4][520/671] Elapsed 2m 46s (remain 0m 47s) Loss: 0.0229(0.0481) Grad: 15669.4883  LR: 0.00000016  \n\nEpoch: [4][540/671] Elapsed 2m 52s (remain 0m 41s) Loss: 0.0971(0.0480) Grad: 72676.8516  LR: 0.00000012  \n\nEpoch: [4][560/671] Elapsed 2m 59s (remain 0m 35s) Loss: 0.0481(0.0485) Grad: 31484.8828  LR: 0.00000009  \n\nEpoch: [4][580/671] Elapsed 3m 6s (remain 0m 28s) Loss: 0.0332(0.0486) Grad: 27541.1113  LR: 0.00000006  \n\nEpoch: [4][600/671] Elapsed 3m 12s (remain 0m 22s) Loss: 0.0541(0.0485) Grad: 37865.2617  LR: 0.00000004  \n\nEpoch: [4][620/671] Elapsed 3m 19s (remain 0m 16s) Loss: 0.0735(0.0481) Grad: 47672.2773  LR: 0.00000002  \n\nEpoch: [4][640/671] Elapsed 3m 26s (remain 0m 9s) Loss: 0.0593(0.0482) Grad: 28217.1719  LR: 0.00000001  \n\nEpoch: [4][660/671] Elapsed 3m 33s (remain 0m 3s) Loss: 0.0513(0.0482) Grad: 35964.4180  LR: 0.00000000  \n\nEpoch: [4][670/671] Elapsed 3m 36s (remain 0m 0s) Loss: 0.0651(0.0482) Grad: 29333.3770  LR: 0.00000000  \n\nEVAL: [0/112] Elapsed 0m 0s (remain 0m 43s) Loss: 0.2042(0.2042) \n\nEVAL: [20/112] Elapsed 0m 3s (remain 0m 16s) Loss: 0.1646(0.1213) \n\nEVAL: [40/112] Elapsed 0m 7s (remain 0m 12s) Loss: 0.1076(0.1168) \n\nEVAL: [60/112] Elapsed 0m 11s (remain 0m 9s) Loss: 0.1208(0.1165) \n\nEVAL: [80/112] Elapsed 0m 14s (remain 0m 5s) Loss: 0.0934(0.1136) \n\nEVAL: [100/112] Elapsed 0m 18s (remain 0m 2s) Loss: 0.0711(0.1093) \n"},{"name":"stderr","output_type":"stream","text":"Epoch 4 - avg_train_loss: 0.0482  avg_val_loss: 0.1099  time: 238s\n\nEpoch 4 - Score: 0.4712  Scores: [0.4054069534502577, 0.5370317731230411]\n\nEpoch 4 - Save Best Score: 0.4712 Model\n"},{"name":"stdout","output_type":"stream","text":"EVAL: [111/112] Elapsed 0m 20s (remain 0m 0s) Loss: 0.0939(0.1099) \n"},{"name":"stderr","output_type":"stream","text":"========== fold: 3 result ==========\n\nScore: 0.4712  Scores: [0.4054069534502577, 0.5370317731230411]\n\n========== CV ==========\n\nScore: 0.4670  Scores: [0.3979851971315948, 0.5360034540441694]\n"}]}]}